{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FKz11/NLP/blob/main/lesson_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlKXwVcK6A5U"
      },
      "source": [
        "# Lesson-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73rvC4kU6tXu"
      },
      "source": [
        "Тема «POS-tagger и NER»\n",
        "\n",
        "Задание 1. Написать теггер на данных с русским языком\n",
        "проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации\n",
        "написать свой теггер как на занятии, попробовать разные векторайзеры, добавить знание не только букв но и слов\n",
        "сравнить все реализованные методы, сделать выводы\n",
        " \n",
        "Задание 2. Проверить, насколько хорошо работает NER\n",
        "Данные брать из http://www.labinform.ru/pub/named_entities/\n",
        "проверить NER из nltk/spacy/deeppavlov.\n",
        "написать свой NER, попробовать разные подходы.\n",
        "передаём в сетку токен и его соседей.\n",
        "передаём в сетку только токен.\n",
        "свой вариант.\n",
        "сравнить свои реализованные подходы на качество — вывести precision/recall/f1_score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJqGLcyr_6X5"
      },
      "source": [
        "### Библиотеки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "kiyMYkKO-neG"
      },
      "outputs": [],
      "source": [
        "!pip install -qq pyconll"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "FQvbPQXM6vnI"
      },
      "outputs": [],
      "source": [
        "import pyconll\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger, TrigramTagger\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalAveragePooling1D, BatchNormalization, Dropout, LSTM, Activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4M477LMEF5r"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyeSlSe8AEHl"
      },
      "source": [
        "## Задание 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx9ANI2Q_9tM"
      },
      "source": [
        "### Данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkjDblFu58dk"
      },
      "outputs": [],
      "source": [
        "!mkdir dataset_ru"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v9oEvjk_J5x"
      },
      "source": [
        "данные взяты из \n",
        "\n",
        "https://github.com/UniversalDependencies/UD_Russian-SynTagRus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbkJWxeT_KJT",
        "outputId": "a16c82fd-2fbb-4d27-990a-a1feba6c77ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-05 15:55:12--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 40736565 (39M) [text/plain]\n",
            "Saving to: ‘./dataset_ru/ru_syntagrus-ud-train-a.conllu’\n",
            "\n",
            "./dataset_ru/ru_syn 100%[===================>]  38.85M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2022-09-05 15:55:16 (411 MB/s) - ‘./dataset_ru/ru_syntagrus-ud-train-a.conllu’ saved [40736565/40736565]\n",
            "\n",
            "--2022-09-05 15:55:16--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 42819806 (41M) [text/plain]\n",
            "Saving to: ‘./dataset_ru/ru_syntagrus-ud-train-b.conllu’\n",
            "\n",
            "./dataset_ru/ru_syn 100%[===================>]  40.84M   203MB/s    in 0.2s    \n",
            "\n",
            "2022-09-05 15:55:21 (203 MB/s) - ‘./dataset_ru/ru_syntagrus-ud-train-b.conllu’ saved [42819806/42819806]\n",
            "\n",
            "--2022-09-05 15:55:21--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-c.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32367464 (31M) [text/plain]\n",
            "Saving to: ‘./dataset_ru/ru_syntagrus-ud-train-c.conllu’\n",
            "\n",
            "./dataset_ru/ru_syn 100%[===================>]  30.87M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-09-05 15:55:24 (308 MB/s) - ‘./dataset_ru/ru_syntagrus-ud-train-c.conllu’ saved [32367464/32367464]\n",
            "\n",
            "--2022-09-05 15:55:24--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14704579 (14M) [text/plain]\n",
            "Saving to: ‘./dataset_ru/ru_syntagrus-ud-dev.conllu’\n",
            "\n",
            "./dataset_ru/ru_syn 100%[===================>]  14.02M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-09-05 15:55:27 (348 MB/s) - ‘./dataset_ru/ru_syntagrus-ud-dev.conllu’ saved [14704579/14704579]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O ./dataset_ru/ru_syntagrus-ud-train-a.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-a.conllu\n",
        "!wget -O ./dataset_ru/ru_syntagrus-ud-train-b.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-b.conllu\n",
        "!wget -O ./dataset_ru/ru_syntagrus-ud-train-c.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train-c.conllu\n",
        "!wget -O ./dataset_ru/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKLn2zbaJaZf"
      },
      "source": [
        "Тэги:\n",
        "\n",
        "1) ADJ - adjective (прилагательное)\n",
        "\n",
        "The 10 most frequent ADJ lemmas: новый, другой, первый, самый, должен, российский, большой, сам, нужный, последний\n",
        "\n",
        "2) ADP - adposition\n",
        "\n",
        "The 10 most frequent ADP lemmas: в, на, с, по, к, из, о, для, за, от\n",
        "\n",
        "3) ADV - adverb (наречие)\n",
        "\n",
        "The 10 most frequent ADV lemmas: так, уже, еще, можно, как, более, очень, где, однако, там\n",
        "\n",
        "4) AUX - auxiliary (вспомогательный)\n",
        "\n",
        "The 10 most frequent AUX lemmas: быть, бы\n",
        "\n",
        "5) CCONJ - coordinating conjunction (координирующее соединение)\n",
        "\n",
        "The 10 most frequent CCONJ lemmas: и, а, но, или, ни, да, либо, причем, зато, только\n",
        "\n",
        "6) DET - determiner (определитель)\n",
        "\n",
        "The 10 most frequent DET lemmas: этот, свой, весь, тот, такой, его, наш, их, мой, какой\n",
        "\n",
        "7) INTJ - interjection (междометие)\n",
        "\n",
        "The 10 most frequent INTJ lemmas: о, ах, а, гм, ой, ох, эх, ура, хахаха, ага\n",
        "\n",
        "8) NOUN - noun (существительное)\n",
        "\n",
        "The 10 most frequent NOUN lemmas: год, человек, время, страна, дело, жизнь, работа, власть, система, день\n",
        "\n",
        "9) NUM - numeral (числительное)\n",
        "\n",
        "The 10 most frequent NUM lemmas: один, два, много, несколько, три, 1, 10, 20, четыре, 2\n",
        "\n",
        "10) PART - particle (частица)\n",
        "\n",
        "The 10 most frequent PART lemmas: не, и, же, только, даже, вот, ли, именно, лишь, просто\n",
        "\n",
        "11) PRON - pronoun (местоимение)\n",
        "\n",
        "The 10 most frequent PRON lemmas: он, это, я, который, они, то, мы, она, что, все\n",
        "\n",
        "12) PROPN - proper noun (имя собственное)\n",
        "\n",
        "The 10 most frequent PROPN lemmas: Россия, Москва, США, Путин, СССР, Европа, Владимир, Германия, Александр, Сергей\n",
        "\n",
        "13) PUNCT - punctuation (пунктуация)\n",
        "\n",
        "The 10 most frequent PUNCT lemmas: ,, ., “, -, :, ?, ), (, !, …\n",
        "\n",
        "14) SCONJ - subordinating conjunction (подчиняющий союз)\n",
        "\n",
        "The 10 most frequent SCONJ lemmas: что, как, если, чтобы, когда, то, чем, хотя, поскольку, пока\n",
        "\n",
        "15) SYM - symbol (символ)\n",
        "\n",
        "The 10 most frequent SYM lemmas: %, $, №, °, &, €, +, =, №№, x\n",
        "\n",
        "16) VERB - verb (глагол)\n",
        "\n",
        "The 10 most frequent VERB lemmas: мочь, быть, стать, говорить, сказать, делать, иметь, знать, идти, нет\n",
        "\n",
        "17) X - other (другое)\n",
        "\n",
        "The 10 most frequent X lemmas: of, the, and, artist, empire, in, V&A, X, for, i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zI4yowkO_Yz0"
      },
      "outputs": [],
      "source": [
        "full_train = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-train-a.conllu')\n",
        "full_train_b = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-train-b.conllu')\n",
        "full_train_c = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-train-c.conllu')\n",
        "\n",
        "full_train.extend([*full_train_b, *full_train_c]) # объединяем\n",
        "\n",
        "full_test = pyconll.load_from_file('dataset_ru/ru_syntagrus-ud-dev.conllu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npBDy2HC_i9E",
        "outputId": "30458cdd-2781-40ca-e8ea-1d3dee119ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Анкета NOUN\n",
            ". PUNCT\n",
            "\n",
            "Начальник NOUN\n",
            "областного ADJ\n",
            "управления NOUN\n",
            "связи NOUN\n",
            "Семен PROPN\n",
            "Еремеевич PROPN\n",
            "был AUX\n",
            "человек NOUN\n",
            "простой ADJ\n",
            ", PUNCT\n",
            "приходил VERB\n",
            "на ADP\n",
            "работу NOUN\n",
            "всегда ADV\n",
            "вовремя ADV\n",
            ", PUNCT\n",
            "здоровался VERB\n",
            "с ADP\n",
            "секретаршей NOUN\n",
            "за ADP\n",
            "руку NOUN\n",
            "и CCONJ\n",
            "иногда ADV\n",
            "даже PART\n",
            "писал VERB\n",
            "в ADP\n",
            "стенгазету NOUN\n",
            "заметки NOUN\n",
            "под ADP\n",
            "псевдонимом NOUN\n",
            "\" PUNCT\n",
            "Муха NOUN\n",
            "\" PUNCT\n",
            ". PUNCT\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for sent in full_train[:2]:\n",
        "    for token in sent:\n",
        "        print(token.form, token.upos)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTooKkq3_4wt"
      },
      "outputs": [],
      "source": [
        "fdata_train = []\n",
        "for sent in full_train[:]:\n",
        "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
        "    \n",
        "fdata_sent_test = []\n",
        "for sent in full_test[:]:\n",
        "    fdata_sent_test.append([token.form for token in sent])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYkDlzifAWzq",
        "outputId": "ee476345-c1a8-4e56-a075-4d8c49aa6c17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Наибольшая длина предложения 205\n",
            "Наибольшая длина токена 47\n"
          ]
        }
      ],
      "source": [
        "MAX_SENT_LEN = max(len(sent) for sent in full_train)\n",
        "MAX_ORIG_TOKEN_LEN = max(len(\"\" if token.form is None else token.form) for sent in full_train for token in sent)\n",
        "print('Наибольшая длина предложения', MAX_SENT_LEN)\n",
        "print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KsBh1hrBPkQ",
        "outputId": "9dc2d089-d76d-4b1a-e56c-5af7a757cd17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Анкета .\n",
            "Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n",
            "В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n",
            "Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n",
            "Приемная была обставлена просто , но по-деловому .\n",
            "У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n",
            "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n",
            "Кабинет отличался скромностью , присущей Семену Еремеевичу .\n",
            "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n",
            "Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n",
            "NOUN PUNCT\n",
            "NOUN ADJ NOUN NOUN PROPN PROPN AUX NOUN ADJ PUNCT VERB ADP NOUN ADV ADV PUNCT VERB ADP NOUN ADP NOUN CCONJ ADV PART VERB ADP NOUN NOUN ADP NOUN PUNCT NOUN PUNCT PUNCT\n",
            "ADP NOUN PRON ADP NOUN VERB NOUN PUNCT PUNCT PRON ADP ADJ NOUN PUNCT CCONJ PRON PART ADP DET PUNCT PRON ADV ADV AUX VERB ADP ADJ NOUN PUNCT PART VERB PROPN PROPN PUNCT\n",
            "ADV NOUN NOUN PROPN PROPN VERB ADP PRON PUNCT SCONJ VERB DET VERB CCONJ ADV VERB ADP NOUN PUNCT\n",
            "NOUN AUX VERB ADV PUNCT CCONJ ADV PUNCT\n",
            "ADP NOUN VERB NOUN NOUN PUNCT ADP NOUN PUNCT VERB NOUN ADP ADJ NOUN PUNCT\n",
            "ADP NOUN VERB NOUN CCONJ VERB NOUN ADP NOUN VERB CCONJ ADV ADP PRON PUNCT SCONJ VERB NOUN NOUN PUNCT VERB ADP NOUN PUNCT ADV SCONJ PUNCT ADV PUNCT ADP NOUN VERB VERB PART ADJ NOUN PUNCT\n",
            "NOUN VERB NOUN PUNCT ADJ PROPN PROPN PUNCT\n",
            "ADP NOUN VERB ADJ ADJ NOUN ADP ADJ NOUN CCONJ ADP PRON NUM ADJ NOUN PUNCT\n",
            "ADV VERB NOUN ADP NOUN PUNCT ADJ PUNCT VERB ADJ NOUN CCONJ ADP NUM NOUN ADV VERB NOUN PUNCT\n"
          ]
        }
      ],
      "source": [
        "all_train_texts = [' '.join(\"\" if token.form is None else token.form for token in sent) for sent in full_train]\n",
        "all_test_texts = [' '.join(\"\" if token.form is None else token.form for token in sent) for sent in full_test]\n",
        "\n",
        "all_train_labels = [' '.join(\"\" if token.upos is None else token.upos for token in sent) for sent in full_train]\n",
        "all_test_labels = [' '.join(\"\" if token.upos is None else token.upos for token in sent) for sent in full_test]\n",
        "print('\\n'.join(all_train_texts[:10]))\n",
        "print('\\n'.join(all_train_labels[:10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qro4K8tZDysV",
        "outputId": "76429d88-859a-4ef7-f0ae-60ccf5e79ac8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Алгоритм',\n",
              " ',',\n",
              " 'от',\n",
              " 'имени',\n",
              " 'учёного',\n",
              " 'аль',\n",
              " '-',\n",
              " 'Хорезми',\n",
              " ',',\n",
              " '-',\n",
              " 'точный',\n",
              " 'набор',\n",
              " 'инструкций',\n",
              " ',',\n",
              " 'описывающих',\n",
              " 'порядок',\n",
              " 'действий',\n",
              " 'исполнителя',\n",
              " 'для',\n",
              " 'достижения',\n",
              " 'результата',\n",
              " 'решения',\n",
              " 'задачи',\n",
              " 'за',\n",
              " 'конечное',\n",
              " 'время',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "example_tag = fdata_sent_test[0]\n",
        "example_tag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCh4kxIYCA8M"
      },
      "source": [
        "### UnigramTagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "5AvmBXduBqQu",
        "outputId": "c92f1990-52be-4d8f-cbd5-c6fae1c990bd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', 'NOUN'),\n",
              " ('аль', 'PART'),\n",
              " ('-', 'PUNCT'),\n",
              " ('Хорезми', None),\n",
              " (',', 'PUNCT'),\n",
              " ('-', 'PUNCT'),\n",
              " ('точный', 'ADJ'),\n",
              " ('набор', 'NOUN'),\n",
              " ('инструкций', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('описывающих', 'VERB'),\n",
              " ('порядок', 'NOUN'),\n",
              " ('действий', 'NOUN'),\n",
              " ('исполнителя', 'NOUN'),\n",
              " ('для', 'ADP'),\n",
              " ('достижения', 'NOUN'),\n",
              " ('результата', 'NOUN'),\n",
              " ('решения', 'NOUN'),\n",
              " ('задачи', 'NOUN'),\n",
              " ('за', 'ADP'),\n",
              " ('конечное', 'ADJ'),\n",
              " ('время', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8782863467673677"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "unigram_tagger = UnigramTagger(fdata_train)\n",
        "display(unigram_tagger.tag(example_tag), unigram_tagger.evaluate(fdata_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xqKvDeXNYD-"
      },
      "source": [
        "### BigramTagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "IC9vIBmuNZeR",
        "outputId": "68dd712f-e1fe-4a2b-e9f3-af6ffd79032c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', None),\n",
              " ('аль', None),\n",
              " ('-', 'PUNCT'),\n",
              " ('Хорезми', None),\n",
              " (',', 'PUNCT'),\n",
              " ('-', 'PUNCT'),\n",
              " ('точный', 'ADJ'),\n",
              " ('набор', 'NOUN'),\n",
              " ('инструкций', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('описывающих', 'VERB'),\n",
              " ('порядок', 'NOUN'),\n",
              " ('действий', 'NOUN'),\n",
              " ('исполнителя', None),\n",
              " ('для', 'ADP'),\n",
              " ('достижения', 'NOUN'),\n",
              " ('результата', 'NOUN'),\n",
              " ('решения', 'NOUN'),\n",
              " ('задачи', 'NOUN'),\n",
              " ('за', 'ADP'),\n",
              " ('конечное', None),\n",
              " ('время', None),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.7101308678950452"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "bigram_tagger = BigramTagger(fdata_train)\n",
        "display(bigram_tagger.tag(example_tag), bigram_tagger.evaluate(fdata_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BU1_MFMNwjr"
      },
      "source": [
        "Результат хуже, так как многие биграмы довольно редкие и не встречались в тренировочных данных, поэтому для таких биграм нужно использовать униграмы в качестве backoff:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "s_3RuiVcOMmY",
        "outputId": "07480664-5461-425b-c69f-e04766285ed6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', 'NOUN'),\n",
              " ('аль', 'PART'),\n",
              " ('-', 'PUNCT'),\n",
              " ('Хорезми', None),\n",
              " (',', 'PUNCT'),\n",
              " ('-', 'PUNCT'),\n",
              " ('точный', 'ADJ'),\n",
              " ('набор', 'NOUN'),\n",
              " ('инструкций', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('описывающих', 'VERB'),\n",
              " ('порядок', 'NOUN'),\n",
              " ('действий', 'NOUN'),\n",
              " ('исполнителя', 'NOUN'),\n",
              " ('для', 'ADP'),\n",
              " ('достижения', 'NOUN'),\n",
              " ('результата', 'NOUN'),\n",
              " ('решения', 'NOUN'),\n",
              " ('задачи', 'NOUN'),\n",
              " ('за', 'ADP'),\n",
              " ('конечное', 'ADJ'),\n",
              " ('время', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8839768214076438"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "bigram_tagger = BigramTagger(fdata_train, backoff=unigram_tagger)\n",
        "display(bigram_tagger.tag(example_tag), bigram_tagger.evaluate(fdata_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC05lprROVKr"
      },
      "source": [
        "### TrigramTagger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "tUgf_JJMOdiB",
        "outputId": "a1f24088-605b-40e3-99b0-fcdb03e4061c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', None),\n",
              " (',', None),\n",
              " ('от', 'ADP'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', None),\n",
              " ('аль', None),\n",
              " ('-', 'PUNCT'),\n",
              " ('Хорезми', None),\n",
              " (',', None),\n",
              " ('-', 'PUNCT'),\n",
              " ('точный', None),\n",
              " ('набор', None),\n",
              " ('инструкций', None),\n",
              " (',', 'PUNCT'),\n",
              " ('описывающих', None),\n",
              " ('порядок', None),\n",
              " ('действий', None),\n",
              " ('исполнителя', None),\n",
              " ('для', 'ADP'),\n",
              " ('достижения', None),\n",
              " ('результата', None),\n",
              " ('решения', None),\n",
              " ('задачи', None),\n",
              " ('за', 'ADP'),\n",
              " ('конечное', None),\n",
              " ('время', None),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.4067191874470994"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trigram_tagger = TrigramTagger(fdata_train)\n",
        "display(trigram_tagger.tag(example_tag), trigram_tagger.evaluate(fdata_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkyQDyqTOqDM"
      },
      "source": [
        "Тут такая же ситуация, как и с биграмами, теперь воспользуемся в качестве backoff биграмами, которые в свою очередь имеют backoff униграмы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "nkKuDnyMO6LN",
        "outputId": "b556ba0d-67b6-4e97-ebdf-9901c5be64bd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', None),\n",
              " (',', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', 'NOUN'),\n",
              " ('аль', 'PART'),\n",
              " ('-', 'PUNCT'),\n",
              " ('Хорезми', None),\n",
              " (',', 'PUNCT'),\n",
              " ('-', 'PUNCT'),\n",
              " ('точный', 'ADJ'),\n",
              " ('набор', 'NOUN'),\n",
              " ('инструкций', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('описывающих', 'VERB'),\n",
              " ('порядок', 'NOUN'),\n",
              " ('действий', 'NOUN'),\n",
              " ('исполнителя', 'NOUN'),\n",
              " ('для', 'ADP'),\n",
              " ('достижения', 'NOUN'),\n",
              " ('результата', 'NOUN'),\n",
              " ('решения', 'NOUN'),\n",
              " ('задачи', 'NOUN'),\n",
              " ('за', 'ADP'),\n",
              " ('конечное', 'ADJ'),\n",
              " ('время', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.8830522820496126"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trigram_tagger = TrigramTagger(fdata_train, backoff=bigram_tagger)\n",
        "display(trigram_tagger.tag(example_tag), trigram_tagger.evaluate(fdata_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reJ31QGTPExb"
      },
      "source": [
        "Результаты чуть хуже, чем у биграм с backoff униграм"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE8dRi0HPX7e"
      },
      "source": [
        "### Комбинация тэггеров"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "id": "qxU_GhsVPeeN",
        "outputId": "9d3a6fd0-3351-4d02-dbdf-3e3b80933ca8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "[('Алгоритм', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('от', 'ADP'),\n",
              " ('имени', 'NOUN'),\n",
              " ('учёного', 'NOUN'),\n",
              " ('аль', 'PART'),\n",
              " ('-', 'PUNCT'),\n",
              " ('Хорезми', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('-', 'PUNCT'),\n",
              " ('точный', 'ADJ'),\n",
              " ('набор', 'NOUN'),\n",
              " ('инструкций', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('описывающих', 'VERB'),\n",
              " ('порядок', 'NOUN'),\n",
              " ('действий', 'NOUN'),\n",
              " ('исполнителя', 'NOUN'),\n",
              " ('для', 'ADP'),\n",
              " ('достижения', 'NOUN'),\n",
              " ('результата', 'NOUN'),\n",
              " ('решения', 'NOUN'),\n",
              " ('задачи', 'NOUN'),\n",
              " ('за', 'ADP'),\n",
              " ('конечное', 'ADJ'),\n",
              " ('время', 'NOUN'),\n",
              " ('.', 'PUNCT')]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0.9119799466111075"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
        "    for cls in tagger_classes:\n",
        "        backoff = cls(train_sents, backoff=backoff)\n",
        "    return backoff\n",
        "\n",
        "\n",
        "backoff = DefaultTagger('NOUN') \n",
        "tag = backoff_tagger(fdata_train,  \n",
        "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
        "                     backoff = backoff) \n",
        "  \n",
        "display(tag.tag(example_tag), tag.evaluate(fdata_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_C6LiBPShWm"
      },
      "source": [
        "Результат получился лучше всех, так как оказывается, что даже некоторых слов не было в тренировочных данных, поэтому при добавлении DefaultTagger('NOUN'), который неизвестным словам ставит в соответсвие сущиствительное, так как это предположительно самая частая часть речи. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3Clqht4TZqZ"
      },
      "source": [
        "### Свой тэггер"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nrub6lz0T5hZ"
      },
      "source": [
        "Напишем нейронную сеть"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLADGN7zjg8g"
      },
      "source": [
        "Предобработка данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIxXf14-T2_k"
      },
      "outputs": [],
      "source": [
        "train_tok = []\n",
        "train_label = []\n",
        "for sent in fdata_train[:]:\n",
        "    norm_sent = []\n",
        "    label_sent = []\n",
        "    for tok in sent:\n",
        "        if (tok[0] is None) or (tok[1] is None):\n",
        "            continue\n",
        "        norm_sent.append(tok[0])\n",
        "        label_sent.append(tok[1])\n",
        "    train_tok.append(norm_sent)\n",
        "    train_label.append(label_sent)\n",
        "        \n",
        "test_tok = []\n",
        "test_label = []\n",
        "for sent in fdata_test[:]:\n",
        "    norm_sent = []\n",
        "    label_sent = []\n",
        "    for tok in sent:\n",
        "        if (tok[0] is None) or (tok[1] is None):\n",
        "            continue\n",
        "        norm_sent.append(tok[0])\n",
        "        label_sent.append(tok[1])\n",
        "    test_tok.append(norm_sent)\n",
        "    test_label.append(label_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teBzdGgXcU_F",
        "outputId": "f84e1c9f-3236-46a4-f956-14427215797c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Анкета', '.', 'Начальник', 'областного', 'управления']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "corpus = [token for sent in train_tok for token in sent]\n",
        "corpus[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm-N3tvJdpjz",
        "outputId": "538f7a86-1b84-4c79-94f8-d32f9efdac85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 97978),\n",
              " ('.', 64114),\n",
              " ('в', 30986),\n",
              " ('и', 30064),\n",
              " ('\"', 21639),\n",
              " ('-', 19280),\n",
              " ('не', 15028),\n",
              " ('на', 14637),\n",
              " ('что', 10590),\n",
              " ('с', 9559)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "freq_dict = Counter(corpus)\n",
        "freq_dict_sorted = sorted(freq_dict.items(), key=lambda x: -x[1])\n",
        "list(freq_dict_sorted)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoWy1ZXOd2AO",
        "outputId": "9f5a369e-bb8d-4545-e30a-9fb7f20ac45e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "132847"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "vocabulary_size = len(freq_dict)\n",
        "vocabulary_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdSVvkawd5A9",
        "outputId": "e9f9c2fd-3841-4660-be44-d3b5fc92773e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "132847"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "VOCABULARY_SIZE = vocabulary_size\n",
        "VOCABULARY_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvTOWGbrd581",
        "outputId": "fb76c873-2037-4051-9fc2-46f1aad29bc0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[',', '.', 'в', 'и', '\"']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "top_n_words = [word for word, freq in freq_dict_sorted[:VOCABULARY_SIZE]]\n",
        "top_n_words[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9CJNv29d_Ht",
        "outputId": "e3a3f5a7-060d-4514-e1de-7c2b344e7715"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1), ('.', 2), ('в', 3), ('и', 4), ('\"', 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "vocab = {y:x for x,y in enumerate(top_n_words, start=1)}\n",
        "list(vocab.items())[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gnUkbNdeNWH",
        "outputId": "44eac020-947f-4d8d-a7b4-3293f28855b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Анкета', '.']\n",
            "[39888, 2]\n"
          ]
        }
      ],
      "source": [
        "train_tok_num = [[vocab.get(i, 0) for i in j] for j in train_tok]\n",
        "test_tok_num = [[vocab.get(i, 0) for i in j] for j in test_tok]\n",
        "print(train_tok[0])\n",
        "print(train_tok_num[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoV1uPUheDK3",
        "outputId": "4ddae41f-8e78-4c34-9e25-54b4c53bfa13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "max_len = max(len(sent) for sent in train_tok_num)\n",
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJpZKfm7goA1",
        "outputId": "23783d0a-df37-430d-da08-70a28610dfde"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "205"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "MAX_LEN = max_len\n",
        "MAX_LEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsGZCZIhgrCW",
        "outputId": "92e42ca5-8280-4b08-c5e7-ac0fbeaad62f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[39888, 2, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "x_train_nn = [i+[0]*(MAX_LEN-len(i)) for i in train_tok_num]\n",
        "x_test_nn = [i+[0]*(MAX_LEN-len(i)) for i in test_tok_num]\n",
        "x_train_nn[0][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ky6y_UbZhuPU",
        "outputId": "96d70c01-8d86-477a-cb9f-662ba604f4e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([39888,     2,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "x_train_nn_array = np.array([[j for j in i] for i in x_train_nn])\n",
        "x_test_nn_array = np.array([[j for j in i] for i in x_test_nn])\n",
        "x_train_nn_array[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXFk0awliKaV"
      },
      "outputs": [],
      "source": [
        "tags = ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp-lapydiD1s",
        "outputId": "b50910c1-a2ca-4ec1-e64f-6506cd9ee661"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'ADJ',\n",
              " 2: 'ADP',\n",
              " 3: 'ADV',\n",
              " 4: 'AUX',\n",
              " 5: 'CCONJ',\n",
              " 6: 'DET',\n",
              " 7: 'INTJ',\n",
              " 8: 'NOUN',\n",
              " 9: 'NUM',\n",
              " 10: 'PART',\n",
              " 11: 'PRON',\n",
              " 12: 'PROPN',\n",
              " 13: 'PUNCT',\n",
              " 14: 'SCONJ',\n",
              " 15: 'SYM',\n",
              " 16: 'VERB',\n",
              " 17: 'X'}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "num2tag = dict(zip(range(1, len(tags)+1), tags))\n",
        "num2tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKbN3VQrivDB",
        "outputId": "c6b6c43e-b9d3-4247-9632-d27d5b9f5344"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ADJ': 1,\n",
              " 'ADP': 2,\n",
              " 'ADV': 3,\n",
              " 'AUX': 4,\n",
              " 'CCONJ': 5,\n",
              " 'DET': 6,\n",
              " 'INTJ': 7,\n",
              " 'NOUN': 8,\n",
              " 'NUM': 9,\n",
              " 'PART': 10,\n",
              " 'PRON': 11,\n",
              " 'PROPN': 12,\n",
              " 'PUNCT': 13,\n",
              " 'SCONJ': 14,\n",
              " 'SYM': 15,\n",
              " 'VERB': 16,\n",
              " 'X': 17}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "tag2num = {j:i for i,j in num2tag.items()}\n",
        "tag2num"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OACBvzDEh-vq",
        "outputId": "67cd9fc4-a46e-459d-aaf6-aa7dbb9ca6d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[8, 13, 0, 0, 0, 0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "y_train_nn = [[tag2num[i] for i in j]+[0]*(MAX_LEN-len(j)) for j in train_label]\n",
        "y_test_nn = [[tag2num[i] for i in j]+[0]*(MAX_LEN-len(j)) for j in test_label]\n",
        "y_train_nn[0][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkgWUqg8kt1W",
        "outputId": "ad37c246-7098-44a8-ad2f-ded7319ad2da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8, 13,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "y_train_nn_array = np.array([[j for j in i] for i in y_train_nn])\n",
        "y_test_nn_array = np.array([[j for j in i] for i in y_test_nn])\n",
        "y_train_nn_array[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBKSHk-cjnMf"
      },
      "source": [
        "Модель:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXa8wjIBWPmM"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 64\n",
        "hidden_dim = 32\n",
        "dropout = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим предложения целиком и будем предсказывать сразу много меток, а не одну, как если бы мы передовали по одному слову.\n",
        "\n",
        "Включим lstm слой, с помощью его, модель должна улавливать контекст и преобретать логику, а не просто смотреть на частоту встречания части речи и данного слова\n"
      ],
      "metadata": {
        "id": "BlER_D0qMRit"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAXRFFUfWS1o"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Embedding(VOCABULARY_SIZE+1, embedding_dim), # VOCABULARY_SIZE+1 так как еще 0, который показывает где нет слова\n",
        "    BatchNormalization(),\n",
        "    Dropout(dropout),\n",
        "\n",
        "    LSTM(embedding_dim, return_sequences=True),\n",
        "    BatchNormalization(),\n",
        "    Dropout(dropout),\n",
        "    Activation(activations.relu),\n",
        "\n",
        "    Dense(hidden_dim, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dropout(dropout),\n",
        "\n",
        "    Dense(len(tags)+1, activation='softmax') # len(tags)+1 так как еще 0, который показывает где нет тега\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H2DF2KR715Q",
        "outputId": "5e8f63e7-5074-4138-94a7-90105e14d1d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 64)          8502272   \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, None, 64)         256       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, None, 64)          0         \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, None, 64)          33024     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, None, 64)         256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, None, 64)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, None, 64)          0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 32)          2080      \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, None, 32)         128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, None, 32)          0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, None, 18)          594       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,538,610\n",
            "Trainable params: 8,538,290\n",
            "Non-trainable params: 320\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UCCqLyQWURP"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3qvRityWVik",
        "outputId": "06fa30dc-ac07-4a42-eadc-5295b1cf0596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "136/136 [==============================] - 14s 41ms/step - loss: 1.2960 - accuracy: 0.8095\n",
            "Epoch 2/50\n",
            "136/136 [==============================] - 6s 40ms/step - loss: 0.1926 - accuracy: 0.9744\n",
            "Epoch 3/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0688 - accuracy: 0.9888\n",
            "Epoch 4/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0355 - accuracy: 0.9936\n",
            "Epoch 5/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0241 - accuracy: 0.9953\n",
            "Epoch 6/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0187 - accuracy: 0.9960\n",
            "Epoch 7/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0156 - accuracy: 0.9965\n",
            "Epoch 8/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0135 - accuracy: 0.9967\n",
            "Epoch 9/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0120 - accuracy: 0.9970\n",
            "Epoch 10/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0108 - accuracy: 0.9972\n",
            "Epoch 11/50\n",
            "136/136 [==============================] - 6s 43ms/step - loss: 0.0099 - accuracy: 0.9973\n",
            "Epoch 12/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0091 - accuracy: 0.9975\n",
            "Epoch 13/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0085 - accuracy: 0.9976\n",
            "Epoch 14/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0079 - accuracy: 0.9977\n",
            "Epoch 15/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0074 - accuracy: 0.9979\n",
            "Epoch 16/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0070 - accuracy: 0.9980\n",
            "Epoch 17/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0066 - accuracy: 0.9981\n",
            "Epoch 18/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0062 - accuracy: 0.9982\n",
            "Epoch 19/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0058 - accuracy: 0.9982\n",
            "Epoch 20/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0055 - accuracy: 0.9983\n",
            "Epoch 21/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0052 - accuracy: 0.9984\n",
            "Epoch 22/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0049 - accuracy: 0.9985\n",
            "Epoch 23/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0046 - accuracy: 0.9986\n",
            "Epoch 24/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0043 - accuracy: 0.9987\n",
            "Epoch 25/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0041 - accuracy: 0.9988\n",
            "Epoch 26/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0039 - accuracy: 0.9988\n",
            "Epoch 27/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0037 - accuracy: 0.9989\n",
            "Epoch 28/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0035 - accuracy: 0.9989\n",
            "Epoch 29/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0034 - accuracy: 0.9990\n",
            "Epoch 30/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0032 - accuracy: 0.9990\n",
            "Epoch 31/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0031 - accuracy: 0.9990\n",
            "Epoch 32/50\n",
            "136/136 [==============================] - 6s 43ms/step - loss: 0.0030 - accuracy: 0.9991\n",
            "Epoch 33/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0029 - accuracy: 0.9991\n",
            "Epoch 34/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0028 - accuracy: 0.9991\n",
            "Epoch 35/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0027 - accuracy: 0.9992\n",
            "Epoch 36/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0026 - accuracy: 0.9992\n",
            "Epoch 37/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0025 - accuracy: 0.9992\n",
            "Epoch 38/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0024 - accuracy: 0.9992\n",
            "Epoch 39/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0024 - accuracy: 0.9992\n",
            "Epoch 40/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0023 - accuracy: 0.9992\n",
            "Epoch 41/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0023 - accuracy: 0.9993\n",
            "Epoch 42/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0022 - accuracy: 0.9993\n",
            "Epoch 43/50\n",
            "136/136 [==============================] - 6s 44ms/step - loss: 0.0022 - accuracy: 0.9993\n",
            "Epoch 44/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0021 - accuracy: 0.9993\n",
            "Epoch 45/50\n",
            "136/136 [==============================] - 6s 41ms/step - loss: 0.0021 - accuracy: 0.9993\n",
            "Epoch 46/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0020 - accuracy: 0.9993\n",
            "Epoch 47/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0020 - accuracy: 0.9993\n",
            "Epoch 48/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0020 - accuracy: 0.9993\n",
            "Epoch 49/50\n",
            "136/136 [==============================] - 6s 42ms/step - loss: 0.0019 - accuracy: 0.9993\n",
            "Epoch 50/50\n",
            "136/136 [==============================] - 6s 44ms/step - loss: 0.0019 - accuracy: 0.9994\n"
          ]
        }
      ],
      "source": [
        "with tf.device(\"GPU:0\"):\n",
        "  hh = model.fit(x_train_nn_array, y_train_nn_array, epochs=50, batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Здесь вычисляется accuracy с учётом нулей, нас это не устраивает, поэтому вычислим заново, а так же вычислим для test:"
      ],
      "metadata": {
        "id": "DFeop09ENVAR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIkfbYr91fU_",
        "outputId": "103fa5a8-8a27-43d5-af3d-165dc5d92a36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4.29459996e-30, 2.03175379e-08, 6.78931356e-10, 5.78906212e-09,\n",
              "        2.08379702e-10, 1.27422051e-09, 2.18958546e-10, 1.86185767e-09,\n",
              "        9.99999166e-01, 6.64112108e-08, 1.93452129e-11, 2.76208206e-11,\n",
              "        7.23405094e-07, 8.41172479e-11, 1.23960953e-09, 1.22484565e-08,\n",
              "        2.63030664e-09, 2.00094519e-08],\n",
              "       [0.00000000e+00, 6.61685859e-17, 1.72875139e-19, 2.75030595e-13,\n",
              "        8.31908056e-14, 4.11649447e-13, 1.51137552e-13, 1.04265235e-10,\n",
              "        3.90433400e-15, 6.01206064e-12, 1.81535593e-14, 2.14096007e-13,\n",
              "        5.79462606e-12, 1.00000000e+00, 3.46063907e-11, 5.76627081e-15,\n",
              "        2.44992806e-12, 2.65939465e-10]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "nn_preds_train = model.predict(x_train_nn_array, verbose=1, batch_size=512)\n",
        "nn_preds_train[0][:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcXwISqK0w2B",
        "outputId": "6905d8a0-eadd-42b8-e82a-15926ad863ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9936772111587017"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "acc_train = 0\n",
        "for i in range(len(x_train_nn_array)):\n",
        "  len_sent = len(full_train[i])\n",
        "  acc_train += sum(np.argmax(nn_preds_train[i][:len_sent], axis=1) == y_train_nn_array[i][:len_sent])/len_sent\n",
        "acc_train/len(x_train_nn_array)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_preds_test = model.predict(x_test_nn_array, verbose=1, batch_size=512)\n",
        "nn_preds_test[0][:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo5hIStzVLJ_",
        "outputId": "1a717b28-c450-4681-eb20-741b30a3b099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.9787211e-01, 9.9606645e-05, 6.7013963e-05, 3.3477211e-04,\n",
              "        1.5539853e-04, 9.9225312e-05, 2.4556992e-05, 7.8378971e-06,\n",
              "        1.4982490e-04, 1.2260582e-04, 2.1979339e-04, 1.9358009e-05,\n",
              "        1.6564654e-05, 1.0609691e-05, 8.3820749e-05, 4.0918429e-05,\n",
              "        6.7130697e-04, 4.6177888e-06],\n",
              "       [0.0000000e+00, 3.7417376e-14, 1.4636751e-16, 1.4374426e-11,\n",
              "        5.6919526e-12, 2.8832406e-10, 1.0472878e-11, 1.9781545e-09,\n",
              "        2.4050903e-13, 1.8677841e-10, 6.7347300e-12, 5.1299073e-12,\n",
              "        4.2676338e-11, 1.0000000e+00, 1.7400141e-09, 9.2168916e-13,\n",
              "        3.7408084e-11, 1.8618272e-09]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQQi2E3b5BqT",
        "outputId": "2413652d-43c3-4e1d-9ad2-bc8c811d9535"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8962984384739782"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "acc_test = 0\n",
        "for i in range(len(x_test_nn_array)):\n",
        "  len_sent = len(full_test[i])\n",
        "  acc_test += sum(np.argmax(nn_preds_test[i][:len_sent], axis=1) == y_test_nn_array[i][:len_sent])/len_sent\n",
        "acc_test/len(x_test_nn_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вывод:\n",
        "\n",
        "Лучше всех отработал комбинированный теггер, нейронка тоже неплохо справилась, но при этом для неё нужно подготовить данные, построить модель, обучить и предсказать, это всё оказалось гораааздо дольше, чем раюотать с теггерами из коробки, хотя при этом, комбинированный теггер показал метрику даже лучше."
      ],
      "metadata": {
        "id": "2TzOoRhnVrwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2"
      ],
      "metadata": {
        "id": "2mArIhphWOVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Данные:"
      ],
      "metadata": {
        "id": "ZYjq2ocQWha7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://www.labinform.ru/pub/named_entities/collection5.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i14iABZUWU83",
        "outputId": "77f2f7e4-c140-4b8a-fd2a-51ee4983ef36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-05 17:56:37--  http://www.labinform.ru/pub/named_entities/collection5.zip\n",
            "Resolving www.labinform.ru (www.labinform.ru)... 95.181.230.181\n",
            "Connecting to www.labinform.ru (www.labinform.ru)|95.181.230.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1899530 (1.8M) [application/zip]\n",
            "Saving to: ‘collection5.zip’\n",
            "\n",
            "collection5.zip     100%[===================>]   1.81M  2.30MB/s    in 0.8s    \n",
            "\n",
            "2022-09-05 17:56:38 (2.30 MB/s) - ‘collection5.zip’ saved [1899530/1899530]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip collection5.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "589cK8LjWsv-",
        "outputId": "5723ab45-3e9c-4a46-c18b-ea5c369f0e74"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  collection5.zip\n",
            "   creating: Collection5/\n",
            "  inflating: Collection5/001.ann     \n",
            "  inflating: Collection5/001.txt     \n",
            "  inflating: Collection5/002.ann     \n",
            "  inflating: Collection5/002.txt     \n",
            "  inflating: Collection5/003.ann     \n",
            "  inflating: Collection5/003.txt     \n",
            "  inflating: Collection5/004.ann     \n",
            "  inflating: Collection5/004.txt     \n",
            "  inflating: Collection5/005.ann     \n",
            "  inflating: Collection5/005.txt     \n",
            "  inflating: Collection5/006.ann     \n",
            "  inflating: Collection5/006.txt     \n",
            "  inflating: Collection5/007.ann     \n",
            "  inflating: Collection5/007.txt     \n",
            "  inflating: Collection5/008.ann     \n",
            "  inflating: Collection5/008.txt     \n",
            "  inflating: Collection5/009.ann     \n",
            "  inflating: Collection5/009.txt     \n",
            "  inflating: Collection5/010.ann     \n",
            "  inflating: Collection5/010.txt     \n",
            "  inflating: Collection5/011.ann     \n",
            "  inflating: Collection5/011.txt     \n",
            "  inflating: Collection5/012.ann     \n",
            "  inflating: Collection5/012.txt     \n",
            "  inflating: Collection5/013.ann     \n",
            "  inflating: Collection5/013.txt     \n",
            "  inflating: Collection5/014.ann     \n",
            "  inflating: Collection5/014.txt     \n",
            "  inflating: Collection5/015 (!).ann  \n",
            "  inflating: Collection5/015 (!).txt  \n",
            "  inflating: Collection5/016.ann     \n",
            "  inflating: Collection5/016.txt     \n",
            "  inflating: Collection5/017.ann     \n",
            "  inflating: Collection5/017.txt     \n",
            "  inflating: Collection5/018.ann     \n",
            "  inflating: Collection5/018.txt     \n",
            "  inflating: Collection5/019.ann     \n",
            "  inflating: Collection5/019.txt     \n",
            "  inflating: Collection5/020.ann     \n",
            "  inflating: Collection5/020.txt     \n",
            "  inflating: Collection5/021.ann     \n",
            "  inflating: Collection5/021.txt     \n",
            "  inflating: Collection5/022.ann     \n",
            "  inflating: Collection5/022.txt     \n",
            "  inflating: Collection5/023.ann     \n",
            "  inflating: Collection5/023.txt     \n",
            "  inflating: Collection5/025.ann     \n",
            "  inflating: Collection5/025.txt     \n",
            "  inflating: Collection5/026.ann     \n",
            "  inflating: Collection5/026.txt     \n",
            "  inflating: Collection5/027.ann     \n",
            "  inflating: Collection5/027.txt     \n",
            "  inflating: Collection5/028.ann     \n",
            "  inflating: Collection5/028.txt     \n",
            "  inflating: Collection5/029.ann     \n",
            "  inflating: Collection5/029.txt     \n",
            "  inflating: Collection5/030.ann     \n",
            "  inflating: Collection5/030.txt     \n",
            "  inflating: Collection5/031.ann     \n",
            "  inflating: Collection5/031.txt     \n",
            "  inflating: Collection5/032.ann     \n",
            "  inflating: Collection5/032.txt     \n",
            "  inflating: Collection5/033.ann     \n",
            "  inflating: Collection5/033.txt     \n",
            "  inflating: Collection5/034.ann     \n",
            "  inflating: Collection5/034.txt     \n",
            "  inflating: Collection5/035.ann     \n",
            "  inflating: Collection5/035.txt     \n",
            "  inflating: Collection5/036.ann     \n",
            "  inflating: Collection5/036.txt     \n",
            "  inflating: Collection5/037.ann     \n",
            "  inflating: Collection5/037.txt     \n",
            "  inflating: Collection5/038.ann     \n",
            "  inflating: Collection5/038.txt     \n",
            "  inflating: Collection5/039.ann     \n",
            "  inflating: Collection5/039.txt     \n",
            "  inflating: Collection5/03_12_12a.ann  \n",
            "  inflating: Collection5/03_12_12a.txt  \n",
            "  inflating: Collection5/03_12_12b.ann  \n",
            "  inflating: Collection5/03_12_12b.txt  \n",
            "  inflating: Collection5/03_12_12c.ann  \n",
            "  inflating: Collection5/03_12_12c.txt  \n",
            "  inflating: Collection5/03_12_12d.ann  \n",
            "  inflating: Collection5/03_12_12d.txt  \n",
            "  inflating: Collection5/03_12_12g.ann  \n",
            "  inflating: Collection5/03_12_12g.txt  \n",
            "  inflating: Collection5/03_12_12h.ann  \n",
            "  inflating: Collection5/03_12_12h.txt  \n",
            "  inflating: Collection5/040.ann     \n",
            "  inflating: Collection5/040.txt     \n",
            "  inflating: Collection5/041.ann     \n",
            "  inflating: Collection5/041.txt     \n",
            "  inflating: Collection5/042.ann     \n",
            "  inflating: Collection5/042.txt     \n",
            "  inflating: Collection5/043.ann     \n",
            "  inflating: Collection5/043.txt     \n",
            "  inflating: Collection5/044.ann     \n",
            "  inflating: Collection5/044.txt     \n",
            "  inflating: Collection5/045.ann     \n",
            "  inflating: Collection5/045.txt     \n",
            "  inflating: Collection5/046.ann     \n",
            "  inflating: Collection5/046.txt     \n",
            "  inflating: Collection5/047.ann     \n",
            "  inflating: Collection5/047.txt     \n",
            "  inflating: Collection5/048.ann     \n",
            "  inflating: Collection5/048.txt     \n",
            "  inflating: Collection5/049.ann     \n",
            "  inflating: Collection5/049.txt     \n",
            "  inflating: Collection5/04_02_13a_abdulatipov.ann  \n",
            "  inflating: Collection5/04_02_13a_abdulatipov.txt  \n",
            "  inflating: Collection5/04_03_13a_sorokin.ann  \n",
            "  inflating: Collection5/04_03_13a_sorokin.txt  \n",
            "  inflating: Collection5/04_12_12b.ann  \n",
            "  inflating: Collection5/04_12_12b.txt  \n",
            "  inflating: Collection5/04_12_12d.ann  \n",
            "  inflating: Collection5/04_12_12d.txt  \n",
            "  inflating: Collection5/04_12_12f.ann  \n",
            "  inflating: Collection5/04_12_12f.txt  \n",
            "  inflating: Collection5/04_12_12g.ann  \n",
            "  inflating: Collection5/04_12_12g.txt  \n",
            "  inflating: Collection5/04_12_12h_corr.ann  \n",
            "  inflating: Collection5/04_12_12h_corr.txt  \n",
            "  inflating: Collection5/050.ann     \n",
            "  inflating: Collection5/050.txt     \n",
            "  inflating: Collection5/051.ann     \n",
            "  inflating: Collection5/051.txt     \n",
            "  inflating: Collection5/052.ann     \n",
            "  inflating: Collection5/052.txt     \n",
            "  inflating: Collection5/053.ann     \n",
            "  inflating: Collection5/053.txt     \n",
            "  inflating: Collection5/054.ann     \n",
            "  inflating: Collection5/054.txt     \n",
            "  inflating: Collection5/055.ann     \n",
            "  inflating: Collection5/055.txt     \n",
            "  inflating: Collection5/056.ann     \n",
            "  inflating: Collection5/056.txt     \n",
            "  inflating: Collection5/057.ann     \n",
            "  inflating: Collection5/057.txt     \n",
            "  inflating: Collection5/058.ann     \n",
            "  inflating: Collection5/058.txt     \n",
            "  inflating: Collection5/059.ann     \n",
            "  inflating: Collection5/059.txt     \n",
            "  inflating: Collection5/060.ann     \n",
            "  inflating: Collection5/060.txt     \n",
            "  inflating: Collection5/061.ann     \n",
            "  inflating: Collection5/061.txt     \n",
            "  inflating: Collection5/062.ann     \n",
            "  inflating: Collection5/062.txt     \n",
            "  inflating: Collection5/063.ann     \n",
            "  inflating: Collection5/063.txt     \n",
            "  inflating: Collection5/064.ann     \n",
            "  inflating: Collection5/064.txt     \n",
            "  inflating: Collection5/065.ann     \n",
            "  inflating: Collection5/065.txt     \n",
            "  inflating: Collection5/066.ann     \n",
            "  inflating: Collection5/066.txt     \n",
            "  inflating: Collection5/067.ann     \n",
            "  inflating: Collection5/067.txt     \n",
            "  inflating: Collection5/068.ann     \n",
            "  inflating: Collection5/068.txt     \n",
            "  inflating: Collection5/069.ann     \n",
            "  inflating: Collection5/069.txt     \n",
            "  inflating: Collection5/070.ann     \n",
            "  inflating: Collection5/070.txt     \n",
            "  inflating: Collection5/071.ann     \n",
            "  inflating: Collection5/071.txt     \n",
            "  inflating: Collection5/072.ann     \n",
            "  inflating: Collection5/072.txt     \n",
            "  inflating: Collection5/073.ann     \n",
            "  inflating: Collection5/073.txt     \n",
            "  inflating: Collection5/074.ann     \n",
            "  inflating: Collection5/074.txt     \n",
            "  inflating: Collection5/075.ann     \n",
            "  inflating: Collection5/075.txt     \n",
            "  inflating: Collection5/076.ann     \n",
            "  inflating: Collection5/076.txt     \n",
            "  inflating: Collection5/077.ann     \n",
            "  inflating: Collection5/077.txt     \n",
            "  inflating: Collection5/078.ann     \n",
            "  inflating: Collection5/078.txt     \n",
            "  inflating: Collection5/079.ann     \n",
            "  inflating: Collection5/079.txt     \n",
            "  inflating: Collection5/080.ann     \n",
            "  inflating: Collection5/080.txt     \n",
            "  inflating: Collection5/081.ann     \n",
            "  inflating: Collection5/081.txt     \n",
            "  inflating: Collection5/082.ann     \n",
            "  inflating: Collection5/082.txt     \n",
            "  inflating: Collection5/083.ann     \n",
            "  inflating: Collection5/083.txt     \n",
            "  inflating: Collection5/084.ann     \n",
            "  inflating: Collection5/084.txt     \n",
            "  inflating: Collection5/085.ann     \n",
            "  inflating: Collection5/085.txt     \n",
            "  inflating: Collection5/086.ann     \n",
            "  inflating: Collection5/086.txt     \n",
            "  inflating: Collection5/087.ann     \n",
            "  inflating: Collection5/087.txt     \n",
            "  inflating: Collection5/088.ann     \n",
            "  inflating: Collection5/088.txt     \n",
            "  inflating: Collection5/089.ann     \n",
            "  inflating: Collection5/089.txt     \n",
            "  inflating: Collection5/090.ann     \n",
            "  inflating: Collection5/090.txt     \n",
            "  inflating: Collection5/091.ann     \n",
            "  inflating: Collection5/091.txt     \n",
            "  inflating: Collection5/092.ann     \n",
            "  inflating: Collection5/092.txt     \n",
            "  inflating: Collection5/093.ann     \n",
            "  inflating: Collection5/093.txt     \n",
            "  inflating: Collection5/094.ann     \n",
            "  inflating: Collection5/094.txt     \n",
            "  inflating: Collection5/095.ann     \n",
            "  inflating: Collection5/095.txt     \n",
            "  inflating: Collection5/096.ann     \n",
            "  inflating: Collection5/096.txt     \n",
            "  inflating: Collection5/097.ann     \n",
            "  inflating: Collection5/097.txt     \n",
            "  inflating: Collection5/098.ann     \n",
            "  inflating: Collection5/098.txt     \n",
            "  inflating: Collection5/099.ann     \n",
            "  inflating: Collection5/099.txt     \n",
            "  inflating: Collection5/09_01_13.ann  \n",
            "  inflating: Collection5/09_01_13.txt  \n",
            "  inflating: Collection5/09_01_13a.ann  \n",
            "  inflating: Collection5/09_01_13a.txt  \n",
            "  inflating: Collection5/09_01_13c.ann  \n",
            "  inflating: Collection5/09_01_13c.txt  \n",
            "  inflating: Collection5/09_01_13d.ann  \n",
            "  inflating: Collection5/09_01_13d.txt  \n",
            "  inflating: Collection5/09_01_13e.ann  \n",
            "  inflating: Collection5/09_01_13e.txt  \n",
            "  inflating: Collection5/09_01_13h.ann  \n",
            "  inflating: Collection5/09_01_13h.txt  \n",
            "  inflating: Collection5/09_01_13i.ann  \n",
            "  inflating: Collection5/09_01_13i.txt  \n",
            "  inflating: Collection5/100.ann     \n",
            "  inflating: Collection5/100.txt     \n",
            "  inflating: Collection5/1000.ann    \n",
            "  inflating: Collection5/1000.txt    \n",
            "  inflating: Collection5/1001.ann    \n",
            "  inflating: Collection5/1001.txt    \n",
            "  inflating: Collection5/1002.ann    \n",
            "  inflating: Collection5/1002.txt    \n",
            "  inflating: Collection5/1003.ann    \n",
            "  inflating: Collection5/1003.txt    \n",
            "  inflating: Collection5/1004.ann    \n",
            "  inflating: Collection5/1004.txt    \n",
            "  inflating: Collection5/1005.ann    \n",
            "  inflating: Collection5/1005.txt    \n",
            "  inflating: Collection5/1006.ann    \n",
            "  inflating: Collection5/1006.txt    \n",
            "  inflating: Collection5/1007.ann    \n",
            "  inflating: Collection5/1007.txt    \n",
            "  inflating: Collection5/1008.ann    \n",
            "  inflating: Collection5/1008.txt    \n",
            "  inflating: Collection5/1009.ann    \n",
            "  inflating: Collection5/1009.txt    \n",
            "  inflating: Collection5/101.ann     \n",
            "  inflating: Collection5/101.txt     \n",
            "  inflating: Collection5/1010.ann    \n",
            "  inflating: Collection5/1010.txt    \n",
            "  inflating: Collection5/1011.ann    \n",
            "  inflating: Collection5/1011.txt    \n",
            "  inflating: Collection5/1012.ann    \n",
            "  inflating: Collection5/1012.txt    \n",
            "  inflating: Collection5/1013.ann    \n",
            "  inflating: Collection5/1013.txt    \n",
            "  inflating: Collection5/1014.ann    \n",
            "  inflating: Collection5/1014.txt    \n",
            "  inflating: Collection5/1015.ann    \n",
            "  inflating: Collection5/1015.txt    \n",
            "  inflating: Collection5/1016.ann    \n",
            "  inflating: Collection5/1016.txt    \n",
            "  inflating: Collection5/1017.ann    \n",
            "  inflating: Collection5/1017.txt    \n",
            "  inflating: Collection5/1018.ann    \n",
            "  inflating: Collection5/1018.txt    \n",
            "  inflating: Collection5/1019.ann    \n",
            "  inflating: Collection5/1019.txt    \n",
            "  inflating: Collection5/102.ann     \n",
            "  inflating: Collection5/102.txt     \n",
            "  inflating: Collection5/1020.ann    \n",
            "  inflating: Collection5/1020.txt    \n",
            "  inflating: Collection5/1021.ann    \n",
            "  inflating: Collection5/1021.txt    \n",
            "  inflating: Collection5/1022.ann    \n",
            "  inflating: Collection5/1022.txt    \n",
            "  inflating: Collection5/1023.ann    \n",
            "  inflating: Collection5/1023.txt    \n",
            "  inflating: Collection5/1024.ann    \n",
            "  inflating: Collection5/1024.txt    \n",
            "  inflating: Collection5/1025.ann    \n",
            "  inflating: Collection5/1025.txt    \n",
            "  inflating: Collection5/1026.ann    \n",
            "  inflating: Collection5/1026.txt    \n",
            "  inflating: Collection5/1027.ann    \n",
            "  inflating: Collection5/1027.txt    \n",
            "  inflating: Collection5/1028.ann    \n",
            "  inflating: Collection5/1028.txt    \n",
            "  inflating: Collection5/1029.ann    \n",
            "  inflating: Collection5/1029.txt    \n",
            "  inflating: Collection5/103.ann     \n",
            "  inflating: Collection5/103.txt     \n",
            "  inflating: Collection5/1030.ann    \n",
            "  inflating: Collection5/1030.txt    \n",
            "  inflating: Collection5/1031.ann    \n",
            "  inflating: Collection5/1031.txt    \n",
            "  inflating: Collection5/1032.ann    \n",
            "  inflating: Collection5/1032.txt    \n",
            "  inflating: Collection5/1033.ann    \n",
            "  inflating: Collection5/1033.txt    \n",
            "  inflating: Collection5/1034.ann    \n",
            "  inflating: Collection5/1034.txt    \n",
            "  inflating: Collection5/1035.ann    \n",
            "  inflating: Collection5/1035.txt    \n",
            "  inflating: Collection5/1036.ann    \n",
            "  inflating: Collection5/1036.txt    \n",
            "  inflating: Collection5/1037.ann    \n",
            "  inflating: Collection5/1037.txt    \n",
            "  inflating: Collection5/1038.ann    \n",
            "  inflating: Collection5/1038.txt    \n",
            "  inflating: Collection5/1039.ann    \n",
            "  inflating: Collection5/1039.txt    \n",
            "  inflating: Collection5/104.ann     \n",
            "  inflating: Collection5/104.txt     \n",
            "  inflating: Collection5/1040.ann    \n",
            "  inflating: Collection5/1040.txt    \n",
            "  inflating: Collection5/1041.ann    \n",
            "  inflating: Collection5/1041.txt    \n",
            "  inflating: Collection5/1042.ann    \n",
            "  inflating: Collection5/1042.txt    \n",
            "  inflating: Collection5/1043.ann    \n",
            "  inflating: Collection5/1043.txt    \n",
            "  inflating: Collection5/1044.ann    \n",
            "  inflating: Collection5/1044.txt    \n",
            "  inflating: Collection5/1045.ann    \n",
            "  inflating: Collection5/1045.txt    \n",
            "  inflating: Collection5/1046.ann    \n",
            "  inflating: Collection5/1046.txt    \n",
            "  inflating: Collection5/1047.ann    \n",
            "  inflating: Collection5/1047.txt    \n",
            "  inflating: Collection5/1048.ann    \n",
            "  inflating: Collection5/1048.txt    \n",
            "  inflating: Collection5/1049.ann    \n",
            "  inflating: Collection5/1049.txt    \n",
            "  inflating: Collection5/105.ann     \n",
            "  inflating: Collection5/105.txt     \n",
            "  inflating: Collection5/1050.ann    \n",
            "  inflating: Collection5/1050.txt    \n",
            "  inflating: Collection5/106.ann     \n",
            "  inflating: Collection5/106.txt     \n",
            "  inflating: Collection5/107.ann     \n",
            "  inflating: Collection5/107.txt     \n",
            "  inflating: Collection5/108.ann     \n",
            "  inflating: Collection5/108.txt     \n",
            "  inflating: Collection5/109.ann     \n",
            "  inflating: Collection5/109.txt     \n",
            "  inflating: Collection5/10_01_13a.ann  \n",
            "  inflating: Collection5/10_01_13a.txt  \n",
            "  inflating: Collection5/10_01_13d.ann  \n",
            "  inflating: Collection5/10_01_13d.txt  \n",
            "  inflating: Collection5/10_01_13i.ann  \n",
            "  inflating: Collection5/10_01_13i.txt  \n",
            "  inflating: Collection5/110.ann     \n",
            "  inflating: Collection5/110.txt     \n",
            "  inflating: Collection5/1100.ann    \n",
            "  inflating: Collection5/1100.txt    \n",
            "  inflating: Collection5/1101.ann    \n",
            "  inflating: Collection5/1101.txt    \n",
            "  inflating: Collection5/1102.ann    \n",
            "  inflating: Collection5/1102.txt    \n",
            "  inflating: Collection5/1103.ann    \n",
            "  inflating: Collection5/1103.txt    \n",
            "  inflating: Collection5/1104.ann    \n",
            "  inflating: Collection5/1104.txt    \n",
            "  inflating: Collection5/1105.ann    \n",
            "  inflating: Collection5/1105.txt    \n",
            "  inflating: Collection5/1106.ann    \n",
            "  inflating: Collection5/1106.txt    \n",
            "  inflating: Collection5/1107.ann    \n",
            "  inflating: Collection5/1107.txt    \n",
            "  inflating: Collection5/1108.ann    \n",
            "  inflating: Collection5/1108.txt    \n",
            "  inflating: Collection5/1109.ann    \n",
            "  inflating: Collection5/1109.txt    \n",
            "  inflating: Collection5/111.ann     \n",
            "  inflating: Collection5/111.txt     \n",
            "  inflating: Collection5/1110.ann    \n",
            "  inflating: Collection5/1110.txt    \n",
            "  inflating: Collection5/1111.ann    \n",
            "  inflating: Collection5/1111.txt    \n",
            "  inflating: Collection5/1112.ann    \n",
            "  inflating: Collection5/1112.txt    \n",
            "  inflating: Collection5/1113.ann    \n",
            "  inflating: Collection5/1113.txt    \n",
            "  inflating: Collection5/1114.ann    \n",
            "  inflating: Collection5/1114.txt    \n",
            "  inflating: Collection5/1115.ann    \n",
            "  inflating: Collection5/1115.txt    \n",
            "  inflating: Collection5/1116.ann    \n",
            "  inflating: Collection5/1116.txt    \n",
            "  inflating: Collection5/1117.ann    \n",
            "  inflating: Collection5/1117.txt    \n",
            "  inflating: Collection5/1118.ann    \n",
            "  inflating: Collection5/1118.txt    \n",
            "  inflating: Collection5/1119.ann    \n",
            "  inflating: Collection5/1119.txt    \n",
            "  inflating: Collection5/112.ann     \n",
            "  inflating: Collection5/112.txt     \n",
            "  inflating: Collection5/1120.ann    \n",
            "  inflating: Collection5/1120.txt    \n",
            "  inflating: Collection5/1121.ann    \n",
            "  inflating: Collection5/1121.txt    \n",
            "  inflating: Collection5/1122.ann    \n",
            "  inflating: Collection5/1122.txt    \n",
            "  inflating: Collection5/1123.ann    \n",
            "  inflating: Collection5/1123.txt    \n",
            "  inflating: Collection5/1124.ann    \n",
            "  inflating: Collection5/1124.txt    \n",
            "  inflating: Collection5/1125.ann    \n",
            "  inflating: Collection5/1125.txt    \n",
            "  inflating: Collection5/1126.ann    \n",
            "  inflating: Collection5/1126.txt    \n",
            "  inflating: Collection5/1127.ann    \n",
            "  inflating: Collection5/1127.txt    \n",
            "  inflating: Collection5/1128.ann    \n",
            "  inflating: Collection5/1128.txt    \n",
            "  inflating: Collection5/113.ann     \n",
            "  inflating: Collection5/113.txt     \n",
            "  inflating: Collection5/1130.ann    \n",
            "  inflating: Collection5/1130.txt    \n",
            "  inflating: Collection5/1131.ann    \n",
            "  inflating: Collection5/1131.txt    \n",
            "  inflating: Collection5/1132.ann    \n",
            "  inflating: Collection5/1132.txt    \n",
            "  inflating: Collection5/1133.ann    \n",
            "  inflating: Collection5/1133.txt    \n",
            "  inflating: Collection5/1134.ann    \n",
            "  inflating: Collection5/1134.txt    \n",
            "  inflating: Collection5/1135.ann    \n",
            "  inflating: Collection5/1135.txt    \n",
            "  inflating: Collection5/1136.ann    \n",
            "  inflating: Collection5/1136.txt    \n",
            "  inflating: Collection5/1137.ann    \n",
            "  inflating: Collection5/1137.txt    \n",
            "  inflating: Collection5/1138.ann    \n",
            "  inflating: Collection5/1138.txt    \n",
            "  inflating: Collection5/1139.ann    \n",
            "  inflating: Collection5/1139.txt    \n",
            "  inflating: Collection5/114.ann     \n",
            "  inflating: Collection5/114.txt     \n",
            "  inflating: Collection5/1140.ann    \n",
            "  inflating: Collection5/1140.txt    \n",
            "  inflating: Collection5/1141.ann    \n",
            "  inflating: Collection5/1141.txt    \n",
            "  inflating: Collection5/1142.ann    \n",
            "  inflating: Collection5/1142.txt    \n",
            "  inflating: Collection5/1143.ann    \n",
            "  inflating: Collection5/1143.txt    \n",
            "  inflating: Collection5/1144.ann    \n",
            "  inflating: Collection5/1144.txt    \n",
            "  inflating: Collection5/1145.ann    \n",
            "  inflating: Collection5/1145.txt    \n",
            "  inflating: Collection5/1146.ann    \n",
            "  inflating: Collection5/1146.txt    \n",
            "  inflating: Collection5/1147.ann    \n",
            "  inflating: Collection5/1147.txt    \n",
            "  inflating: Collection5/1148.ann    \n",
            "  inflating: Collection5/1148.txt    \n",
            "  inflating: Collection5/1149.ann    \n",
            "  inflating: Collection5/1149.txt    \n",
            "  inflating: Collection5/115.ann     \n",
            "  inflating: Collection5/115.txt     \n",
            "  inflating: Collection5/1150.ann    \n",
            "  inflating: Collection5/1150.txt    \n",
            "  inflating: Collection5/1151.ann    \n",
            "  inflating: Collection5/1151.txt    \n",
            "  inflating: Collection5/1152.ann    \n",
            "  inflating: Collection5/1152.txt    \n",
            "  inflating: Collection5/1153.ann    \n",
            "  inflating: Collection5/1153.txt    \n",
            "  inflating: Collection5/1154.ann    \n",
            "  inflating: Collection5/1154.txt    \n",
            "  inflating: Collection5/1155.ann    \n",
            "  inflating: Collection5/1155.txt    \n",
            "  inflating: Collection5/1156.ann    \n",
            "  inflating: Collection5/1156.txt    \n",
            "  inflating: Collection5/1157.ann    \n",
            "  inflating: Collection5/1157.txt    \n",
            "  inflating: Collection5/1158.ann    \n",
            "  inflating: Collection5/1158.txt    \n",
            "  inflating: Collection5/1159.ann    \n",
            "  inflating: Collection5/1159.txt    \n",
            "  inflating: Collection5/116.ann     \n",
            "  inflating: Collection5/116.txt     \n",
            "  inflating: Collection5/1160.ann    \n",
            "  inflating: Collection5/1160.txt    \n",
            "  inflating: Collection5/1161.ann    \n",
            "  inflating: Collection5/1161.txt    \n",
            "  inflating: Collection5/1162.ann    \n",
            "  inflating: Collection5/1162.txt    \n",
            "  inflating: Collection5/1163.ann    \n",
            "  inflating: Collection5/1163.txt    \n",
            "  inflating: Collection5/1164.ann    \n",
            "  inflating: Collection5/1164.txt    \n",
            "  inflating: Collection5/1165.ann    \n",
            "  inflating: Collection5/1165.txt    \n",
            "  inflating: Collection5/1166.ann    \n",
            "  inflating: Collection5/1166.txt    \n",
            "  inflating: Collection5/1167.ann    \n",
            "  inflating: Collection5/1167.txt    \n",
            "  inflating: Collection5/1168.ann    \n",
            "  inflating: Collection5/1168.txt    \n",
            "  inflating: Collection5/1169.ann    \n",
            "  inflating: Collection5/1169.txt    \n",
            "  inflating: Collection5/117.ann     \n",
            "  inflating: Collection5/117.txt     \n",
            "  inflating: Collection5/1170.ann    \n",
            "  inflating: Collection5/1170.txt    \n",
            "  inflating: Collection5/1171.ann    \n",
            "  inflating: Collection5/1171.txt    \n",
            "  inflating: Collection5/1172.ann    \n",
            "  inflating: Collection5/1172.txt    \n",
            "  inflating: Collection5/1173.ann    \n",
            "  inflating: Collection5/1173.txt    \n",
            "  inflating: Collection5/1174.ann    \n",
            "  inflating: Collection5/1174.txt    \n",
            "  inflating: Collection5/1175.ann    \n",
            "  inflating: Collection5/1175.txt    \n",
            "  inflating: Collection5/1176.ann    \n",
            "  inflating: Collection5/1176.txt    \n",
            "  inflating: Collection5/1177.ann    \n",
            "  inflating: Collection5/1177.txt    \n",
            "  inflating: Collection5/1178.ann    \n",
            "  inflating: Collection5/1178.txt    \n",
            "  inflating: Collection5/1179.ann    \n",
            "  inflating: Collection5/1179.txt    \n",
            "  inflating: Collection5/118.ann     \n",
            "  inflating: Collection5/118.txt     \n",
            "  inflating: Collection5/1180.ann    \n",
            "  inflating: Collection5/1180.txt    \n",
            "  inflating: Collection5/1181.ann    \n",
            "  inflating: Collection5/1181.txt    \n",
            "  inflating: Collection5/1182.ann    \n",
            "  inflating: Collection5/1182.txt    \n",
            "  inflating: Collection5/1183.ann    \n",
            "  inflating: Collection5/1183.txt    \n",
            "  inflating: Collection5/1184.ann    \n",
            "  inflating: Collection5/1184.txt    \n",
            "  inflating: Collection5/1185.ann    \n",
            "  inflating: Collection5/1185.txt    \n",
            "  inflating: Collection5/1186.ann    \n",
            "  inflating: Collection5/1186.txt    \n",
            "  inflating: Collection5/1187.ann    \n",
            "  inflating: Collection5/1187.txt    \n",
            "  inflating: Collection5/1188.ann    \n",
            "  inflating: Collection5/1188.txt    \n",
            "  inflating: Collection5/1189.ann    \n",
            "  inflating: Collection5/1189.txt    \n",
            "  inflating: Collection5/119.ann     \n",
            "  inflating: Collection5/119.txt     \n",
            "  inflating: Collection5/1190.ann    \n",
            "  inflating: Collection5/1190.txt    \n",
            "  inflating: Collection5/1191.ann    \n",
            "  inflating: Collection5/1191.txt    \n",
            "  inflating: Collection5/1192.ann    \n",
            "  inflating: Collection5/1192.txt    \n",
            "  inflating: Collection5/1193.ann    \n",
            "  inflating: Collection5/1193.txt    \n",
            "  inflating: Collection5/1194.ann    \n",
            "  inflating: Collection5/1194.txt    \n",
            "  inflating: Collection5/1195.ann    \n",
            "  inflating: Collection5/1195.txt    \n",
            "  inflating: Collection5/1196.ann    \n",
            "  inflating: Collection5/1196.txt    \n",
            "  inflating: Collection5/1197.ann    \n",
            "  inflating: Collection5/1197.txt    \n",
            "  inflating: Collection5/1198.ann    \n",
            "  inflating: Collection5/1198.txt    \n",
            "  inflating: Collection5/1199.ann    \n",
            "  inflating: Collection5/1199.txt    \n",
            "  inflating: Collection5/11_01_13b.ann  \n",
            "  inflating: Collection5/11_01_13b.txt  \n",
            "  inflating: Collection5/11_01_13e.ann  \n",
            "  inflating: Collection5/11_01_13e.txt  \n",
            "  inflating: Collection5/120.ann     \n",
            "  inflating: Collection5/120.txt     \n",
            "  inflating: Collection5/1200.ann    \n",
            "  inflating: Collection5/1200.txt    \n",
            "  inflating: Collection5/121.ann     \n",
            "  inflating: Collection5/121.txt     \n",
            "  inflating: Collection5/122.ann     \n",
            "  inflating: Collection5/122.txt     \n",
            "  inflating: Collection5/123.ann     \n",
            "  inflating: Collection5/123.txt     \n",
            "  inflating: Collection5/124.ann     \n",
            "  inflating: Collection5/124.txt     \n",
            "  inflating: Collection5/125.ann     \n",
            "  inflating: Collection5/125.txt     \n",
            "  inflating: Collection5/126.ann     \n",
            "  inflating: Collection5/126.txt     \n",
            "  inflating: Collection5/127.ann     \n",
            "  inflating: Collection5/127.txt     \n",
            "  inflating: Collection5/128.ann     \n",
            "  inflating: Collection5/128.txt     \n",
            "  inflating: Collection5/129.ann     \n",
            "  inflating: Collection5/129.txt     \n",
            "  inflating: Collection5/130.ann     \n",
            "  inflating: Collection5/130.txt     \n",
            "  inflating: Collection5/131.ann     \n",
            "  inflating: Collection5/131.txt     \n",
            "  inflating: Collection5/132.ann     \n",
            "  inflating: Collection5/132.txt     \n",
            "  inflating: Collection5/133.ann     \n",
            "  inflating: Collection5/133.txt     \n",
            "  inflating: Collection5/134.ann     \n",
            "  inflating: Collection5/134.txt     \n",
            "  inflating: Collection5/135.ann     \n",
            "  inflating: Collection5/135.txt     \n",
            "  inflating: Collection5/136.ann     \n",
            "  inflating: Collection5/136.txt     \n",
            "  inflating: Collection5/137.ann     \n",
            "  inflating: Collection5/137.txt     \n",
            "  inflating: Collection5/138.ann     \n",
            "  inflating: Collection5/138.txt     \n",
            "  inflating: Collection5/139.ann     \n",
            "  inflating: Collection5/139.txt     \n",
            "  inflating: Collection5/140.ann     \n",
            "  inflating: Collection5/140.txt     \n",
            "  inflating: Collection5/141.ann     \n",
            "  inflating: Collection5/141.txt     \n",
            "  inflating: Collection5/142.ann     \n",
            "  inflating: Collection5/142.txt     \n",
            "  inflating: Collection5/143.ann     \n",
            "  inflating: Collection5/143.txt     \n",
            "  inflating: Collection5/144.ann     \n",
            "  inflating: Collection5/144.txt     \n",
            "  inflating: Collection5/145.ann     \n",
            "  inflating: Collection5/145.txt     \n",
            "  inflating: Collection5/146.ann     \n",
            "  inflating: Collection5/146.txt     \n",
            "  inflating: Collection5/147.ann     \n",
            "  inflating: Collection5/147.txt     \n",
            "  inflating: Collection5/148.ann     \n",
            "  inflating: Collection5/148.txt     \n",
            "  inflating: Collection5/149.ann     \n",
            "  inflating: Collection5/149.txt     \n",
            "  inflating: Collection5/14_01_13c.ann  \n",
            "  inflating: Collection5/14_01_13c.txt  \n",
            "  inflating: Collection5/14_01_13g.ann  \n",
            "  inflating: Collection5/14_01_13g.txt  \n",
            "  inflating: Collection5/14_01_13i.ann  \n",
            "  inflating: Collection5/14_01_13i.txt  \n",
            "  inflating: Collection5/150.ann     \n",
            "  inflating: Collection5/150.txt     \n",
            "  inflating: Collection5/151.ann     \n",
            "  inflating: Collection5/151.txt     \n",
            "  inflating: Collection5/152.ann     \n",
            "  inflating: Collection5/152.txt     \n",
            "  inflating: Collection5/153.ann     \n",
            "  inflating: Collection5/153.txt     \n",
            "  inflating: Collection5/154.ann     \n",
            "  inflating: Collection5/154.txt     \n",
            "  inflating: Collection5/155.ann     \n",
            "  inflating: Collection5/155.txt     \n",
            "  inflating: Collection5/156.ann     \n",
            "  inflating: Collection5/156.txt     \n",
            "  inflating: Collection5/157.ann     \n",
            "  inflating: Collection5/157.txt     \n",
            "  inflating: Collection5/158.ann     \n",
            "  inflating: Collection5/158.txt     \n",
            "  inflating: Collection5/159.ann     \n",
            "  inflating: Collection5/159.txt     \n",
            "  inflating: Collection5/15_01_13a.ann  \n",
            "  inflating: Collection5/15_01_13a.txt  \n",
            "  inflating: Collection5/15_01_13b.ann  \n",
            "  inflating: Collection5/15_01_13b.txt  \n",
            "  inflating: Collection5/15_01_13e.ann  \n",
            "  inflating: Collection5/15_01_13e.txt  \n",
            "  inflating: Collection5/15_01_13f.ann  \n",
            "  inflating: Collection5/15_01_13f.txt  \n",
            "  inflating: Collection5/160.ann     \n",
            "  inflating: Collection5/160.txt     \n",
            "  inflating: Collection5/161.ann     \n",
            "  inflating: Collection5/161.txt     \n",
            "  inflating: Collection5/162.ann     \n",
            "  inflating: Collection5/162.txt     \n",
            "  inflating: Collection5/163.ann     \n",
            "  inflating: Collection5/163.txt     \n",
            "  inflating: Collection5/164.ann     \n",
            "  inflating: Collection5/164.txt     \n",
            "  inflating: Collection5/165.ann     \n",
            "  inflating: Collection5/165.txt     \n",
            "  inflating: Collection5/166.ann     \n",
            "  inflating: Collection5/166.txt     \n",
            "  inflating: Collection5/167.ann     \n",
            "  inflating: Collection5/167.txt     \n",
            "  inflating: Collection5/168.ann     \n",
            "  inflating: Collection5/168.txt     \n",
            "  inflating: Collection5/169.ann     \n",
            "  inflating: Collection5/169.txt     \n",
            "  inflating: Collection5/170.ann     \n",
            "  inflating: Collection5/170.txt     \n",
            "  inflating: Collection5/171.ann     \n",
            "  inflating: Collection5/171.txt     \n",
            "  inflating: Collection5/172.ann     \n",
            "  inflating: Collection5/172.txt     \n",
            "  inflating: Collection5/173.ann     \n",
            "  inflating: Collection5/173.txt     \n",
            "  inflating: Collection5/174.ann     \n",
            "  inflating: Collection5/174.txt     \n",
            "  inflating: Collection5/175.ann     \n",
            "  inflating: Collection5/175.txt     \n",
            "  inflating: Collection5/176.ann     \n",
            "  inflating: Collection5/176.txt     \n",
            "  inflating: Collection5/177.ann     \n",
            "  inflating: Collection5/177.txt     \n",
            "  inflating: Collection5/178.ann     \n",
            "  inflating: Collection5/178.txt     \n",
            "  inflating: Collection5/179.ann     \n",
            "  inflating: Collection5/179.txt     \n",
            "  inflating: Collection5/180.ann     \n",
            "  inflating: Collection5/180.txt     \n",
            "  inflating: Collection5/181.ann     \n",
            "  inflating: Collection5/181.txt     \n",
            "  inflating: Collection5/182.ann     \n",
            "  inflating: Collection5/182.txt     \n",
            "  inflating: Collection5/183.ann     \n",
            "  inflating: Collection5/183.txt     \n",
            "  inflating: Collection5/184.ann     \n",
            "  inflating: Collection5/184.txt     \n",
            "  inflating: Collection5/185.ann     \n",
            "  inflating: Collection5/185.txt     \n",
            "  inflating: Collection5/186.ann     \n",
            "  inflating: Collection5/186.txt     \n",
            "  inflating: Collection5/187.ann     \n",
            "  inflating: Collection5/187.txt     \n",
            "  inflating: Collection5/188.ann     \n",
            "  inflating: Collection5/188.txt     \n",
            "  inflating: Collection5/189.ann     \n",
            "  inflating: Collection5/189.txt     \n",
            "  inflating: Collection5/190.ann     \n",
            "  inflating: Collection5/190.txt     \n",
            "  inflating: Collection5/191.ann     \n",
            "  inflating: Collection5/191.txt     \n",
            "  inflating: Collection5/192.ann     \n",
            "  inflating: Collection5/192.txt     \n",
            "  inflating: Collection5/193.ann     \n",
            "  inflating: Collection5/193.txt     \n",
            "  inflating: Collection5/194.ann     \n",
            "  inflating: Collection5/194.txt     \n",
            "  inflating: Collection5/195.ann     \n",
            "  inflating: Collection5/195.txt     \n",
            "  inflating: Collection5/196.ann     \n",
            "  inflating: Collection5/196.txt     \n",
            "  inflating: Collection5/197.ann     \n",
            "  inflating: Collection5/197.txt     \n",
            "  inflating: Collection5/198.ann     \n",
            "  inflating: Collection5/198.txt     \n",
            "  inflating: Collection5/199.ann     \n",
            "  inflating: Collection5/199.txt     \n",
            "  inflating: Collection5/19_11_12d.ann  \n",
            "  inflating: Collection5/19_11_12d.txt  \n",
            "  inflating: Collection5/19_11_12h.ann  \n",
            "  inflating: Collection5/19_11_12h.txt  \n",
            "  inflating: Collection5/200.ann     \n",
            "  inflating: Collection5/200.txt     \n",
            "  inflating: Collection5/2001.ann    \n",
            "  inflating: Collection5/2001.txt    \n",
            "  inflating: Collection5/2002.ann    \n",
            "  inflating: Collection5/2002.txt    \n",
            "  inflating: Collection5/2003.ann    \n",
            "  inflating: Collection5/2003.txt    \n",
            "  inflating: Collection5/2004.ann    \n",
            "  inflating: Collection5/2004.txt    \n",
            "  inflating: Collection5/2005.ann    \n",
            "  inflating: Collection5/2005.txt    \n",
            "  inflating: Collection5/2006.ann    \n",
            "  inflating: Collection5/2006.txt    \n",
            "  inflating: Collection5/2007.ann    \n",
            "  inflating: Collection5/2007.txt    \n",
            "  inflating: Collection5/2008.ann    \n",
            "  inflating: Collection5/2008.txt    \n",
            "  inflating: Collection5/2009.ann    \n",
            "  inflating: Collection5/2009.txt    \n",
            "  inflating: Collection5/201.ann     \n",
            "  inflating: Collection5/201.txt     \n",
            "  inflating: Collection5/2010.ann    \n",
            "  inflating: Collection5/2010.txt    \n",
            "  inflating: Collection5/2011.ann    \n",
            "  inflating: Collection5/2011.txt    \n",
            "  inflating: Collection5/2012.ann    \n",
            "  inflating: Collection5/2012.txt    \n",
            "  inflating: Collection5/2013.ann    \n",
            "  inflating: Collection5/2013.txt    \n",
            "  inflating: Collection5/2014.ann    \n",
            "  inflating: Collection5/2014.txt    \n",
            "  inflating: Collection5/2015.ann    \n",
            "  inflating: Collection5/2015.txt    \n",
            "  inflating: Collection5/2016.ann    \n",
            "  inflating: Collection5/2016.txt    \n",
            "  inflating: Collection5/2017.ann    \n",
            "  inflating: Collection5/2017.txt    \n",
            "  inflating: Collection5/2018.ann    \n",
            "  inflating: Collection5/2018.txt    \n",
            "  inflating: Collection5/2019.ann    \n",
            "  inflating: Collection5/2019.txt    \n",
            "  inflating: Collection5/202.ann     \n",
            "  inflating: Collection5/202.txt     \n",
            "  inflating: Collection5/2020.ann    \n",
            "  inflating: Collection5/2020.txt    \n",
            "  inflating: Collection5/2021.ann    \n",
            "  inflating: Collection5/2021.txt    \n",
            "  inflating: Collection5/2022.ann    \n",
            "  inflating: Collection5/2022.txt    \n",
            "  inflating: Collection5/2023.ann    \n",
            "  inflating: Collection5/2023.txt    \n",
            "  inflating: Collection5/2024.ann    \n",
            "  inflating: Collection5/2024.txt    \n",
            "  inflating: Collection5/2025.ann    \n",
            "  inflating: Collection5/2025.txt    \n",
            "  inflating: Collection5/2026.ann    \n",
            "  inflating: Collection5/2026.txt    \n",
            "  inflating: Collection5/2027.ann    \n",
            "  inflating: Collection5/2027.txt    \n",
            "  inflating: Collection5/2028.ann    \n",
            "  inflating: Collection5/2028.txt    \n",
            "  inflating: Collection5/2029.ann    \n",
            "  inflating: Collection5/2029.txt    \n",
            "  inflating: Collection5/203.ann     \n",
            "  inflating: Collection5/203.txt     \n",
            "  inflating: Collection5/2030.ann    \n",
            "  inflating: Collection5/2030.txt    \n",
            "  inflating: Collection5/2031.ann    \n",
            "  inflating: Collection5/2031.txt    \n",
            "  inflating: Collection5/2032.ann    \n",
            "  inflating: Collection5/2032.txt    \n",
            "  inflating: Collection5/2034.ann    \n",
            "  inflating: Collection5/2034.txt    \n",
            "  inflating: Collection5/2035.ann    \n",
            "  inflating: Collection5/2035.txt    \n",
            "  inflating: Collection5/2036.ann    \n",
            "  inflating: Collection5/2036.txt    \n",
            "  inflating: Collection5/2037.ann    \n",
            "  inflating: Collection5/2037.txt    \n",
            "  inflating: Collection5/2038.ann    \n",
            "  inflating: Collection5/2038.txt    \n",
            "  inflating: Collection5/2039.ann    \n",
            "  inflating: Collection5/2039.txt    \n",
            "  inflating: Collection5/204.ann     \n",
            "  inflating: Collection5/204.txt     \n",
            "  inflating: Collection5/2040.ann    \n",
            "  inflating: Collection5/2040.txt    \n",
            "  inflating: Collection5/2041.ann    \n",
            "  inflating: Collection5/2041.txt    \n",
            "  inflating: Collection5/2042.ann    \n",
            "  inflating: Collection5/2042.txt    \n",
            "  inflating: Collection5/2043.ann    \n",
            "  inflating: Collection5/2043.txt    \n",
            "  inflating: Collection5/2044.ann    \n",
            "  inflating: Collection5/2044.txt    \n",
            "  inflating: Collection5/2045.ann    \n",
            "  inflating: Collection5/2045.txt    \n",
            "  inflating: Collection5/2046.ann    \n",
            "  inflating: Collection5/2046.txt    \n",
            "  inflating: Collection5/2047.ann    \n",
            "  inflating: Collection5/2047.txt    \n",
            "  inflating: Collection5/2048.ann    \n",
            "  inflating: Collection5/2048.txt    \n",
            "  inflating: Collection5/2049.ann    \n",
            "  inflating: Collection5/2049.txt    \n",
            "  inflating: Collection5/205.ann     \n",
            "  inflating: Collection5/205.txt     \n",
            "  inflating: Collection5/2050.ann    \n",
            "  inflating: Collection5/2050.txt    \n",
            "  inflating: Collection5/206.ann     \n",
            "  inflating: Collection5/206.txt     \n",
            "  inflating: Collection5/207.ann     \n",
            "  inflating: Collection5/207.txt     \n",
            "  inflating: Collection5/208.ann     \n",
            "  inflating: Collection5/208.txt     \n",
            "  inflating: Collection5/209.ann     \n",
            "  inflating: Collection5/209.txt     \n",
            "  inflating: Collection5/20_11_12a.ann  \n",
            "  inflating: Collection5/20_11_12a.txt  \n",
            "  inflating: Collection5/20_11_12b.ann  \n",
            "  inflating: Collection5/20_11_12b.txt  \n",
            "  inflating: Collection5/20_11_12c.ann  \n",
            "  inflating: Collection5/20_11_12c.txt  \n",
            "  inflating: Collection5/20_11_12d.ann  \n",
            "  inflating: Collection5/20_11_12d.txt  \n",
            "  inflating: Collection5/20_11_12i.ann  \n",
            "  inflating: Collection5/20_11_12i.txt  \n",
            "  inflating: Collection5/210.ann     \n",
            "  inflating: Collection5/210.txt     \n",
            "  inflating: Collection5/211.ann     \n",
            "  inflating: Collection5/211.txt     \n",
            "  inflating: Collection5/212.ann     \n",
            "  inflating: Collection5/212.txt     \n",
            "  inflating: Collection5/213.ann     \n",
            "  inflating: Collection5/213.txt     \n",
            "  inflating: Collection5/214.ann     \n",
            "  inflating: Collection5/214.txt     \n",
            "  inflating: Collection5/215.ann     \n",
            "  inflating: Collection5/215.txt     \n",
            "  inflating: Collection5/216.ann     \n",
            "  inflating: Collection5/216.txt     \n",
            "  inflating: Collection5/217.ann     \n",
            "  inflating: Collection5/217.txt     \n",
            "  inflating: Collection5/218.ann     \n",
            "  inflating: Collection5/218.txt     \n",
            "  inflating: Collection5/219.ann     \n",
            "  inflating: Collection5/219.txt     \n",
            "  inflating: Collection5/21_11_12c.ann  \n",
            "  inflating: Collection5/21_11_12c.txt  \n",
            "  inflating: Collection5/21_11_12h.ann  \n",
            "  inflating: Collection5/21_11_12h.txt  \n",
            "  inflating: Collection5/21_11_12i.ann  \n",
            "  inflating: Collection5/21_11_12i.txt  \n",
            "  inflating: Collection5/21_11_12j.ann  \n",
            "  inflating: Collection5/21_11_12j.txt  \n",
            "  inflating: Collection5/220.ann     \n",
            "  inflating: Collection5/220.txt     \n",
            "  inflating: Collection5/221.ann     \n",
            "  inflating: Collection5/221.txt     \n",
            "  inflating: Collection5/222.ann     \n",
            "  inflating: Collection5/222.txt     \n",
            "  inflating: Collection5/223.ann     \n",
            "  inflating: Collection5/223.txt     \n",
            "  inflating: Collection5/224.ann     \n",
            "  inflating: Collection5/224.txt     \n",
            "  inflating: Collection5/225.ann     \n",
            "  inflating: Collection5/225.txt     \n",
            "  inflating: Collection5/226.ann     \n",
            "  inflating: Collection5/226.txt     \n",
            "  inflating: Collection5/227.ann     \n",
            "  inflating: Collection5/227.txt     \n",
            "  inflating: Collection5/228.ann     \n",
            "  inflating: Collection5/228.txt     \n",
            "  inflating: Collection5/229.ann     \n",
            "  inflating: Collection5/229.txt     \n",
            "  inflating: Collection5/22_11_12a.ann  \n",
            "  inflating: Collection5/22_11_12a.txt  \n",
            "  inflating: Collection5/22_11_12c.ann  \n",
            "  inflating: Collection5/22_11_12c.txt  \n",
            "  inflating: Collection5/22_11_12d.ann  \n",
            "  inflating: Collection5/22_11_12d.txt  \n",
            "  inflating: Collection5/22_11_12g.ann  \n",
            "  inflating: Collection5/22_11_12g.txt  \n",
            "  inflating: Collection5/22_11_12h.ann  \n",
            "  inflating: Collection5/22_11_12h.txt  \n",
            "  inflating: Collection5/22_11_12i.ann  \n",
            "  inflating: Collection5/22_11_12i.txt  \n",
            "  inflating: Collection5/22_11_12j.ann  \n",
            "  inflating: Collection5/22_11_12j.txt  \n",
            "  inflating: Collection5/230.ann     \n",
            "  inflating: Collection5/230.txt     \n",
            "  inflating: Collection5/231.ann     \n",
            "  inflating: Collection5/231.txt     \n",
            "  inflating: Collection5/232.ann     \n",
            "  inflating: Collection5/232.txt     \n",
            "  inflating: Collection5/233.ann     \n",
            "  inflating: Collection5/233.txt     \n",
            "  inflating: Collection5/234.ann     \n",
            "  inflating: Collection5/234.txt     \n",
            "  inflating: Collection5/235.ann     \n",
            "  inflating: Collection5/235.txt     \n",
            "  inflating: Collection5/236.ann     \n",
            "  inflating: Collection5/236.txt     \n",
            "  inflating: Collection5/237.ann     \n",
            "  inflating: Collection5/237.txt     \n",
            "  inflating: Collection5/238.ann     \n",
            "  inflating: Collection5/238.txt     \n",
            "  inflating: Collection5/239.ann     \n",
            "  inflating: Collection5/239.txt     \n",
            "  inflating: Collection5/23_11_12a.ann  \n",
            "  inflating: Collection5/23_11_12a.txt  \n",
            "  inflating: Collection5/23_11_12b.ann  \n",
            "  inflating: Collection5/23_11_12b.txt  \n",
            "  inflating: Collection5/23_11_12c.ann  \n",
            "  inflating: Collection5/23_11_12c.txt  \n",
            "  inflating: Collection5/23_11_12d.ann  \n",
            "  inflating: Collection5/23_11_12d.txt  \n",
            "  inflating: Collection5/23_11_12e.ann  \n",
            "  inflating: Collection5/23_11_12e.txt  \n",
            "  inflating: Collection5/23_11_12f.ann  \n",
            "  inflating: Collection5/23_11_12f.txt  \n",
            "  inflating: Collection5/240.ann     \n",
            "  inflating: Collection5/240.txt     \n",
            "  inflating: Collection5/241.ann     \n",
            "  inflating: Collection5/241.txt     \n",
            "  inflating: Collection5/242.ann     \n",
            "  inflating: Collection5/242.txt     \n",
            "  inflating: Collection5/243.ann     \n",
            "  inflating: Collection5/243.txt     \n",
            "  inflating: Collection5/244.ann     \n",
            "  inflating: Collection5/244.txt     \n",
            "  inflating: Collection5/245.ann     \n",
            "  inflating: Collection5/245.txt     \n",
            "  inflating: Collection5/246.ann     \n",
            "  inflating: Collection5/246.txt     \n",
            "  inflating: Collection5/247.ann     \n",
            "  inflating: Collection5/247.txt     \n",
            "  inflating: Collection5/248.ann     \n",
            "  inflating: Collection5/248.txt     \n",
            "  inflating: Collection5/249.ann     \n",
            "  inflating: Collection5/249.txt     \n",
            "  inflating: Collection5/250.ann     \n",
            "  inflating: Collection5/250.txt     \n",
            "  inflating: Collection5/251.ann     \n",
            "  inflating: Collection5/251.txt     \n",
            "  inflating: Collection5/252.ann     \n",
            "  inflating: Collection5/252.txt     \n",
            "  inflating: Collection5/253.ann     \n",
            "  inflating: Collection5/253.txt     \n",
            "  inflating: Collection5/254.ann     \n",
            "  inflating: Collection5/254.txt     \n",
            "  inflating: Collection5/255.ann     \n",
            "  inflating: Collection5/255.txt     \n",
            "  inflating: Collection5/256.ann     \n",
            "  inflating: Collection5/256.txt     \n",
            "  inflating: Collection5/257.ann     \n",
            "  inflating: Collection5/257.txt     \n",
            "  inflating: Collection5/258.ann     \n",
            "  inflating: Collection5/258.txt     \n",
            "  inflating: Collection5/259.ann     \n",
            "  inflating: Collection5/259.txt     \n",
            "  inflating: Collection5/25_12_12a.ann  \n",
            "  inflating: Collection5/25_12_12a.txt  \n",
            "  inflating: Collection5/25_12_12c.ann  \n",
            "  inflating: Collection5/25_12_12c.txt  \n",
            "  inflating: Collection5/25_12_12d.ann  \n",
            "  inflating: Collection5/25_12_12d.txt  \n",
            "  inflating: Collection5/25_12_12e.ann  \n",
            "  inflating: Collection5/25_12_12e.txt  \n",
            "  inflating: Collection5/260.ann     \n",
            "  inflating: Collection5/260.txt     \n",
            "  inflating: Collection5/261.ann     \n",
            "  inflating: Collection5/261.txt     \n",
            "  inflating: Collection5/262.ann     \n",
            "  inflating: Collection5/262.txt     \n",
            "  inflating: Collection5/263.ann     \n",
            "  inflating: Collection5/263.txt     \n",
            "  inflating: Collection5/264.ann     \n",
            "  inflating: Collection5/264.txt     \n",
            "  inflating: Collection5/265.ann     \n",
            "  inflating: Collection5/265.txt     \n",
            "  inflating: Collection5/266.ann     \n",
            "  inflating: Collection5/266.txt     \n",
            "  inflating: Collection5/267.ann     \n",
            "  inflating: Collection5/267.txt     \n",
            "  inflating: Collection5/268.ann     \n",
            "  inflating: Collection5/268.txt     \n",
            "  inflating: Collection5/269.ann     \n",
            "  inflating: Collection5/269.txt     \n",
            "  inflating: Collection5/26_11_12b.ann  \n",
            "  inflating: Collection5/26_11_12b.txt  \n",
            "  inflating: Collection5/26_11_12c.ann  \n",
            "  inflating: Collection5/26_11_12c.txt  \n",
            "  inflating: Collection5/26_11_12e.ann  \n",
            "  inflating: Collection5/26_11_12e.txt  \n",
            "  inflating: Collection5/26_11_12f.ann  \n",
            "  inflating: Collection5/26_11_12f.txt  \n",
            "  inflating: Collection5/270.ann     \n",
            "  inflating: Collection5/270.txt     \n",
            "  inflating: Collection5/271.ann     \n",
            "  inflating: Collection5/271.txt     \n",
            "  inflating: Collection5/272.ann     \n",
            "  inflating: Collection5/272.txt     \n",
            "  inflating: Collection5/273.ann     \n",
            "  inflating: Collection5/273.txt     \n",
            "  inflating: Collection5/274.ann     \n",
            "  inflating: Collection5/274.txt     \n",
            "  inflating: Collection5/275.ann     \n",
            "  inflating: Collection5/275.txt     \n",
            "  inflating: Collection5/276.ann     \n",
            "  inflating: Collection5/276.txt     \n",
            "  inflating: Collection5/277.ann     \n",
            "  inflating: Collection5/277.txt     \n",
            "  inflating: Collection5/278.ann     \n",
            "  inflating: Collection5/278.txt     \n",
            "  inflating: Collection5/279.ann     \n",
            "  inflating: Collection5/279.txt     \n",
            "  inflating: Collection5/27_11_12a.ann  \n",
            "  inflating: Collection5/27_11_12a.txt  \n",
            "  inflating: Collection5/27_11_12c.ann  \n",
            "  inflating: Collection5/27_11_12c.txt  \n",
            "  inflating: Collection5/27_11_12d.ann  \n",
            "  inflating: Collection5/27_11_12d.txt  \n",
            "  inflating: Collection5/27_11_12e.ann  \n",
            "  inflating: Collection5/27_11_12e.txt  \n",
            "  inflating: Collection5/27_11_12j.ann  \n",
            "  inflating: Collection5/27_11_12j.txt  \n",
            "  inflating: Collection5/280.ann     \n",
            "  inflating: Collection5/280.txt     \n",
            "  inflating: Collection5/281.ann     \n",
            "  inflating: Collection5/281.txt     \n",
            "  inflating: Collection5/282.ann     \n",
            "  inflating: Collection5/282.txt     \n",
            "  inflating: Collection5/283.ann     \n",
            "  inflating: Collection5/283.txt     \n",
            "  inflating: Collection5/284.ann     \n",
            "  inflating: Collection5/284.txt     \n",
            "  inflating: Collection5/285.ann     \n",
            "  inflating: Collection5/285.txt     \n",
            "  inflating: Collection5/286.ann     \n",
            "  inflating: Collection5/286.txt     \n",
            "  inflating: Collection5/287.ann     \n",
            "  inflating: Collection5/287.txt     \n",
            "  inflating: Collection5/288.ann     \n",
            "  inflating: Collection5/288.txt     \n",
            "  inflating: Collection5/289.ann     \n",
            "  inflating: Collection5/289.txt     \n",
            "  inflating: Collection5/28_11_12a.ann  \n",
            "  inflating: Collection5/28_11_12a.txt  \n",
            "  inflating: Collection5/28_11_12f.ann  \n",
            "  inflating: Collection5/28_11_12f.txt  \n",
            "  inflating: Collection5/28_11_12g.ann  \n",
            "  inflating: Collection5/28_11_12g.txt  \n",
            "  inflating: Collection5/28_11_12h.ann  \n",
            "  inflating: Collection5/28_11_12h.txt  \n",
            "  inflating: Collection5/28_11_12i.ann  \n",
            "  inflating: Collection5/28_11_12i.txt  \n",
            "  inflating: Collection5/28_11_12j.ann  \n",
            "  inflating: Collection5/28_11_12j.txt  \n",
            "  inflating: Collection5/290.ann     \n",
            "  inflating: Collection5/290.txt     \n",
            "  inflating: Collection5/291.ann     \n",
            "  inflating: Collection5/291.txt     \n",
            "  inflating: Collection5/292.ann     \n",
            "  inflating: Collection5/292.txt     \n",
            "  inflating: Collection5/293.ann     \n",
            "  inflating: Collection5/293.txt     \n",
            "  inflating: Collection5/294.ann     \n",
            "  inflating: Collection5/294.txt     \n",
            "  inflating: Collection5/295.ann     \n",
            "  inflating: Collection5/295.txt     \n",
            "  inflating: Collection5/296.ann     \n",
            "  inflating: Collection5/296.txt     \n",
            "  inflating: Collection5/297.ann     \n",
            "  inflating: Collection5/297.txt     \n",
            "  inflating: Collection5/298.ann     \n",
            "  inflating: Collection5/298.txt     \n",
            "  inflating: Collection5/299.ann     \n",
            "  inflating: Collection5/299.txt     \n",
            "  inflating: Collection5/29_11_12a.ann  \n",
            "  inflating: Collection5/29_11_12a.txt  \n",
            "  inflating: Collection5/29_11_12b.ann  \n",
            "  inflating: Collection5/29_11_12b.txt  \n",
            "  inflating: Collection5/300.ann     \n",
            "  inflating: Collection5/300.txt     \n",
            "  inflating: Collection5/301.ann     \n",
            "  inflating: Collection5/301.txt     \n",
            "  inflating: Collection5/302.ann     \n",
            "  inflating: Collection5/302.txt     \n",
            "  inflating: Collection5/303.ann     \n",
            "  inflating: Collection5/303.txt     \n",
            "  inflating: Collection5/304.ann     \n",
            "  inflating: Collection5/304.txt     \n",
            "  inflating: Collection5/305.ann     \n",
            "  inflating: Collection5/305.txt     \n",
            "  inflating: Collection5/306.ann     \n",
            "  inflating: Collection5/306.txt     \n",
            "  inflating: Collection5/307.ann     \n",
            "  inflating: Collection5/307.txt     \n",
            "  inflating: Collection5/308.ann     \n",
            "  inflating: Collection5/308.txt     \n",
            "  inflating: Collection5/309.ann     \n",
            "  inflating: Collection5/309.txt     \n",
            "  inflating: Collection5/30_11_12b.ann  \n",
            "  inflating: Collection5/30_11_12b.txt  \n",
            "  inflating: Collection5/30_11_12h.ann  \n",
            "  inflating: Collection5/30_11_12h.txt  \n",
            "  inflating: Collection5/30_11_12i.ann  \n",
            "  inflating: Collection5/30_11_12i.txt  \n",
            "  inflating: Collection5/310.ann     \n",
            "  inflating: Collection5/310.txt     \n",
            "  inflating: Collection5/311.ann     \n",
            "  inflating: Collection5/311.txt     \n",
            "  inflating: Collection5/312.ann     \n",
            "  inflating: Collection5/312.txt     \n",
            "  inflating: Collection5/313.ann     \n",
            "  inflating: Collection5/313.txt     \n",
            "  inflating: Collection5/314.ann     \n",
            "  inflating: Collection5/314.txt     \n",
            "  inflating: Collection5/315.ann     \n",
            "  inflating: Collection5/315.txt     \n",
            "  inflating: Collection5/316.ann     \n",
            "  inflating: Collection5/316.txt     \n",
            "  inflating: Collection5/317.ann     \n",
            "  inflating: Collection5/317.txt     \n",
            "  inflating: Collection5/318.ann     \n",
            "  inflating: Collection5/318.txt     \n",
            "  inflating: Collection5/319.ann     \n",
            "  inflating: Collection5/319.txt     \n",
            "  inflating: Collection5/320.ann     \n",
            "  inflating: Collection5/320.txt     \n",
            "  inflating: Collection5/321.ann     \n",
            "  inflating: Collection5/321.txt     \n",
            "  inflating: Collection5/322.ann     \n",
            "  inflating: Collection5/322.txt     \n",
            "  inflating: Collection5/323.ann     \n",
            "  inflating: Collection5/323.txt     \n",
            "  inflating: Collection5/324.ann     \n",
            "  inflating: Collection5/324.txt     \n",
            "  inflating: Collection5/325.ann     \n",
            "  inflating: Collection5/325.txt     \n",
            "  inflating: Collection5/326.ann     \n",
            "  inflating: Collection5/326.txt     \n",
            "  inflating: Collection5/327.ann     \n",
            "  inflating: Collection5/327.txt     \n",
            "  inflating: Collection5/328.ann     \n",
            "  inflating: Collection5/328.txt     \n",
            "  inflating: Collection5/329.ann     \n",
            "  inflating: Collection5/329.txt     \n",
            "  inflating: Collection5/330.ann     \n",
            "  inflating: Collection5/330.txt     \n",
            "  inflating: Collection5/331.ann     \n",
            "  inflating: Collection5/331.txt     \n",
            "  inflating: Collection5/332.ann     \n",
            "  inflating: Collection5/332.txt     \n",
            "  inflating: Collection5/333.ann     \n",
            "  inflating: Collection5/333.txt     \n",
            "  inflating: Collection5/334.ann     \n",
            "  inflating: Collection5/334.txt     \n",
            "  inflating: Collection5/335.ann     \n",
            "  inflating: Collection5/335.txt     \n",
            "  inflating: Collection5/336.ann     \n",
            "  inflating: Collection5/336.txt     \n",
            "  inflating: Collection5/337.ann     \n",
            "  inflating: Collection5/337.txt     \n",
            "  inflating: Collection5/338.ann     \n",
            "  inflating: Collection5/338.txt     \n",
            "  inflating: Collection5/339.ann     \n",
            "  inflating: Collection5/339.txt     \n",
            "  inflating: Collection5/340.ann     \n",
            "  inflating: Collection5/340.txt     \n",
            "  inflating: Collection5/341.ann     \n",
            "  inflating: Collection5/341.txt     \n",
            "  inflating: Collection5/342.ann     \n",
            "  inflating: Collection5/342.txt     \n",
            "  inflating: Collection5/343.ann     \n",
            "  inflating: Collection5/343.txt     \n",
            "  inflating: Collection5/344.ann     \n",
            "  inflating: Collection5/344.txt     \n",
            "  inflating: Collection5/345.ann     \n",
            "  inflating: Collection5/345.txt     \n",
            "  inflating: Collection5/346.ann     \n",
            "  inflating: Collection5/346.txt     \n",
            "  inflating: Collection5/347.ann     \n",
            "  inflating: Collection5/347.txt     \n",
            "  inflating: Collection5/348.ann     \n",
            "  inflating: Collection5/348.txt     \n",
            "  inflating: Collection5/349.ann     \n",
            "  inflating: Collection5/349.txt     \n",
            "  inflating: Collection5/350.ann     \n",
            "  inflating: Collection5/350.txt     \n",
            "  inflating: Collection5/351.ann     \n",
            "  inflating: Collection5/351.txt     \n",
            "  inflating: Collection5/352.ann     \n",
            "  inflating: Collection5/352.txt     \n",
            "  inflating: Collection5/353.ann     \n",
            "  inflating: Collection5/353.txt     \n",
            "  inflating: Collection5/354.ann     \n",
            "  inflating: Collection5/354.txt     \n",
            "  inflating: Collection5/355.ann     \n",
            "  inflating: Collection5/355.txt     \n",
            "  inflating: Collection5/356.ann     \n",
            "  inflating: Collection5/356.txt     \n",
            "  inflating: Collection5/357.ann     \n",
            "  inflating: Collection5/357.txt     \n",
            "  inflating: Collection5/358.ann     \n",
            "  inflating: Collection5/358.txt     \n",
            "  inflating: Collection5/359.ann     \n",
            "  inflating: Collection5/359.txt     \n",
            "  inflating: Collection5/360.ann     \n",
            "  inflating: Collection5/360.txt     \n",
            "  inflating: Collection5/361.ann     \n",
            "  inflating: Collection5/361.txt     \n",
            "  inflating: Collection5/362.ann     \n",
            "  inflating: Collection5/362.txt     \n",
            "  inflating: Collection5/363.ann     \n",
            "  inflating: Collection5/363.txt     \n",
            "  inflating: Collection5/364.ann     \n",
            "  inflating: Collection5/364.txt     \n",
            "  inflating: Collection5/365.ann     \n",
            "  inflating: Collection5/365.txt     \n",
            "  inflating: Collection5/366.ann     \n",
            "  inflating: Collection5/366.txt     \n",
            "  inflating: Collection5/367.ann     \n",
            "  inflating: Collection5/367.txt     \n",
            "  inflating: Collection5/368.ann     \n",
            "  inflating: Collection5/368.txt     \n",
            "  inflating: Collection5/369.ann     \n",
            "  inflating: Collection5/369.txt     \n",
            "  inflating: Collection5/370.ann     \n",
            "  inflating: Collection5/370.txt     \n",
            "  inflating: Collection5/371.ann     \n",
            "  inflating: Collection5/371.txt     \n",
            "  inflating: Collection5/372.ann     \n",
            "  inflating: Collection5/372.txt     \n",
            "  inflating: Collection5/373.ann     \n",
            "  inflating: Collection5/373.txt     \n",
            "  inflating: Collection5/374.ann     \n",
            "  inflating: Collection5/374.txt     \n",
            "  inflating: Collection5/375.ann     \n",
            "  inflating: Collection5/375.txt     \n",
            "  inflating: Collection5/376.ann     \n",
            "  inflating: Collection5/376.txt     \n",
            "  inflating: Collection5/377.ann     \n",
            "  inflating: Collection5/377.txt     \n",
            "  inflating: Collection5/378.ann     \n",
            "  inflating: Collection5/378.txt     \n",
            "  inflating: Collection5/379.ann     \n",
            "  inflating: Collection5/379.txt     \n",
            "  inflating: Collection5/380.ann     \n",
            "  inflating: Collection5/380.txt     \n",
            "  inflating: Collection5/381.ann     \n",
            "  inflating: Collection5/381.txt     \n",
            "  inflating: Collection5/382.ann     \n",
            "  inflating: Collection5/382.txt     \n",
            "  inflating: Collection5/383.ann     \n",
            "  inflating: Collection5/383.txt     \n",
            "  inflating: Collection5/384.ann     \n",
            "  inflating: Collection5/384.txt     \n",
            "  inflating: Collection5/385.ann     \n",
            "  inflating: Collection5/385.txt     \n",
            "  inflating: Collection5/386.ann     \n",
            "  inflating: Collection5/386.txt     \n",
            "  inflating: Collection5/387.ann     \n",
            "  inflating: Collection5/387.txt     \n",
            "  inflating: Collection5/388.ann     \n",
            "  inflating: Collection5/388.txt     \n",
            "  inflating: Collection5/389.ann     \n",
            "  inflating: Collection5/389.txt     \n",
            "  inflating: Collection5/390.ann     \n",
            "  inflating: Collection5/390.txt     \n",
            "  inflating: Collection5/391.ann     \n",
            "  inflating: Collection5/391.txt     \n",
            "  inflating: Collection5/392.ann     \n",
            "  inflating: Collection5/392.txt     \n",
            "  inflating: Collection5/393.ann     \n",
            "  inflating: Collection5/393.txt     \n",
            "  inflating: Collection5/394.ann     \n",
            "  inflating: Collection5/394.txt     \n",
            "  inflating: Collection5/395.ann     \n",
            "  inflating: Collection5/395.txt     \n",
            "  inflating: Collection5/396.ann     \n",
            "  inflating: Collection5/396.txt     \n",
            "  inflating: Collection5/397.ann     \n",
            "  inflating: Collection5/397.txt     \n",
            "  inflating: Collection5/398.ann     \n",
            "  inflating: Collection5/398.txt     \n",
            "  inflating: Collection5/399.ann     \n",
            "  inflating: Collection5/399.txt     \n",
            "  inflating: Collection5/400.ann     \n",
            "  inflating: Collection5/400.txt     \n",
            "  inflating: Collection5/401.ann     \n",
            "  inflating: Collection5/401.txt     \n",
            "  inflating: Collection5/402.ann     \n",
            "  inflating: Collection5/402.txt     \n",
            "  inflating: Collection5/403.ann     \n",
            "  inflating: Collection5/403.txt     \n",
            "  inflating: Collection5/404.ann     \n",
            "  inflating: Collection5/404.txt     \n",
            "  inflating: Collection5/405.ann     \n",
            "  inflating: Collection5/405.txt     \n",
            "  inflating: Collection5/406.ann     \n",
            "  inflating: Collection5/406.txt     \n",
            "  inflating: Collection5/407.ann     \n",
            "  inflating: Collection5/407.txt     \n",
            "  inflating: Collection5/408.ann     \n",
            "  inflating: Collection5/408.txt     \n",
            "  inflating: Collection5/409.ann     \n",
            "  inflating: Collection5/409.txt     \n",
            "  inflating: Collection5/410.ann     \n",
            "  inflating: Collection5/410.txt     \n",
            "  inflating: Collection5/411.ann     \n",
            "  inflating: Collection5/411.txt     \n",
            "  inflating: Collection5/412.ann     \n",
            "  inflating: Collection5/412.txt     \n",
            "  inflating: Collection5/413.ann     \n",
            "  inflating: Collection5/413.txt     \n",
            "  inflating: Collection5/414.ann     \n",
            "  inflating: Collection5/414.txt     \n",
            "  inflating: Collection5/415.ann     \n",
            "  inflating: Collection5/415.txt     \n",
            "  inflating: Collection5/416.ann     \n",
            "  inflating: Collection5/416.txt     \n",
            "  inflating: Collection5/417.ann     \n",
            "  inflating: Collection5/417.txt     \n",
            "  inflating: Collection5/418.ann     \n",
            "  inflating: Collection5/418.txt     \n",
            "  inflating: Collection5/419.ann     \n",
            "  inflating: Collection5/419.txt     \n",
            "  inflating: Collection5/420.ann     \n",
            "  inflating: Collection5/420.txt     \n",
            "  inflating: Collection5/421.ann     \n",
            "  inflating: Collection5/421.txt     \n",
            "  inflating: Collection5/422.ann     \n",
            "  inflating: Collection5/422.txt     \n",
            "  inflating: Collection5/423.ann     \n",
            "  inflating: Collection5/423.txt     \n",
            "  inflating: Collection5/424.ann     \n",
            "  inflating: Collection5/424.txt     \n",
            "  inflating: Collection5/425.ann     \n",
            "  inflating: Collection5/425.txt     \n",
            "  inflating: Collection5/426.ann     \n",
            "  inflating: Collection5/426.txt     \n",
            "  inflating: Collection5/427.ann     \n",
            "  inflating: Collection5/427.txt     \n",
            "  inflating: Collection5/428.ann     \n",
            "  inflating: Collection5/428.txt     \n",
            "  inflating: Collection5/429.ann     \n",
            "  inflating: Collection5/429.txt     \n",
            "  inflating: Collection5/430.ann     \n",
            "  inflating: Collection5/430.txt     \n",
            "  inflating: Collection5/431.ann     \n",
            "  inflating: Collection5/431.txt     \n",
            "  inflating: Collection5/432.ann     \n",
            "  inflating: Collection5/432.txt     \n",
            "  inflating: Collection5/433.ann     \n",
            "  inflating: Collection5/433.txt     \n",
            "  inflating: Collection5/434.ann     \n",
            "  inflating: Collection5/434.txt     \n",
            "  inflating: Collection5/435.ann     \n",
            "  inflating: Collection5/435.txt     \n",
            "  inflating: Collection5/436.ann     \n",
            "  inflating: Collection5/436.txt     \n",
            "  inflating: Collection5/437.ann     \n",
            "  inflating: Collection5/437.txt     \n",
            "  inflating: Collection5/438.ann     \n",
            "  inflating: Collection5/438.txt     \n",
            "  inflating: Collection5/439.ann     \n",
            "  inflating: Collection5/439.txt     \n",
            "  inflating: Collection5/440.ann     \n",
            "  inflating: Collection5/440.txt     \n",
            "  inflating: Collection5/441.ann     \n",
            "  inflating: Collection5/441.txt     \n",
            "  inflating: Collection5/442.ann     \n",
            "  inflating: Collection5/442.txt     \n",
            "  inflating: Collection5/443.ann     \n",
            "  inflating: Collection5/443.txt     \n",
            "  inflating: Collection5/444.ann     \n",
            "  inflating: Collection5/444.txt     \n",
            "  inflating: Collection5/445.ann     \n",
            "  inflating: Collection5/445.txt     \n",
            "  inflating: Collection5/446.ann     \n",
            "  inflating: Collection5/446.txt     \n",
            "  inflating: Collection5/447.ann     \n",
            "  inflating: Collection5/447.txt     \n",
            "  inflating: Collection5/448.ann     \n",
            "  inflating: Collection5/448.txt     \n",
            "  inflating: Collection5/449.ann     \n",
            "  inflating: Collection5/449.txt     \n",
            "  inflating: Collection5/450.ann     \n",
            "  inflating: Collection5/450.txt     \n",
            "  inflating: Collection5/451.ann     \n",
            "  inflating: Collection5/451.txt     \n",
            "  inflating: Collection5/452.ann     \n",
            "  inflating: Collection5/452.txt     \n",
            "  inflating: Collection5/453.ann     \n",
            "  inflating: Collection5/453.txt     \n",
            "  inflating: Collection5/454.ann     \n",
            "  inflating: Collection5/454.txt     \n",
            "  inflating: Collection5/455.ann     \n",
            "  inflating: Collection5/455.txt     \n",
            "  inflating: Collection5/457.ann     \n",
            "  inflating: Collection5/457.txt     \n",
            "  inflating: Collection5/458.ann     \n",
            "  inflating: Collection5/458.txt     \n",
            "  inflating: Collection5/459.ann     \n",
            "  inflating: Collection5/459.txt     \n",
            "  inflating: Collection5/460.ann     \n",
            "  inflating: Collection5/460.txt     \n",
            "  inflating: Collection5/461.ann     \n",
            "  inflating: Collection5/461.txt     \n",
            "  inflating: Collection5/462.ann     \n",
            "  inflating: Collection5/462.txt     \n",
            "  inflating: Collection5/463.ann     \n",
            "  inflating: Collection5/463.txt     \n",
            "  inflating: Collection5/464.ann     \n",
            "  inflating: Collection5/464.txt     \n",
            "  inflating: Collection5/465.ann     \n",
            "  inflating: Collection5/465.txt     \n",
            "  inflating: Collection5/466.ann     \n",
            "  inflating: Collection5/466.txt     \n",
            "  inflating: Collection5/467.ann     \n",
            "  inflating: Collection5/467.txt     \n",
            "  inflating: Collection5/468.ann     \n",
            "  inflating: Collection5/468.txt     \n",
            "  inflating: Collection5/469.ann     \n",
            "  inflating: Collection5/469.txt     \n",
            "  inflating: Collection5/470.ann     \n",
            "  inflating: Collection5/470.txt     \n",
            "  inflating: Collection5/471.ann     \n",
            "  inflating: Collection5/471.txt     \n",
            "  inflating: Collection5/472.ann     \n",
            "  inflating: Collection5/472.txt     \n",
            "  inflating: Collection5/473.ann     \n",
            "  inflating: Collection5/473.txt     \n",
            "  inflating: Collection5/474.ann     \n",
            "  inflating: Collection5/474.txt     \n",
            "  inflating: Collection5/475.ann     \n",
            "  inflating: Collection5/475.txt     \n",
            "  inflating: Collection5/476.ann     \n",
            "  inflating: Collection5/476.txt     \n",
            "  inflating: Collection5/477.ann     \n",
            "  inflating: Collection5/477.txt     \n",
            "  inflating: Collection5/478.ann     \n",
            "  inflating: Collection5/478.txt     \n",
            "  inflating: Collection5/479.ann     \n",
            "  inflating: Collection5/479.txt     \n",
            "  inflating: Collection5/480.ann     \n",
            "  inflating: Collection5/480.txt     \n",
            "  inflating: Collection5/481.ann     \n",
            "  inflating: Collection5/481.txt     \n",
            "  inflating: Collection5/482.ann     \n",
            "  inflating: Collection5/482.txt     \n",
            "  inflating: Collection5/483.ann     \n",
            "  inflating: Collection5/483.txt     \n",
            "  inflating: Collection5/484.ann     \n",
            "  inflating: Collection5/484.txt     \n",
            "  inflating: Collection5/485.ann     \n",
            "  inflating: Collection5/485.txt     \n",
            "  inflating: Collection5/486.ann     \n",
            "  inflating: Collection5/486.txt     \n",
            "  inflating: Collection5/487.ann     \n",
            "  inflating: Collection5/487.txt     \n",
            "  inflating: Collection5/488.ann     \n",
            "  inflating: Collection5/488.txt     \n",
            "  inflating: Collection5/489.ann     \n",
            "  inflating: Collection5/489.txt     \n",
            "  inflating: Collection5/490.ann     \n",
            "  inflating: Collection5/490.txt     \n",
            "  inflating: Collection5/491.ann     \n",
            "  inflating: Collection5/491.txt     \n",
            "  inflating: Collection5/492.ann     \n",
            "  inflating: Collection5/492.txt     \n",
            "  inflating: Collection5/493.ann     \n",
            "  inflating: Collection5/493.txt     \n",
            "  inflating: Collection5/494.ann     \n",
            "  inflating: Collection5/494.txt     \n",
            "  inflating: Collection5/495.ann     \n",
            "  inflating: Collection5/495.txt     \n",
            "  inflating: Collection5/496.ann     \n",
            "  inflating: Collection5/496.txt     \n",
            "  inflating: Collection5/497.ann     \n",
            "  inflating: Collection5/497.txt     \n",
            "  inflating: Collection5/498.ann     \n",
            "  inflating: Collection5/498.txt     \n",
            "  inflating: Collection5/499.ann     \n",
            "  inflating: Collection5/499.txt     \n",
            "  inflating: Collection5/500.ann     \n",
            "  inflating: Collection5/500.txt     \n",
            "  inflating: Collection5/501.ann     \n",
            "  inflating: Collection5/501.txt     \n",
            "  inflating: Collection5/502.ann     \n",
            "  inflating: Collection5/502.txt     \n",
            "  inflating: Collection5/503.ann     \n",
            "  inflating: Collection5/503.txt     \n",
            "  inflating: Collection5/504.ann     \n",
            "  inflating: Collection5/504.txt     \n",
            "  inflating: Collection5/505.ann     \n",
            "  inflating: Collection5/505.txt     \n",
            "  inflating: Collection5/506.ann     \n",
            "  inflating: Collection5/506.txt     \n",
            "  inflating: Collection5/507.ann     \n",
            "  inflating: Collection5/507.txt     \n",
            "  inflating: Collection5/508.ann     \n",
            "  inflating: Collection5/508.txt     \n",
            "  inflating: Collection5/509.ann     \n",
            "  inflating: Collection5/509.txt     \n",
            "  inflating: Collection5/510.ann     \n",
            "  inflating: Collection5/510.txt     \n",
            "  inflating: Collection5/511.ann     \n",
            "  inflating: Collection5/511.txt     \n",
            "  inflating: Collection5/512.ann     \n",
            "  inflating: Collection5/512.txt     \n",
            "  inflating: Collection5/513.ann     \n",
            "  inflating: Collection5/513.txt     \n",
            "  inflating: Collection5/514.ann     \n",
            "  inflating: Collection5/514.txt     \n",
            "  inflating: Collection5/515.ann     \n",
            "  inflating: Collection5/515.txt     \n",
            "  inflating: Collection5/516.ann     \n",
            "  inflating: Collection5/516.txt     \n",
            "  inflating: Collection5/517.ann     \n",
            "  inflating: Collection5/517.txt     \n",
            "  inflating: Collection5/518.ann     \n",
            "  inflating: Collection5/518.txt     \n",
            "  inflating: Collection5/519.ann     \n",
            "  inflating: Collection5/519.txt     \n",
            "  inflating: Collection5/520.ann     \n",
            "  inflating: Collection5/520.txt     \n",
            "  inflating: Collection5/521.ann     \n",
            "  inflating: Collection5/521.txt     \n",
            "  inflating: Collection5/522.ann     \n",
            "  inflating: Collection5/522.txt     \n",
            "  inflating: Collection5/523.ann     \n",
            "  inflating: Collection5/523.txt     \n",
            "  inflating: Collection5/524.ann     \n",
            "  inflating: Collection5/524.txt     \n",
            "  inflating: Collection5/525.ann     \n",
            "  inflating: Collection5/525.txt     \n",
            "  inflating: Collection5/526.ann     \n",
            "  inflating: Collection5/526.txt     \n",
            "  inflating: Collection5/527.ann     \n",
            "  inflating: Collection5/527.txt     \n",
            "  inflating: Collection5/528.ann     \n",
            "  inflating: Collection5/528.txt     \n",
            "  inflating: Collection5/529.ann     \n",
            "  inflating: Collection5/529.txt     \n",
            "  inflating: Collection5/530.ann     \n",
            "  inflating: Collection5/530.txt     \n",
            "  inflating: Collection5/531.ann     \n",
            "  inflating: Collection5/531.txt     \n",
            "  inflating: Collection5/532.ann     \n",
            "  inflating: Collection5/532.txt     \n",
            "  inflating: Collection5/533 (!).ann  \n",
            "  inflating: Collection5/533 (!).txt  \n",
            "  inflating: Collection5/534.ann     \n",
            "  inflating: Collection5/534.txt     \n",
            "  inflating: Collection5/535.ann     \n",
            "  inflating: Collection5/535.txt     \n",
            "  inflating: Collection5/536.ann     \n",
            "  inflating: Collection5/536.txt     \n",
            "  inflating: Collection5/537.ann     \n",
            "  inflating: Collection5/537.txt     \n",
            "  inflating: Collection5/538.ann     \n",
            "  inflating: Collection5/538.txt     \n",
            "  inflating: Collection5/539.ann     \n",
            "  inflating: Collection5/539.txt     \n",
            "  inflating: Collection5/540.ann     \n",
            "  inflating: Collection5/540.txt     \n",
            "  inflating: Collection5/541.ann     \n",
            "  inflating: Collection5/541.txt     \n",
            "  inflating: Collection5/542.ann     \n",
            "  inflating: Collection5/542.txt     \n",
            "  inflating: Collection5/543.ann     \n",
            "  inflating: Collection5/543.txt     \n",
            "  inflating: Collection5/544.ann     \n",
            "  inflating: Collection5/544.txt     \n",
            "  inflating: Collection5/545.ann     \n",
            "  inflating: Collection5/545.txt     \n",
            "  inflating: Collection5/546.ann     \n",
            "  inflating: Collection5/546.txt     \n",
            "  inflating: Collection5/547.ann     \n",
            "  inflating: Collection5/547.txt     \n",
            "  inflating: Collection5/548.ann     \n",
            "  inflating: Collection5/548.txt     \n",
            "  inflating: Collection5/549.ann     \n",
            "  inflating: Collection5/549.txt     \n",
            "  inflating: Collection5/550.ann     \n",
            "  inflating: Collection5/550.txt     \n",
            "  inflating: Collection5/551.ann     \n",
            "  inflating: Collection5/551.txt     \n",
            "  inflating: Collection5/552.ann     \n",
            "  inflating: Collection5/552.txt     \n",
            "  inflating: Collection5/553.ann     \n",
            "  inflating: Collection5/553.txt     \n",
            "  inflating: Collection5/554.ann     \n",
            "  inflating: Collection5/554.txt     \n",
            "  inflating: Collection5/555 (!).ann  \n",
            "  inflating: Collection5/555 (!).txt  \n",
            "  inflating: Collection5/556.ann     \n",
            "  inflating: Collection5/556.txt     \n",
            "  inflating: Collection5/557.ann     \n",
            "  inflating: Collection5/557.txt     \n",
            "  inflating: Collection5/558.ann     \n",
            "  inflating: Collection5/558.txt     \n",
            "  inflating: Collection5/559.ann     \n",
            "  inflating: Collection5/559.txt     \n",
            "  inflating: Collection5/560.ann     \n",
            "  inflating: Collection5/560.txt     \n",
            "  inflating: Collection5/561.ann     \n",
            "  inflating: Collection5/561.txt     \n",
            "  inflating: Collection5/562.ann     \n",
            "  inflating: Collection5/562.txt     \n",
            "  inflating: Collection5/563.ann     \n",
            "  inflating: Collection5/563.txt     \n",
            "  inflating: Collection5/564.ann     \n",
            "  inflating: Collection5/564.txt     \n",
            "  inflating: Collection5/565.ann     \n",
            "  inflating: Collection5/565.txt     \n",
            "  inflating: Collection5/567.ann     \n",
            "  inflating: Collection5/567.txt     \n",
            "  inflating: Collection5/568.ann     \n",
            "  inflating: Collection5/568.txt     \n",
            "  inflating: Collection5/569.ann     \n",
            "  inflating: Collection5/569.txt     \n",
            "  inflating: Collection5/570.ann     \n",
            "  inflating: Collection5/570.txt     \n",
            "  inflating: Collection5/571.ann     \n",
            "  inflating: Collection5/571.txt     \n",
            "  inflating: Collection5/572.ann     \n",
            "  inflating: Collection5/572.txt     \n",
            "  inflating: Collection5/574.ann     \n",
            "  inflating: Collection5/574.txt     \n",
            "  inflating: Collection5/575.ann     \n",
            "  inflating: Collection5/575.txt     \n",
            "  inflating: Collection5/576.ann     \n",
            "  inflating: Collection5/576.txt     \n",
            "  inflating: Collection5/577.ann     \n",
            "  inflating: Collection5/577.txt     \n",
            "  inflating: Collection5/578.ann     \n",
            "  inflating: Collection5/578.txt     \n",
            "  inflating: Collection5/579.ann     \n",
            "  inflating: Collection5/579.txt     \n",
            "  inflating: Collection5/581.ann     \n",
            "  inflating: Collection5/581.txt     \n",
            "  inflating: Collection5/582.ann     \n",
            "  inflating: Collection5/582.txt     \n",
            "  inflating: Collection5/583.ann     \n",
            "  inflating: Collection5/583.txt     \n",
            "  inflating: Collection5/584 (!).ann  \n",
            "  inflating: Collection5/584 (!).txt  \n",
            "  inflating: Collection5/585.ann     \n",
            "  inflating: Collection5/585.txt     \n",
            "  inflating: Collection5/586.ann     \n",
            "  inflating: Collection5/586.txt     \n",
            "  inflating: Collection5/587.ann     \n",
            "  inflating: Collection5/587.txt     \n",
            "  inflating: Collection5/588.ann     \n",
            "  inflating: Collection5/588.txt     \n",
            "  inflating: Collection5/589.ann     \n",
            "  inflating: Collection5/589.txt     \n",
            "  inflating: Collection5/590.ann     \n",
            "  inflating: Collection5/590.txt     \n",
            "  inflating: Collection5/591.ann     \n",
            "  inflating: Collection5/591.txt     \n",
            "  inflating: Collection5/592.ann     \n",
            "  inflating: Collection5/592.txt     \n",
            "  inflating: Collection5/593.ann     \n",
            "  inflating: Collection5/593.txt     \n",
            "  inflating: Collection5/594.ann     \n",
            "  inflating: Collection5/594.txt     \n",
            "  inflating: Collection5/595.ann     \n",
            "  inflating: Collection5/595.txt     \n",
            "  inflating: Collection5/596.ann     \n",
            "  inflating: Collection5/596.txt     \n",
            "  inflating: Collection5/597.ann     \n",
            "  inflating: Collection5/597.txt     \n",
            "  inflating: Collection5/598 (!).ann  \n",
            "  inflating: Collection5/598 (!).txt  \n",
            "  inflating: Collection5/599.ann     \n",
            "  inflating: Collection5/599.txt     \n",
            "  inflating: Collection5/600.ann     \n",
            "  inflating: Collection5/600.txt     \n",
            "  inflating: Collection5/601.ann     \n",
            "  inflating: Collection5/601.txt     \n",
            "  inflating: Collection5/602.ann     \n",
            "  inflating: Collection5/602.txt     \n",
            "  inflating: Collection5/610.ann     \n",
            "  inflating: Collection5/610.txt     \n",
            "  inflating: Collection5/611.ann     \n",
            "  inflating: Collection5/611.txt     \n",
            "  inflating: Collection5/612.ann     \n",
            "  inflating: Collection5/612.txt     \n",
            "  inflating: Collection5/613.ann     \n",
            "  inflating: Collection5/613.txt     \n",
            "  inflating: Collection5/614.ann     \n",
            "  inflating: Collection5/614.txt     \n",
            "  inflating: Collection5/615.ann     \n",
            "  inflating: Collection5/615.txt     \n",
            "  inflating: Collection5/616.ann     \n",
            "  inflating: Collection5/616.txt     \n",
            "  inflating: Collection5/617.ann     \n",
            "  inflating: Collection5/617.txt     \n",
            "  inflating: Collection5/618.ann     \n",
            "  inflating: Collection5/618.txt     \n",
            "  inflating: Collection5/619.ann     \n",
            "  inflating: Collection5/619.txt     \n",
            "  inflating: Collection5/620.ann     \n",
            "  inflating: Collection5/620.txt     \n",
            "  inflating: Collection5/621.ann     \n",
            "  inflating: Collection5/621.txt     \n",
            "  inflating: Collection5/622.ann     \n",
            "  inflating: Collection5/622.txt     \n",
            "  inflating: Collection5/623.ann     \n",
            "  inflating: Collection5/623.txt     \n",
            "  inflating: Collection5/624.ann     \n",
            "  inflating: Collection5/624.txt     \n",
            "  inflating: Collection5/625.ann     \n",
            "  inflating: Collection5/625.txt     \n",
            "  inflating: Collection5/626.ann     \n",
            "  inflating: Collection5/626.txt     \n",
            "  inflating: Collection5/627.ann     \n",
            "  inflating: Collection5/627.txt     \n",
            "  inflating: Collection5/628.ann     \n",
            "  inflating: Collection5/628.txt     \n",
            "  inflating: Collection5/629.ann     \n",
            "  inflating: Collection5/629.txt     \n",
            "  inflating: Collection5/630.ann     \n",
            "  inflating: Collection5/630.txt     \n",
            "  inflating: Collection5/631.ann     \n",
            "  inflating: Collection5/631.txt     \n",
            "  inflating: Collection5/632.ann     \n",
            "  inflating: Collection5/632.txt     \n",
            "  inflating: Collection5/633.ann     \n",
            "  inflating: Collection5/633.txt     \n",
            "  inflating: Collection5/abdulatipov.ann  \n",
            "  inflating: Collection5/abdulatipov.txt  \n",
            "  inflating: Collection5/artjakov.ann  \n",
            "  inflating: Collection5/artjakov.txt  \n",
            "  inflating: Collection5/Avtovaz.ann  \n",
            "  inflating: Collection5/Avtovaz.txt  \n",
            "  inflating: Collection5/blokhin.ann  \n",
            "  inflating: Collection5/blokhin.txt  \n",
            "  inflating: Collection5/chaves.ann  \n",
            "  inflating: Collection5/chaves.txt  \n",
            "  inflating: Collection5/chirkunov.ann  \n",
            "  inflating: Collection5/chirkunov.txt  \n",
            "  inflating: Collection5/kamchatka.ann  \n",
            "  inflating: Collection5/kamchatka.txt  \n",
            "  inflating: Collection5/klinton.ann  \n",
            "  inflating: Collection5/klinton.txt  \n",
            "  inflating: Collection5/kuleshov.ann  \n",
            "  inflating: Collection5/kuleshov.txt  \n",
            "  inflating: Collection5/last_01.ann  \n",
            "  inflating: Collection5/last_01.txt  \n",
            "  inflating: Collection5/last_02.ann  \n",
            "  inflating: Collection5/last_02.txt  \n",
            "  inflating: Collection5/last_03.ann  \n",
            "  inflating: Collection5/last_03.txt  \n",
            "  inflating: Collection5/last_04.ann  \n",
            "  inflating: Collection5/last_04.txt  \n",
            "  inflating: Collection5/last_05.ann  \n",
            "  inflating: Collection5/last_05.txt  \n",
            "  inflating: Collection5/last_06.ann  \n",
            "  inflating: Collection5/last_06.txt  \n",
            "  inflating: Collection5/last_07_new.ann  \n",
            "  inflating: Collection5/last_07_new.txt  \n",
            "  inflating: Collection5/last_08.ann  \n",
            "  inflating: Collection5/last_08.txt  \n",
            "  inflating: Collection5/last_09.ann  \n",
            "  inflating: Collection5/last_09.txt  \n",
            "  inflating: Collection5/last_10.ann  \n",
            "  inflating: Collection5/last_10.txt  \n",
            "  inflating: Collection5/last_11.ann  \n",
            "  inflating: Collection5/last_11.txt  \n",
            "  inflating: Collection5/last_12.ann  \n",
            "  inflating: Collection5/last_12.txt  \n",
            "  inflating: Collection5/last_13.ann  \n",
            "  inflating: Collection5/last_13.txt  \n",
            "  inflating: Collection5/last_14.ann  \n",
            "  inflating: Collection5/last_14.txt  \n",
            "  inflating: Collection5/last_15.ann  \n",
            "  inflating: Collection5/last_15.txt  \n",
            "  inflating: Collection5/last_16.ann  \n",
            "  inflating: Collection5/last_16.txt  \n",
            "  inflating: Collection5/last_17.ann  \n",
            "  inflating: Collection5/last_17.txt  \n",
            "  inflating: Collection5/last_18.ann  \n",
            "  inflating: Collection5/last_18.txt  \n",
            "  inflating: Collection5/last_19.ann  \n",
            "  inflating: Collection5/last_19.txt  \n",
            "  inflating: Collection5/last_20.ann  \n",
            "  inflating: Collection5/last_20.txt  \n",
            "  inflating: Collection5/last_21.ann  \n",
            "  inflating: Collection5/last_21.txt  \n",
            "  inflating: Collection5/last_22.ann  \n",
            "  inflating: Collection5/last_22.txt  \n",
            "  inflating: Collection5/last_23.ann  \n",
            "  inflating: Collection5/last_23.txt  \n",
            "  inflating: Collection5/last_24.ann  \n",
            "  inflating: Collection5/last_24.txt  \n",
            "  inflating: Collection5/last_25.ann  \n",
            "  inflating: Collection5/last_25.txt  \n",
            "  inflating: Collection5/last_26.ann  \n",
            "  inflating: Collection5/last_26.txt  \n",
            "  inflating: Collection5/last_27.ann  \n",
            "  inflating: Collection5/last_27.txt  \n",
            "  inflating: Collection5/last_28.ann  \n",
            "  inflating: Collection5/last_28.txt  \n",
            "  inflating: Collection5/last_29.ann  \n",
            "  inflating: Collection5/last_29.txt  \n",
            "  inflating: Collection5/last_30_new.ann  \n",
            "  inflating: Collection5/last_30_new.txt  \n",
            "  inflating: Collection5/last_31.ann  \n",
            "  inflating: Collection5/last_31.txt  \n",
            "  inflating: Collection5/last_32.ann  \n",
            "  inflating: Collection5/last_32.txt  \n",
            "  inflating: Collection5/last_33.ann  \n",
            "  inflating: Collection5/last_33.txt  \n",
            "  inflating: Collection5/last_34.ann  \n",
            "  inflating: Collection5/last_34.txt  \n",
            "  inflating: Collection5/last_35.ann  \n",
            "  inflating: Collection5/last_35.txt  \n",
            "  inflating: Collection5/last_36.ann  \n",
            "  inflating: Collection5/last_36.txt  \n",
            "  inflating: Collection5/last_37.ann  \n",
            "  inflating: Collection5/last_37.txt  \n",
            "  inflating: Collection5/last_38.ann  \n",
            "  inflating: Collection5/last_38.txt  \n",
            "  inflating: Collection5/last_39.ann  \n",
            "  inflating: Collection5/last_39.txt  \n",
            "  inflating: Collection5/last_40.ann  \n",
            "  inflating: Collection5/last_40.txt  \n",
            "  inflating: Collection5/last_41.ann  \n",
            "  inflating: Collection5/last_41.txt  \n",
            "  inflating: Collection5/last_42.ann  \n",
            "  inflating: Collection5/last_42.txt  \n",
            "  inflating: Collection5/last_43.ann  \n",
            "  inflating: Collection5/last_43.txt  \n",
            "  inflating: Collection5/last_44.ann  \n",
            "  inflating: Collection5/last_44.txt  \n",
            "  inflating: Collection5/last_45.ann  \n",
            "  inflating: Collection5/last_45.txt  \n",
            "  inflating: Collection5/last_46.ann  \n",
            "  inflating: Collection5/last_46.txt  \n",
            "  inflating: Collection5/last_47.ann  \n",
            "  inflating: Collection5/last_47.txt  \n",
            "  inflating: Collection5/last_48.ann  \n",
            "  inflating: Collection5/last_48.txt  \n",
            "  inflating: Collection5/last_49.ann  \n",
            "  inflating: Collection5/last_49.txt  \n",
            "  inflating: Collection5/last_50.ann  \n",
            "  inflating: Collection5/last_50.txt  \n",
            "  inflating: Collection5/last_51.ann  \n",
            "  inflating: Collection5/last_51.txt  \n",
            "  inflating: Collection5/last_52.ann  \n",
            "  inflating: Collection5/last_52.txt  \n",
            "  inflating: Collection5/last_53.ann  \n",
            "  inflating: Collection5/last_53.txt  \n",
            "  inflating: Collection5/last_54.ann  \n",
            "  inflating: Collection5/last_54.txt  \n",
            "  inflating: Collection5/last_55.ann  \n",
            "  inflating: Collection5/last_55.txt  \n",
            "  inflating: Collection5/last_56.ann  \n",
            "  inflating: Collection5/last_56.txt  \n",
            "  inflating: Collection5/last_57.ann  \n",
            "  inflating: Collection5/last_57.txt  \n",
            "  inflating: Collection5/last_58.ann  \n",
            "  inflating: Collection5/last_58.txt  \n",
            "  inflating: Collection5/last_59.ann  \n",
            "  inflating: Collection5/last_59.txt  \n",
            "  inflating: Collection5/last_60.ann  \n",
            "  inflating: Collection5/last_60.txt  \n",
            "  inflating: Collection5/last_61.ann  \n",
            "  inflating: Collection5/last_61.txt  \n",
            "  inflating: Collection5/last_62.ann  \n",
            "  inflating: Collection5/last_62.txt  \n",
            "  inflating: Collection5/last_63.ann  \n",
            "  inflating: Collection5/last_63.txt  \n",
            "  inflating: Collection5/last_64.ann  \n",
            "  inflating: Collection5/last_64.txt  \n",
            "  inflating: Collection5/last_65.ann  \n",
            "  inflating: Collection5/last_65.txt  \n",
            "  inflating: Collection5/last_66.ann  \n",
            "  inflating: Collection5/last_66.txt  \n",
            "  inflating: Collection5/last_67.ann  \n",
            "  inflating: Collection5/last_67.txt  \n",
            "  inflating: Collection5/last_68.ann  \n",
            "  inflating: Collection5/last_68.txt  \n",
            "  inflating: Collection5/last_69.ann  \n",
            "  inflating: Collection5/last_69.txt  \n",
            "  inflating: Collection5/last_70.ann  \n",
            "  inflating: Collection5/last_70.txt  \n",
            "  inflating: Collection5/last_71.ann  \n",
            "  inflating: Collection5/last_71.txt  \n",
            "  inflating: Collection5/last_72.ann  \n",
            "  inflating: Collection5/last_72.txt  \n",
            "  inflating: Collection5/last_73.ann  \n",
            "  inflating: Collection5/last_73.txt  \n",
            "  inflating: Collection5/last_74.ann  \n",
            "  inflating: Collection5/last_74.txt  \n",
            "  inflating: Collection5/last_75.ann  \n",
            "  inflating: Collection5/last_75.txt  \n",
            "  inflating: Collection5/lenoblast.ann  \n",
            "  inflating: Collection5/lenoblast.txt  \n",
            "  inflating: Collection5/maykl dzhekson.ann  \n",
            "  inflating: Collection5/maykl dzhekson.txt  \n",
            "  inflating: Collection5/mvd.ann     \n",
            "  inflating: Collection5/mvd.txt     \n",
            "  inflating: Collection5/mvd2.ann    \n",
            "  inflating: Collection5/mvd2.txt    \n",
            "  inflating: Collection5/rosobrnadzor.ann  \n",
            "  inflating: Collection5/rosobrnadzor.txt  \n",
            "  inflating: Collection5/ryadovoy chelah.ann  \n",
            "  inflating: Collection5/ryadovoy chelah.txt  \n",
            "  inflating: Collection5/semenenko.ann  \n",
            "  inflating: Collection5/semenenko.txt  \n",
            "  inflating: Collection5/shojgu1.ann  \n",
            "  inflating: Collection5/shojgu1.txt  \n",
            "  inflating: Collection5/shojgu3.ann  \n",
            "  inflating: Collection5/shojgu3.txt  \n",
            "  inflating: Collection5/shojgu4.ann  \n",
            "  inflating: Collection5/shojgu4.txt  \n",
            "  inflating: Collection5/shojgu6.ann  \n",
            "  inflating: Collection5/shojgu6.txt  \n",
            "  inflating: Collection5/si_tzjanpin.ann  \n",
            "  inflating: Collection5/si_tzjanpin.txt  \n",
            "  inflating: Collection5/sobjanin2.ann  \n",
            "  inflating: Collection5/sobjanin2.txt  \n",
            "  inflating: Collection5/turkmenija.ann  \n",
            "  inflating: Collection5/turkmenija.txt  \n",
            "  inflating: Collection5/uchitel.ann  \n",
            "  inflating: Collection5/uchitel.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq corus"
      ],
      "metadata": {
        "id": "f0nFyygEW48C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e832dc84-5374-448a-de03-87e0512b3db9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |████                            | 10 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20 kB 29.9 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 30 kB 37.0 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 40 kB 32.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 51 kB 25.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 61 kB 28.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 71 kB 27.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 81 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 83 kB 2.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from corus import load_ne5\n",
        "\n",
        "path_coll5 = 'Collection5/'\n",
        "records = load_ne5(path_coll5)\n",
        "next(records)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdhxzsilYnP_",
        "outputId": "3c9d814b-f3af-4e9d-a76f-8ee6fba7b0ad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ne5Markup(\n",
              "    id='last_66',\n",
              "    text='\\r\\nВ департаментах правительства Москвы произошли кадровые перестановки\\r\\n\\r\\nВрио мэра Москвы Сергей Собянин произвел кадровые перестановки в столичных департаментах и ведомствах. Как сообщили корреспонденту ИА REGNUM в информационном центре правительства Москвы, заместителем начальника главного архивного управления города Москвы с 1 августа назначен Михаил Горинов, а заместителем руководителя департамента науки, промышленной политики и предпринимательства Григорий Сенчень. С Михаилом Гориновым заключен служебный контракт сроком на один год, а с Григорием Сенченым - на пять лет.\\r\\n\\r\\nСергей Золотухин назначен заместителем начальника управления по обеспечению деятельности мировых судей с заключением служебного контракта до 22 августа 2013 года. Ранее Сергей Золотухин был помощником мэра Москвы отдела организации совещаний у мэра.\\r\\n\\r\\nС заместителем префекта Центрального административного округа Александром Литошиным продлен срок действия служебного контракта на один год.\\r\\n',\n",
              "    spans=[Ne5Span(\n",
              "         index='T1',\n",
              "         type='LOC',\n",
              "         start=32,\n",
              "         stop=38,\n",
              "         text='Москвы'\n",
              "     ), Ne5Span(\n",
              "         index='T2',\n",
              "         type='LOC',\n",
              "         start=84,\n",
              "         stop=90,\n",
              "         text='Москвы'\n",
              "     ), Ne5Span(\n",
              "         index='T3',\n",
              "         type='PER',\n",
              "         start=91,\n",
              "         stop=105,\n",
              "         text='Сергей Собянин'\n",
              "     ), Ne5Span(\n",
              "         index='T4',\n",
              "         type='MEDIA',\n",
              "         start=205,\n",
              "         stop=214,\n",
              "         text='ИА REGNUM'\n",
              "     ), Ne5Span(\n",
              "         index='T5',\n",
              "         type='LOC',\n",
              "         start=253,\n",
              "         stop=259,\n",
              "         text='Москвы'\n",
              "     ), Ne5Span(\n",
              "         index='T6',\n",
              "         type='LOC',\n",
              "         start=322,\n",
              "         stop=328,\n",
              "         text='Москвы'\n",
              "     ), Ne5Span(\n",
              "         index='T7',\n",
              "         type='PER',\n",
              "         start=350,\n",
              "         stop=364,\n",
              "         text='Михаил Горинов'\n",
              "     ), Ne5Span(\n",
              "         index='T8',\n",
              "         type='PER',\n",
              "         start=458,\n",
              "         stop=474,\n",
              "         text='Григорий Сенчень'\n",
              "     ), Ne5Span(\n",
              "         index='T9',\n",
              "         type='PER',\n",
              "         start=478,\n",
              "         stop=496,\n",
              "         text='Михаилом Гориновым'\n",
              "     ), Ne5Span(\n",
              "         index='T10',\n",
              "         type='PER',\n",
              "         start=549,\n",
              "         stop=567,\n",
              "         text='Григорием Сенченым'\n",
              "     ), Ne5Span(\n",
              "         index='T11',\n",
              "         type='PER',\n",
              "         start=586,\n",
              "         stop=602,\n",
              "         text='Сергей Золотухин'\n",
              "     ), Ne5Span(\n",
              "         index='T12',\n",
              "         type='PER',\n",
              "         start=755,\n",
              "         stop=771,\n",
              "         text='Сергей Золотухин'\n",
              "     ), Ne5Span(\n",
              "         index='T13',\n",
              "         type='LOC',\n",
              "         start=792,\n",
              "         stop=798,\n",
              "         text='Москвы'\n",
              "     ), Ne5Span(\n",
              "         index='T14',\n",
              "         type='LOC',\n",
              "         start=863,\n",
              "         stop=900,\n",
              "         text='Центрального административного округа'\n",
              "     ), Ne5Span(\n",
              "         index='T15',\n",
              "         type='PER',\n",
              "         start=901,\n",
              "         stop=922,\n",
              "         text='Александром Литошиным'\n",
              "     )]\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install razdel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CHutsrwYxAn",
        "outputId": "13509384-15d2-4caf-e642-320710f64b5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting razdel\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: razdel\n",
            "Successfully installed razdel-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from razdel import tokenize"
      ],
      "metadata": {
        "id": "z3iy220YY0Wf"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_docs = []\n",
        "text_data = []\n",
        "for ix, rec in enumerate(records):\n",
        "    text_data.append(rec.text)\n",
        "    words = []\n",
        "    for token in tokenize(rec.text):\n",
        "        type_ent = 'OUT'\n",
        "        for ent in rec.spans:\n",
        "            if (token.start >= ent.start) and (token.stop <= ent.stop):\n",
        "                type_ent = ent.type\n",
        "                break\n",
        "        words.append([token.text, type_ent])\n",
        "    words_docs.extend(words)"
      ],
      "metadata": {
        "id": "7Xkz9vxfY17d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "a5iUTaxaY9d0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"
      ],
      "metadata": {
        "id": "dqm4dshrY9xP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_words['tag'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O43o_Qw4Y_8z",
        "outputId": "fc37a11c-e1ab-4cd3-a341-6a7ea8e08319"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OUT         219110\n",
              "PER          21184\n",
              "ORG          13651\n",
              "LOC           4560\n",
              "GEOPOLIT      4356\n",
              "MEDIA         2480\n",
              "Name: tag, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_words.head(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Mk0JlsXIZCiH",
        "outputId": "4ac85822-aece-486c-a0ea-3660ba89aa42"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     word  tag\n",
              "0     Мэр  OUT\n",
              "1  Москвы  LOC\n",
              "2  Сергей  PER"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac760a47-4e14-465d-8d40-105fc9f259fc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Мэр</td>\n",
              "      <td>OUT</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Москвы</td>\n",
              "      <td>LOC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Сергей</td>\n",
              "      <td>PER</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac760a47-4e14-465d-8d40-105fc9f259fc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac760a47-4e14-465d-8d40-105fc9f259fc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac760a47-4e14-465d-8d40-105fc9f259fc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_words.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BDbNalDZD6E",
        "outputId": "26249272-de2e-4069-9ce6-423593948dce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(265341, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NLTK"
      ],
      "metadata": {
        "id": "t0up3LxaaHwK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью функции nltk.ne_chunk () мы можем распознавать именованные сущности с помощью классификатора, который добавляет метки категорий, такие как PERSON, ORGANIZATION и GPE."
      ],
      "metadata": {
        "id": "trioOVrqbKQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCqws6hCbNr0",
        "outputId": "8adb8f02-c25f-42c5-8693-00400202ee44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "qRhhC9-ceFqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_nltk = []\n",
        "for text in tqdm(text_data):\n",
        "  result_nltk.append({(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(text))) if hasattr(chunk, 'label')})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyVSIMwUaSyd",
        "outputId": "328d225f-d63d-4cb7-84c3-5043a7cd6313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 999/999 [01:03<00:00, 15.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_nltk[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCVu_oZFeYuk",
        "outputId": "ed6eb001-fe3a-49e4-f18a-8698abbbb196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{('Кроме', 'PERSON'),\n",
              "  ('Леонида Сидорова', 'PERSON'),\n",
              "  ('Москвы', 'PERSON'),\n",
              "  ('Москвы Сергей Собянин', 'PERSON'),\n",
              "  ('Новости', 'PERSON'),\n",
              "  ('Своим', 'PERSON'),\n",
              "  ('Сергей Байдаков', 'PERSON'),\n",
              "  ('Собянин', 'PERSON'),\n",
              "  ('ЦАО', 'ORGANIZATION')},\n",
              " {('AFI Development', 'ORGANIZATION'),\n",
              "  ('Africa Israel Investment', 'PERSON'),\n",
              "  ('LSE', 'ORGANIZATION'),\n",
              "  ('Алмазный', 'PERSON'),\n",
              "  ('Леваев', 'PERSON'),\n",
              "  ('Леваева', 'PERSON'),\n",
              "  ('Москвы', 'PERSON'),\n",
              "  ('Новости Игорь Катаев МОСКВА', 'PERSON'),\n",
              "  ('РИА Новости', 'ORGANIZATION'),\n",
              "  ('Теперь Леваеву', 'PERSON')}]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spacy"
      ],
      "metadata": {
        "id": "0H9Ql1wUenpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qq install spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoDeWQege17U",
        "outputId": "415b581a-42d3-409a-ed89-d89e13ab9bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U spacy\n",
        "!python -m spacy info\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBf54XCVezSi",
        "outputId": "165bb73e-62e3-4970-ba3a-a3bcc463bc9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m\n",
            "============================== Info about spaCy ==============================\u001b[0m\n",
            "\n",
            "spaCy version    3.4.1                         \n",
            "Location         /usr/local/lib/python3.7/dist-packages/spacy\n",
            "Platform         Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic\n",
            "Python version   3.7.13                        \n",
            "Pipelines        en_core_web_sm (3.4.0)        \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = en_core_web_sm.load()\n",
        "result_spacy = []\n",
        "for text in tqdm(text_data):\n",
        "  result_spacy.append(nlp(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7MBiEuZtgrJU",
        "outputId": "097822cb-697a-4b86-b399-26c21e8e241f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 999/999 [00:46<00:00, 21.37it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for article in result_spacy[:2]:\n",
        "  displacy.render(article, jupyter=True, style='ent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0D6z-kenhMst",
        "outputId": "e1133d9a-7428-4151-cd6b-fe19bd4f9020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Мэр Москвы\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Сергей Собянин подписал\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    распоряжение об увольнении заместителя\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " префекта \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Центрального\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " административного округа (\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ЦАО\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ") \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Москвы Леонида Сидорова\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", сообщил \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    РИА Новости\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " в четверг источник в городской администрации. &quot;Своим распоряжением от \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    17\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " ноября мэр \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Москвы Сергей Собянин\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " освободил \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Леонида Сидорова\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " от замещаемой должности заместителя префекта \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ЦАО Москвы\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " и уволил с государственной гражданской службы города \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    по\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " собственной инициативе&quot;, - сказал собеседник агентства.\r</br>\r</br>Кроме того, \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    по его словам\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Собянин\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " назначил руководителем аппарата префектуры \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Северного\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " административного округа \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Москвы\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " в ранге заместителя префекта \n",
              "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Ольгу Устимову\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
              "</mark>\n",
              ".\r</br>\r</br>&quot;Устимова назначена с заключением служебного контракта сроком на четыре года&quot;, - добавил он.\r</br>\r</br>Ранее префектом \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ЦАО\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " был назначен \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Сергей Байдаков\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", который с октября \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2008\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " года занимал должность заместителя мэра столицы \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    по\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " вопросам физкультуры, спорта и межрегионального сотрудничества. С декабря \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2003\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " года \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    по\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    октябрь 2008\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " года Байдаков уже был префектом \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    ЦАО\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", а до этого, с \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2000\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " года, работал первым заместителем префекта.</div></span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Алмазный магнат Лев Леваев получил новые полномочия в AFI Development\r</br>Израильский бизнесмен Лев Леваев\r</br>© РИА Новости Игорь Катаев\r</br>\r</br>МОСКВА, \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    23\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " ноября - РИА Новости. Алмазный магнат \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Лев Леваев\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", который ранее занимал должности \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    неисполнительного\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    директора\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " и \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    председателя совета\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " директоров принадлежащего ему девелопера \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    AFI Development\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", получил новые полномочия в компании, позволяющие ему теснее общаться с властями \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Москвы\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " и других регионов \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    РФ\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", говорится в сообщении \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    AFI Development\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", размещенном на сайте Лондонской фондовой биржи (LSE).\r</br>\r</br> \r</br>\r</br>В нем уточняется, что Леваев назначен на пост председателя совета директоров компании с исполнительными полномочиями.\r</br>\r</br> \r</br>\r</br>&quot;В своей новой должности господин Леваев обеспечит стратегическое превосходство и будет вести ключевые переговоры с московскими властями, с властями в регионах, где \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    AFI Development\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " ведет деятельность, и с их \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    контрагентами по сделкам стратегической важности\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "&quot;, - указывается в пресс-релизе.\r</br>\r</br> \r</br>\r</br>Теперь Леваеву, как и другим исполнительным топ-менеджерам, полагаются вознаграждения, и, кроме \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    того\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", он получит право на участие в опционной программе компании, добавляется в сообщении.\r</br>\r</br> \r</br>\r</br>\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Назначение Леваева\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " не изменит функций и задач \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    управляющего директора\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " AFI \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Development Марка Гройсмана\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", который продолжит заниматься текущей деятельностью компании, в то время как Леваев сосредоточится на переговорах с властями и стратегических сделках, отмечается в материалах компании.\r</br>\r</br> \r</br>\r</br>Компания AFI Development создана в \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2001\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " году для управления российскими активами холдинга \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Africa\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Israel Investment\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " бизнесмена \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Льва Леваева\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " в сфере недвижимости. \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    AFI Development\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " специализируется на полном девелоперском цикле и \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    реализует\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " высококачественные коммерческие и жилые объекты, включая офисы, \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    торговые центры\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", отели, многофункциональные и жилые проекты. \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    64,88%\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERCENT</span>\n",
              "</mark>\n",
              " акций компании находится в собственности \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Africa\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              " Israel Investment, остальное - в свободном обращении.\r</br>\r</br> \r</br>\r</br>Чистая прибыль компании в \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2011\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " году составила \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    172\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " миллиона долларов (почти в 7 раз больше, чем в \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    2010\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " году). В первом полугодии 2012 года компания зафиксировала чистый убыток \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    в размере\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " 240,64 миллиона долларов.\r</br>\r</br>    \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Отставки\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " и назначения\r</br>\r</br></div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deeppavlov"
      ],
      "metadata": {
        "id": "ht3X78rshiUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используем предтренированную модель \"ner_ontonotes_bert_mult\", которая работает с разными языками, в том числе и русским. Подадим на распознование предложение на русском языке."
      ],
      "metadata": {
        "id": "7pK1OIuShuXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq deeppavlov"
      ],
      "metadata": {
        "id": "LNE_31c2h15O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import deeppavlov\n",
        "from deeppavlov import configs, build_model"
      ],
      "metadata": {
        "id": "TQVoCW25iJGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rbV4zAdkwBK",
        "outputId": "3c809f55-12e8-4818-9067-0d278eb174f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.18.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.22.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "deeppavlov_ner = build_model(configs.ner.ner_ontonotes_bert_torch, download=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYzX5mUIhmkb",
        "outputId": "b0cc03dd-af91-4d76-e567-49962c3212b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-09-05 17:05:06.496 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/v1/ner/ner_ontonotes_bert_torch.tar.gz download because of matching hashes\n",
            "INFO:deeppavlov.download:Skipped http://files.deeppavlov.ai/v1/ner/ner_ontonotes_bert_torch.tar.gz download because of matching hashes\n",
            "2022-09-05 17:05:15.517 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/ner_ontonotes_bert_torch/bert-base-cased/tag.dict]\n",
            "INFO:deeppavlov.core.data.simple_vocab:[loading vocabulary from /root/.deeppavlov/models/ner_ontonotes_bert_torch/bert-base-cased/tag.dict]\n",
            "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "2022-09-05 17:05:19.367 INFO in 'deeppavlov.models.torch_bert.torch_transformers_sequence_tagger'['torch_transformers_sequence_tagger'] at line 360: Load path /root/.deeppavlov/models/ner_ontonotes_bert_torch/bert-base-cased/model is given.\n",
            "INFO:deeppavlov.models.torch_bert.torch_transformers_sequence_tagger:Load path /root/.deeppavlov/models/ner_ontonotes_bert_torch/bert-base-cased/model is given.\n",
            "2022-09-05 17:05:19.374 INFO in 'deeppavlov.models.torch_bert.torch_transformers_sequence_tagger'['torch_transformers_sequence_tagger'] at line 367: Load path /root/.deeppavlov/models/ner_ontonotes_bert_torch/bert-base-cased/model.pth.tar exists.\n",
            "INFO:deeppavlov.models.torch_bert.torch_transformers_sequence_tagger:Load path /root/.deeppavlov/models/ner_ontonotes_bert_torch/bert-base-cased/model.pth.tar exists.\n",
            "2022-09-05 17:05:19.380 INFO in 'deeppavlov.models.torch_bert.torch_transformers_sequence_tagger'['torch_transformers_sequence_tagger'] at line 368: Initializing `TorchTransformersSequenceTagger` from saved.\n",
            "INFO:deeppavlov.models.torch_bert.torch_transformers_sequence_tagger:Initializing `TorchTransformersSequenceTagger` from saved.\n",
            "2022-09-05 17:05:19.385 INFO in 'deeppavlov.models.torch_bert.torch_transformers_sequence_tagger'['torch_transformers_sequence_tagger'] at line 371: Loading weights from /root/.deeppavlov/models/ner_ontonotes_bert_torch/bert-base-cased/model.pth.tar.\n",
            "INFO:deeppavlov.models.torch_bert.torch_transformers_sequence_tagger:Loading weights from /root/.deeppavlov/models/ner_ontonotes_bert_torch/bert-base-cased/model.pth.tar.\n",
            "2022-09-05 17:05:20.149 INFO in 'deeppavlov.core.models.torch_model'['torch_model'] at line 102: Model was successfully initialized! Model summary:\n",
            " BertForTokenClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=37, bias=True)\n",
            ")\n",
            "INFO:deeppavlov.core.models.torch_model:Model was successfully initialized! Model summary:\n",
            " BertForTokenClassification(\n",
            "  (bert): BertModel(\n",
            "    (embeddings): BertEmbeddings(\n",
            "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
            "      (position_embeddings): Embedding(512, 768)\n",
            "      (token_type_embeddings): Embedding(2, 768)\n",
            "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (encoder): BertEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (1): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (2): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (3): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (4): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (5): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (6): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (7): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (8): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (9): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (10): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (11): BertLayer(\n",
            "          (attention): BertAttention(\n",
            "            (self): BertSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (output): BertSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): BertIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): BertOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (classifier): Linear(in_features=768, out_features=37, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видно, модель ждёт длину не более 512 символов, поэтому предварительно нужно разделить на предложения."
      ],
      "metadata": {
        "id": "5rROrbsloMtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "FCPhWWzhovlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_deeppavlov = []\n",
        "for text in tqdm(text_data):\n",
        "  res = []\n",
        "  for sent in sent_tokenize(text):\n",
        "    if len(sent)<=512:\n",
        "      res.extend(deeppavlov_ner([sent])[1][0])\n",
        "    else:\n",
        "      for i in range(1, (-1*len(sent)//512*-1)+1):\n",
        "        res.extend(deeppavlov_ner([sent[(i-1)*512:i*512]])[1][0])\n",
        "  result_deeppavlov.append(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "damJb2skmHTc",
        "outputId": "f0ae1ffc-fbdf-439a-adb9-dcc1a4563f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 999/999 [03:17<00:00,  5.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_deeppavlov[:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbZmjhJPnrEx",
        "outputId": "230be530-c16c-4299-e832-58f8e1e4e275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['B-PERSON',\n",
              "  'I-PERSON',\n",
              "  'I-PERSON',\n",
              "  'I-PERSON',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-PERSON',\n",
              "  'I-PERSON',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-CARDINAL',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'I-WORK_OF_ART',\n",
              "  'O',\n",
              "  'O',\n",
              "  'I-PERSON',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-PERSON',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'I-PERSON',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-DATE',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-DATE',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-DATE',\n",
              "  'O',\n",
              "  'B-PERSON',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-DATE',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O'],\n",
              " ['O',\n",
              "  'O',\n",
              "  'I-PERSON',\n",
              "  'I-PERSON',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'I-PERSON',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'I-WORK_OF_ART',\n",
              "  'I-PERSON',\n",
              "  'I-PERSON',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-CARDINAL',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-PERSON',\n",
              "  'O',\n",
              "  'I-PERSON',\n",
              "  'I-PERSON',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'I-PERSON',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'I-WORK_OF_ART',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-DATE',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'I-PERSON',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-PERCENT',\n",
              "  'B-PERCENT',\n",
              "  'B-PERCENT',\n",
              "  'I-PERCENT',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'I-ORG',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-DATE',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-CARDINAL',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-CARDINAL',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-DATE',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-DATE',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-CARDINAL',\n",
              "  'O',\n",
              "  'B-CARDINAL',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O']]"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Свой NER"
      ],
      "metadata": {
        "id": "bkTSMl_tsTB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
      ],
      "metadata": {
        "id": "QcPxLfU1sbNH"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### a. Передаём в сетку токен и его соседей."
      ],
      "metadata": {
        "id": "ZF59xufisXJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a_x = []\n",
        "a_y = []\n",
        "for i in range(len(words_docs)):\n",
        "  a_y.append(words_docs[i][1])\n",
        "  if i==0:\n",
        "    a_x.append(['', words_docs[i][0], words_docs[i+1][0]])\n",
        "  elif i==(len(words_docs)-1):\n",
        "    a_x.append([words_docs[i-1][0], words_docs[i][0], ''])\n",
        "  else:\n",
        "    a_x.append([words_docs[i-1][0], words_docs[i][0], words_docs[i+1][0]])"
      ],
      "metadata": {
        "id": "oUi8jKNo0NiE"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_ner = dict(sorted(Counter(a_y).items(), key=lambda x: -x[1]))\n",
        "ner2num = dict(zip(count_ner.keys(), range(len(count_ner))))\n",
        "ner2num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kpcqr3a44Y7",
        "outputId": "34d17877-eeac-4971-e6b6-23e7a1eb8235"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'OUT': 0, 'PER': 1, 'ORG': 2, 'LOC': 3, 'GEOPOLIT': 4, 'MEDIA': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(a_y)):\n",
        "    a_y[i]=[ner2num[a_y[i]]]"
      ],
      "metadata": {
        "id": "NfGy8YfN6cDx"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow = []\n",
        "for i in a_x:\n",
        "  for j in i:\n",
        "    bow.append(j)\n",
        "count_bow = dict(sorted(Counter(bow).items(), key=lambda x: -x[1]))\n",
        "word2num = dict(zip(count_bow.keys(), range(1, len(count_bow))))\n",
        "list(word2num.items())[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxd7ZUEo_VAY",
        "outputId": "95a5b216-b0dc-495f-8948-4e0648e377de"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 1),\n",
              " ('.', 2),\n",
              " ('в', 3),\n",
              " ('\"', 4),\n",
              " ('и', 5),\n",
              " ('на', 6),\n",
              " ('с', 7),\n",
              " ('что', 8),\n",
              " ('по', 9),\n",
              " ('В', 10)]"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word2num)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-SmN6XwB_rM",
        "outputId": "93bb6135-c219-435f-fb48-02bdecb70fdf"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34930"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(a_x)):\n",
        "  for j in range(3):\n",
        "    a_x[i][j]=word2num.get(a_x[i][j], 0)"
      ],
      "metadata": {
        "id": "CDJFGzgpBVeX"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection, preprocessing, linear_model\n",
        "\n",
        "train_x, test_x, train_y, test_y = model_selection.train_test_split(a_x, a_y)"
      ],
      "metadata": {
        "id": "686wlNcxviGw"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 64\n",
        "\n",
        "class modelNER(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(modelNER, self).__init__()\n",
        "        self.emb = Embedding(vocab_size+1, embedding_dim)\n",
        "        self.gPool = GlobalMaxPooling1D()\n",
        "        self.fc1 = Dense(300, activation='relu')\n",
        "        self.fc2 = Dense(50, activation='relu')\n",
        "        self.fc3 = Dense(6, activation='softmax')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.emb(x)\n",
        "        pool_x = self.gPool(x)\n",
        "        \n",
        "        fc_x = self.fc1(pool_x)\n",
        "        fc_x = self.fc2(fc_x)\n",
        "        \n",
        "        concat_x = tf.concat([pool_x, fc_x], axis=1)\n",
        "        prob = self.fc3(concat_x)\n",
        "        return prob"
      ],
      "metadata": {
        "id": "bEuKipqkv34m"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmodel = modelNER()"
      ],
      "metadata": {
        "id": "CGtzKzwgzJPK"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmodel.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "gU-vXwuAzNOQ"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmodel.fit(train_x, train_y, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5khSnp3OzORS",
        "outputId": "61bad7be-f08c-4812-d33b-21e6118dcb24"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "6219/6219 [==============================] - 24s 4ms/step - loss: 0.3489 - accuracy: 0.8900\n",
            "Epoch 2/3\n",
            "6219/6219 [==============================] - 24s 4ms/step - loss: 0.1593 - accuracy: 0.9439\n",
            "Epoch 3/3\n",
            "6219/6219 [==============================] - 23s 4ms/step - loss: 0.0966 - accuracy: 0.9650\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fae97bdce10>"
            ]
          },
          "metadata": {},
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Суть: вместо передачи слова, также передаём его соседей слева и справа, но предсказываем мы так же только одну метку для слова по центру."
      ],
      "metadata": {
        "id": "kKpj9cPzFWH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "A_9OcSbiGkhN"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_y, np.argmax(mmodel.predict(test_x), axis=1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kOIgb4zGP2E",
        "outputId": "2a57b82a-8e26-4bb1-aeb8-de47aeae34bb"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.96      0.96     54707\n",
            "           1       0.84      0.79      0.82      5368\n",
            "           2       0.74      0.72      0.73      3354\n",
            "           3       0.81      0.66      0.73      1182\n",
            "           4       0.62      0.60      0.61      1133\n",
            "           5       0.74      0.82      0.78       592\n",
            "\n",
            "    accuracy                           0.92     66336\n",
            "   macro avg       0.78      0.76      0.77     66336\n",
            "weighted avg       0.92      0.92      0.92     66336\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Видно, что 0 класс предсказывается лучше всего, а 4 класс предсказывает хуже всего, посмотрим, что это за классы:"
      ],
      "metadata": {
        "id": "RVOAR8QaIIQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num2ner = {j:i for i,j in ner2num.items()}\n",
        "print(0, num2ner[0])\n",
        "print(4, num2ner[4])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fpg04C-vIPWK",
        "outputId": "80d62ba3-0565-4144-f337-80a2e4c7122c"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 OUT\n",
            "4 GEOPOLIT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В целом етрики очень хорошие, теперь посмотрим, что будет, если передовать по одному слову:"
      ],
      "metadata": {
        "id": "Q-LnpsOjGtL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### b. передаём в сетку только токен."
      ],
      "metadata": {
        "id": "FOqRjgI3HG09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сетку не меняем, просто pool слой не будет работать, так как и так всего один токен"
      ],
      "metadata": {
        "id": "1g8QK0GmHXmj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QtqqyG0YHL4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Я ещё доделываю, извините за заглушку ;c это в первый и последний раз, просто очень очень большое дз >((((\n",
        "\n",
        "#### доделаю до 23:59 05.09.2022"
      ],
      "metadata": {
        "id": "1GtwInhplIXo"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "authorship_tag": "ABX9TyNh9gFHI7gvNXjEv8Ya906U",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}