{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FKz11/NLP/blob/main/lesson_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lesson-2"
      ],
      "metadata": {
        "id": "ZMSlry-ZTDVr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUotKHRULVPD"
      },
      "source": [
        "# Инструменты для работы с языком "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ba5Z02VLVPK"
      },
      "source": [
        "## Задача: классификация твитов по тональности\n",
        "\n",
        "У нас есть датасет из твитов, про каждый указано, как он эмоционально окрашен: положительно или отрицательно. Задача: предсказывать эмоциональную окраску.\n",
        "\n",
        "Скачиваем куски датасета ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2zHB70n5LVPN",
        "outputId": "4d524b45-cf0a-448b-eda2-747b2281800c",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-25 17:47:22--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/fnpq3z4bcnoktiv/positive.csv [following]\n",
            "--2022-08-25 17:47:23--  https://www.dropbox.com/s/raw/fnpq3z4bcnoktiv/positive.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uce009531029f0a684e970f30db5.dl.dropboxusercontent.com/cd/0/inline/Brso7vI7l0C9rOdjd0sSlvEmeO3wMYaYJkZUSn2GWht1cacLLlSf9XDRc58nH49TYqyagqpmig-HGuP0zwPRzuajEKuNpf0tnT_F-i71-LWa9tgJhJCXsydWRd_pmP4hn7v5MtfwygfjY8fHbeB85FN6kkiLE3Kr2MyRCnrTRpI7Dw/file# [following]\n",
            "--2022-08-25 17:47:23--  https://uce009531029f0a684e970f30db5.dl.dropboxusercontent.com/cd/0/inline/Brso7vI7l0C9rOdjd0sSlvEmeO3wMYaYJkZUSn2GWht1cacLLlSf9XDRc58nH49TYqyagqpmig-HGuP0zwPRzuajEKuNpf0tnT_F-i71-LWa9tgJhJCXsydWRd_pmP4hn7v5MtfwygfjY8fHbeB85FN6kkiLE3Kr2MyRCnrTRpI7Dw/file\n",
            "Resolving uce009531029f0a684e970f30db5.dl.dropboxusercontent.com (uce009531029f0a684e970f30db5.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uce009531029f0a684e970f30db5.dl.dropboxusercontent.com (uce009531029f0a684e970f30db5.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26233379 (25M) [text/plain]\n",
            "Saving to: ‘positive.csv.4’\n",
            "\n",
            "positive.csv.4      100%[===================>]  25.02M  20.6MB/s    in 1.2s    \n",
            "\n",
            "2022-08-25 17:47:25 (20.6 MB/s) - ‘positive.csv.4’ saved [26233379/26233379]\n",
            "\n",
            "--2022-08-25 17:47:25--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/r6u59ljhhjdg6j0/negative.csv [following]\n",
            "--2022-08-25 17:47:25--  https://www.dropbox.com/s/raw/r6u59ljhhjdg6j0/negative.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc7187f50ebacda4a9dec7cf7583.dl.dropboxusercontent.com/cd/0/inline/Brua_iF_uHIAKGZmLMYPpXkzinx5Tq9LwRTl0gmk3tawGM7ax0biYzlqmZ71u8h7oC5Kv_NUYbo7Z7a_C3qKHDghFehG08Gg6Lxkh2z8REB-giW_5n4UPnS1kx1WEUbogphCyV9PSG-jk5nZIXQMWM5jhLTnDbKUxWUxKqCmqyCCgw/file# [following]\n",
            "--2022-08-25 17:47:25--  https://uc7187f50ebacda4a9dec7cf7583.dl.dropboxusercontent.com/cd/0/inline/Brua_iF_uHIAKGZmLMYPpXkzinx5Tq9LwRTl0gmk3tawGM7ax0biYzlqmZ71u8h7oC5Kv_NUYbo7Z7a_C3qKHDghFehG08Gg6Lxkh2z8REB-giW_5n4UPnS1kx1WEUbogphCyV9PSG-jk5nZIXQMWM5jhLTnDbKUxWUxKqCmqyCCgw/file\n",
            "Resolving uc7187f50ebacda4a9dec7cf7583.dl.dropboxusercontent.com (uc7187f50ebacda4a9dec7cf7583.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc7187f50ebacda4a9dec7cf7583.dl.dropboxusercontent.com (uc7187f50ebacda4a9dec7cf7583.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24450101 (23M) [text/plain]\n",
            "Saving to: ‘negative.csv.4’\n",
            "\n",
            "negative.csv.4      100%[===================>]  23.32M  29.1MB/s    in 0.8s    \n",
            "\n",
            "2022-08-25 17:47:26 (29.1 MB/s) - ‘negative.csv.4’ saved [24450101/24450101]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# если у вас линукс / мак / collab или ещё какая-то среда, в которой работает wget, можно так:\n",
        "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
        "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "J5YiZNCPLVPe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "DFLtXAZ-LVPq"
      },
      "outputs": [],
      "source": [
        "# считываем данные и заполняем общий датасет\n",
        "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
        "positive['label'] = ['positive'] * len(positive)\n",
        "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
        "negative['label'] = ['negative'] * len(negative)\n",
        "df = positive.append(negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "j1AEISlBLVP0",
        "outputId": "a49f9b52-a717-4aea-a7d2-8120d0433203"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                     text     label\n",
              "111918  Но не каждый хочет что то исправлять:( http://...  negative\n",
              "111919  скучаю так :-( только @taaannyaaa вправляет мо...  negative\n",
              "111920          Вот и в школу, в говно это идти уже надо(  negative\n",
              "111921  RT @_Them__: @LisaBeroud Тауриэль, не грусти :...  negative\n",
              "111922  Такси везет меня на работу. Раздумываю приплат...  negative"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd565575-7629-4477-9e06-a446095e493c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>111918</th>\n",
              "      <td>Но не каждый хочет что то исправлять:( http://...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111919</th>\n",
              "      <td>скучаю так :-( только @taaannyaaa вправляет мо...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111920</th>\n",
              "      <td>Вот и в школу, в говно это идти уже надо(</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111921</th>\n",
              "      <td>RT @_Them__: @LisaBeroud Тауриэль, не грусти :...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>111922</th>\n",
              "      <td>Такси везет меня на работу. Раздумываю приплат...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd565575-7629-4477-9e06-a446095e493c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bd565575-7629-4477-9e06-a446095e493c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bd565575-7629-4477-9e06-a446095e493c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "ZWta7oDgLVP8"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAapBC7VLVQC"
      },
      "source": [
        "## Baseline: классификация необработанных n-грамм\n",
        "\n",
        "### Векторизаторы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "M-AvVt8XLVQD"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSuoVoxcLVQI"
      },
      "source": [
        "Что такое n-граммы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "zeNA7732LVQJ"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeApDOmrLVQN",
        "outputId": "e7639caa-4a3a-4fcd-80ad-ad756e4c86c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ],
      "source": [
        "sent = 'Если б мне платили каждый раз'.split()\n",
        "list(ngrams(sent, 1)) # униграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPAS0fS-LVQQ",
        "outputId": "0ab74fd4-2848-43cb-df60-68f6a49a27ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если', 'б'),\n",
              " ('б', 'мне'),\n",
              " ('мне', 'платили'),\n",
              " ('платили', 'каждый'),\n",
              " ('каждый', 'раз')]"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ],
      "source": [
        "list(ngrams(sent, 2)) # биграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d77jmVPhLVQU",
        "outputId": "92ef0411-3213-4872-dc14-88a30dd767cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне'),\n",
              " ('б', 'мне', 'платили'),\n",
              " ('мне', 'платили', 'каждый'),\n",
              " ('платили', 'каждый', 'раз')]"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ],
      "source": [
        "list(ngrams(sent, 3)) # триграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xXTBrGELVQX",
        "outputId": "79d3905e-7c41-44c9-cd09-2f73efe1ab9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
              " ('б', 'мне', 'платили', 'каждый', 'раз')]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ],
      "source": [
        "list(ngrams(sent, 5)) # ... пентаграммы?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHGJBEm-LVQb"
      },
      "source": [
        "Самый простой способ извлечь фичи из текстовых данных -- векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
        "\n",
        "Объект `CountVectorizer` делает простую вещь:\n",
        "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
        "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "eMqZFBTgLVQb"
      },
      "outputs": [],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train) # bow -- bag of words (мешок слов)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZkpqVtILVQe"
      },
      "source": [
        "ngram_range отвечает за то, какие n-граммы мы используем в качестве фичей:<br/>\n",
        "ngram_range=(1, 1) -- униграммы<br/>\n",
        "ngram_range=(3, 3) -- триграммы<br/>\n",
        "ngram_range=(1, 3) -- униграммы, биграммы и триграммы.\n",
        "\n",
        "В vec.vocabulary_ лежит словарь: мэппинг слов к их индексам:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWRtOSzKLVQf",
        "outputId": "3996428e-8621-4f6b-b9c2-caef5f3773d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('можно', 159332),\n",
              " ('бы', 109117),\n",
              " ('было', 109183),\n",
              " ('нарисовать', 163854),\n",
              " ('это', 241374),\n",
              " ('но', 168395),\n",
              " ('не', 165247),\n",
              " ('люблю', 154342),\n",
              " ('пейринг', 179573),\n",
              " ('уиллганнибал', 226453)]"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ],
      "source": [
        "list(vec.vocabulary_.items())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkmX3iBbLVQi",
        "outputId": "72bf9add-2ef6-4dd4-b5f9-84a10d9f294b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(random_state=42)"
            ]
          },
          "metadata": {},
          "execution_count": 131
        }
      ],
      "source": [
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJ8q5_59LVQm",
        "outputId": "4a549a0f-1734-4cc2-e079-7ca1aa3c7520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.76      0.76     28406\n",
            "    positive       0.76      0.77      0.77     28303\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.77      0.76      0.76     56709\n",
            "weighted avg       0.77      0.76      0.76     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAhgaYgqLVQp"
      },
      "source": [
        "Попробуем сделать то же самое для триграмм:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPWXlh6ALVQq",
        "outputId": "50b4dd22-84c7-43bd-bc7f-6f23b0054bb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.47      0.72      0.57     18217\n",
            "    positive       0.82      0.61      0.70     38492\n",
            "\n",
            "    accuracy                           0.65     56709\n",
            "   macro avg       0.65      0.67      0.64     56709\n",
            "weighted avg       0.71      0.65      0.66     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(3, 3))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnfyJkzTLVQu"
      },
      "source": [
        "(как вы думаете, почему в результатах теперь такой разброс по сравнению с униграммами?)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ответ:"
      ],
      "metadata": {
        "id": "RUZ-gz1DYnZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow._shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twVKCnlTYegc",
        "outputId": "b6b57aa9-f618-46f3-bf6c-d41e35e36a49"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(170125, 1327869)"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = clf.predict(vec.transform(x_train))\n",
        "print(classification_report(pred, y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk3t0FULaerV",
        "outputId": "399eecce-5245-4de6-8920-a0f5ec927799"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     83970\n",
            "    positive       1.00      1.00      1.00     86155\n",
            "\n",
            "    accuracy                           1.00    170125\n",
            "   macro avg       1.00      1.00      1.00    170125\n",
            "weighted avg       1.00      1.00      1.00    170125\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "С ростом n-грамм, размер словаря увеличился, вероятность встретить определённое сочетание из трёх слов меньше, чем вероятность встретить одно какое-то слово, из-за этого увеличивается разряженность дынных и следовательно модель начинает переобучаться, становится менее устойчивая к новым данным.\n",
        "\n",
        "Так же вероятно, радость (positive) вызывает похожий контекст (любовь, друзья, путешествие) и похожие устойчивые словосочетания (я люблю тебя), а для грусти (negative) контекст часто разный, всякие нестандартные ситуации, когда всё должно было быть как обычно, но так не было, нагрубил таксист, непробилась покупка и т.д. И если униграмы ещё находили по похожим словам какой-то похожий контекст, злость, грубость и т.д. То у триграмм, это уже не получается."
      ],
      "metadata": {
        "id": "IFwzeK9NangO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJABxhalLVQu"
      },
      "source": [
        "## TF-IDF векторизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LJES2s-LVQv"
      },
      "source": [
        "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений – tf-idf каждого слова.\n",
        "\n",
        "Как считается tf-idf:\n",
        "\n",
        "TF (term frequency) – относительная частотность слова в документе:\n",
        "$$ TF(t,d) = \\frac{n_t}{\\sum_k n_k} $$\n",
        "\n",
        "`t` -- слово (term), `d` -- документ, $n_t$ -- количество вхождений слова, $n_k$ -- количество вхождений остальных слов\n",
        "\n",
        "IDF (inverse document frequency) – обратная частота документов, в которых есть это слово:\n",
        "$$ IDF(t, D) = \\mbox{log} \\frac{|D|}{|{d : t \\in d}|} $$\n",
        "\n",
        "`t` -- слово (term), `D` -- коллекция документов\n",
        "\n",
        "Перемножаем их:\n",
        "$$TFIDF_(t,d,D) = TF(t,d) \\times IDF(i, D)$$\n",
        "\n",
        "Сакральный смысл – если слово часто встречается в одном документе, но в целом по корпусу встречается в небольшом \n",
        "количестве документов, у него высокий TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "FmEcRD28LVQ0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWLhMl9xLVQ3",
        "outputId": "cc7ddefa-4249-403e-a3dc-c7c0a5cbd76d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.74      0.77      0.75     26805\n",
            "    positive       0.78      0.75      0.77     29904\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.76      0.76      0.76     56709\n",
            "weighted avg       0.76      0.76      0.76     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTODTRnKLVQ6"
      },
      "source": [
        "В этот раз получилось хуже :( Вернёмся к `CountVectorizer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8v9Scpn9Y0M"
      },
      "source": [
        "## PMI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVRqLcSY0etj"
      },
      "source": [
        "Можно оценить взаимосвязь слов в корпусе и понять, какие биграммы наиболее часто встречаются в тексте. Для этого можно использовать метрику PMI (Pointwise Mutual Information) - поточечная взаимная информация. Метрика PMI для двух слов вычисляется по формуле:\n",
        "\n",
        "$$pmi(x; y) = log \\frac{p(x,y)}{p(x)p(y)} $$\n",
        "\n",
        "Здесь p(y|x) - вероятность встретить слово $y$ после $x$, $p(y)$ - вероятность встретить слово $y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXgDwf6W6Kk5"
      },
      "source": [
        "Оценим важность биграмм в нашем обучающем корпусе."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLA0eMMtgN3r",
        "outputId": "617f074a-0dea-4117-fbab-32bc7df0358d"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKmiOEaW53F9",
        "outputId": "78321c12-ca8d-4814-90c4-8664fe8b5acd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Package genesis is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('+1239', '728'), ('+Никита', '=полностью'), ('+СОННО', '+НЕ'), ('+живіт', 'болить.ну'), ('+погода', 'крутая='), (',Дела', 'рез'), ('-/////', 'прбрм-прбрм'), ('-10,11', 'болсо'), ('-163', '-КРАСНЫЙ'), ('-165', '-СИНИЙ'), ('-53', 'dBm'), ('-800', 'нахууй'), ('-АХАХАХАХХАХАХАХАХАХХА', '-АХАХАХХАХАХАХАХАХ'), ('-Айгуль', 'Маратовна'), ('-Алина', '-Синие'), ('-Аха', 'спетросянил'), ('-ВСЕМ', 'СПОКОЙНЫХ'), ('-Весело', 'кншн:3'), ('-Вообще-то', '3.Я'), ('-Восьмигрудый', 'трипи'), ('-Время', 'эмокора'), ('-Выздоравливай', 'педрилк'), ('-ГНИДОТА', '-Над'), ('-ДЕТЕЙ', 'НАКРЫЛО'), ('-ДОВАЙТИ', 'АЛДСКУЛ'), ('-Дирол', 'Сенсес'), ('-Еще', 'дуешься'), ('-ЗАШЛА', 'ОДЕЛА'), ('-Зелено-карие', '-Киллджой'), ('-КРАСНЫЙ', '-ЧЕРНЫЕ'), ('-Керем', 'севгили'), ('-Киллджой', '-Котик'), ('-МАРИЯ', '-МИША'), ('-МНОГО', 'СЛИВОК'), ('-Маладец', '-Лол'), ('-Мамаааа', 'поправь'), ('-НА', 'РЕАЛЬНЫХ'), ('-НАЧИНАЕТ', 'БЕСИТЬ'), ('-ОЗВУЧИВАТЕЛЬ', 'МУЛЬТИКОВ'), ('-ОНИ', 'СТОЯТ'), ('-Песня', 'грусная='), ('-Плохое', 'пищеварение'), ('-Поэзия', 'заключает'), ('-ПриФетиГг', 'СолНыСко='), ('-Рыбу', 'соленую'), ('-СЕРЫЕ', '-ВРАЧ'), ('-СИНИЙ', '-БЕЛЫЕ'), ('-СУКА', 'ЛУКАШИН'), ('-Серые', '-НЕМЕЦКИЕ'), ('-Ти', 'кантужена'), ('-Тиць', 'дурне'), ('-Трахався', '-Іди'), ('-ШАПКА', '-ФОН'), ('-Юлия', 'Зазулина'), ('-Я.банан', '-ахх'), ('-анал', '-абстиненция'), ('-анатолий', 'николаевич'), ('-бляя', 'хммммммм'), ('-бывших', 'преступников'), ('-водичку', 'лью'), ('-г', 'үзээ'), ('-гoop', 'Хакүхо'), ('-говорит', 'одноногий'), ('-дайте', 'гондоны.-сникерс'), ('-дирекшионер', '-one'), ('-иногда', '.Ностальгирую'), ('-киллджой', '-шатенка'), ('-классный', 'ответ.в'), ('-крутенько', '-выйду'), ('-кучи', 'мутики'), ('-ладно', '-АХАХАХАХХАХАХАХАХАХХА'), ('-ложусь', 'спать-темно'), ('-любому', 'умрёшь'), ('-мега', 'шизофреничная'), ('-место', '-кровать'), ('-наш', 'класс^^'), ('-пиздишь', '-отвечаю'), ('-посмотри', 'внимательней'), ('-раздевайся', '-дак'), ('-распускаю', 'волосы-'), ('-руу', 'орох'), ('-рүү', 'мэншндсэн'), ('-случайные', 'вопросы-'), ('-спрашивает', 'жена.-У'), ('-столько', 'бабосов'), ('-тай', 'бна.эхний'), ('-такое', 'ВЧ-8='), ('-то.Даже', 'старух'), ('-указуказуказуказуа', '-материшся'), ('-хаски', '-розовое'), ('-хор', '-найл'), ('-цвета', 'Магнита'), ('-цитирую', 'гастролога'), ('-цытата', 'Шимы'), ('-черные', '-аниме'), ('-шапку', '-фон'), ('-ыг', 'үгүйсгэж'), ('-юн', '-юп'), ('-юп', '-шапку'), ('-языком', 'владеешь')]\n"
          ]
        }
      ],
      "source": [
        "from nltk import collocations \n",
        "nltk.download('genesis')\n",
        "\n",
        "bigram_measures = collocations.BigramAssocMeasures()\n",
        "# bigram_finder.apply_freq_filter(5)\n",
        "bigram_finder = collocations.BigramCollocationFinder.from_documents([nltk.word_tokenize(x) for x in x_train])\n",
        "bigrams = bigram_finder.nbest(bigram_measures.pmi, 100)\n",
        "print(bigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h4Cq1PUTTc-"
      },
      "source": [
        "Можно рассмотреть другие метрики оценки важности биграмм, например, метрику правдоподобия (подробнее про вычисление метрики можно посмотреть [здесь (пункт 5.3.4)](http://www.corpus.unam.mx/cursoenah/ManningSchutze_1999_FoundationsofStatisticalNaturalLanguageProcessing.pdf):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTOJg4KoOo84",
        "outputId": "fc54948d-5002-4697-98be-52bad1c65be4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('(', '('), ('RT', '@'), (')', ')'), ('http', ':'), ('!', '!'), (':', 'D'), ('у', 'меня'), (':', '('), (',', 'а'), (',', 'что'), (',', 'но'), (')', 'http'), ('*', '*'), (':', ')'), ('(', ','), ('у', 'нас'), (',', '('), ('не', 'могу'), (':', '-'), ('?', '?'), (',', ')'), (')', ','), (',', ':'), ('@', '('), (',', ','), (':', ','), ('(', ':'), ('@', ','), ('со', 'мной'), ('&', 'lt'), ('@', ':'), (':', ':'), ('(', '@'), ('gt', ';'), ('новый', 'год'), (';', ')'), (':', '*'), (')', ':'), ('не', 'знаю'), (',', '@'), ('а', 'я'), ('@', '@'), ('У', 'меня'), (',', 'когда'), ('сих', 'пор'), ('lt', ';'), ('потому', 'что'), ('&', 'gt'), ('у', 'тебя'), ('все', 'равно'), (';', '('), ('с', 'тобой'), (',', 'как'), ('в', 'школу'), ('&', 'amp'), ('(', 'http'), (',', 'я'), ('ничего', 'не'), ('--', '--'), ('Доброе', 'утро'), (')', '@'), ('Как', 'же'), ('-', ')'), (':', 'DD'), ('не', '('), ('я', 'не'), ('самом', 'деле'), ('amp', ';'), ('не', ')'), ('как', 'же'), ('(', '!'), ('до', 'сих'), (',', 'чтобы'), ('об', 'этом'), ('что', 'я'), (',', '!'), ('с', 'кем'), ('D', 'http'), ('не', ':'), ('.', 'А'), ('и', '('), ('=', ')'), ('никто', 'не'), ('!', ','), (':', '!'), ('а', 'потом'), ('?', '—'), ('никогда', 'не'), (':', '|'), ('и', ')'), ('Новый', 'Год'), ('.', 'Но'), (',', '.'), ('.', ','), ('не', '@'), ('@', 'не'), ('#', 'євромайдан'), ('=', '('), ('не', 'хочу'), ('в', 'этом')]\n"
          ]
        }
      ],
      "source": [
        "bigrams = bigram_finder.nbest(bigram_measures.likelihood_ratio, 100)\n",
        "print(bigrams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfjCYZa8TeX_"
      },
      "source": [
        "Как можно заметить, немаловажную роль в текстах занимает пунктуация."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AJk1B39LVRP"
      },
      "source": [
        "## Стоп-слова и пунктуация\n",
        "\n",
        "*Стоп-слова* -- это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpWhsTuRLVRP",
        "outputId": "24e2a28a-8ece-4b76-a4ee-49772e993a2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# у вас здесь, вероятно, выскочит ошибка и надо будет загрузить стоп слова (в тексте ошибки написано, как)\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('russian'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "OdRF7rlyLVRS",
        "outputId": "28478ec4-e910-42a2-d51f-c1b2974af30b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 142
        }
      ],
      "source": [
        "from string import punctuation\n",
        "punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "OfXiH98XLVRV"
      },
      "outputs": [],
      "source": [
        "noise = stopwords.words('russian') + list(punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtiIhHDMLVRY"
      },
      "source": [
        "В векторизаторах за стоп-слова, логичным образом, отвечает аргумент `stop_words`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZnbarm_LVRY",
        "outputId": "5b7da342-9b99-4b1e-bef5-9acc61881d31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.76      0.78     29210\n",
            "    positive       0.76      0.80      0.78     27499\n",
            "\n",
            "    accuracy                           0.78     56709\n",
            "   macro avg       0.78      0.78      0.78     56709\n",
            "weighted avg       0.78      0.78      0.78     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=nltk.word_tokenize, stop_words=noise)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr934O7yLVRb"
      },
      "source": [
        "Получилось чууть лучше. Что ещё можно сделать?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7O_oD1fLVRc"
      },
      "source": [
        "## Лемматизация\n",
        "\n",
        "Лемматизация – это сведение разных форм одного слова к начальной форме – *лемме*. Почему это хорошо?\n",
        "* Во-первых, мы хотим рассматривать как отдельную фичу каждое *слово*, а не каждую его отдельную форму.\n",
        "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n",
        "\n",
        "Для русского есть два хороших лемматизатора: mystem и pymorphy:\n",
        "\n",
        "### [Mystem](https://tech.yandex.ru/mystem/)\n",
        "Как с ним работать:\n",
        "* можно скачать mystem и запускать [из терминала с разными параметрами](https://tech.yandex.ru/mystem/doc/)\n",
        "* [pymystem3](https://pythonhosted.org/pymystem3/pymystem3.html) - обертка для питона, работает медленнее, но это удобно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96HdoB7zLVRc",
        "outputId": "e1cca933-ba16-4736-a740-d261078cfc7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-08-25 17:51:06--  http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
            "Resolving download.cdn.yandex.net (download.cdn.yandex.net)... 5.45.205.241, 5.45.205.242, 5.45.205.243, ...\n",
            "Connecting to download.cdn.yandex.net (download.cdn.yandex.net)|5.45.205.241|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: http://cachev2-mskm913.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz?lid=235 [following]\n",
            "--2022-08-25 17:51:07--  http://cachev2-mskm913.cdn.yandex.net/download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz?lid=235\n",
            "Resolving cachev2-mskm913.cdn.yandex.net (cachev2-mskm913.cdn.yandex.net)... 5.45.220.124, 2a02:6b8:0:2002::924\n",
            "Connecting to cachev2-mskm913.cdn.yandex.net (cachev2-mskm913.cdn.yandex.net)|5.45.220.124|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16457938 (16M) [application/octet-stream]\n",
            "Saving to: ‘mystem-3.0-linux3.1-64bit.tar.gz.1’\n",
            "\n",
            "mystem-3.0-linux3.1 100%[===================>]  15.70M  8.93MB/s    in 1.8s    \n",
            "\n",
            "2022-08-25 17:51:09 (8.93 MB/s) - ‘mystem-3.0-linux3.1-64bit.tar.gz.1’ saved [16457938/16457938]\n",
            "\n",
            "mystem\n"
          ]
        }
      ],
      "source": [
        "!wget http://download.cdn.yandex.net/mystem/mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!tar -xvf mystem-3.0-linux3.1-64bit.tar.gz\n",
        "!cp mystem /bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "kzQwGwAaZWV5"
      },
      "outputs": [],
      "source": [
        "from pymystem3 import Mystem\n",
        "mystem_analyzer = Mystem()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_w-_fkNtLVRf"
      },
      "source": [
        "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
        "* mystem_bin - путь к `mystem`, если их несколько\n",
        "* grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
        "* disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
        "* entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
        "\n",
        "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
        "\n",
        "Можно просто лемматизировать текст:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example = 'Но не каждый хочет что-то исправлять :(\\n'"
      ],
      "metadata": {
        "id": "ZMtUPACYincp"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjHHLQv9txDq",
        "outputId": "da70b742-507e-4971-cde2-649b6b050bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['но', ' ', 'не', ' ', 'каждый', ' ', 'хотеть', ' ', 'что-то', ' ', 'исправлять', ' :(\\n']\n"
          ]
        }
      ],
      "source": [
        "print(mystem_analyzer.lemmatize(example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI1eftjkLVRi"
      },
      "source": [
        "А можно получить грамматическую информацию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4MLqlZnxNEj",
        "outputId": "78cfcb52-f496-497e-dcc8-a2db87fe8e86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'analysis': [{'lex': 'но', 'wt': 0.9998906255, 'gr': 'CONJ='}],\n",
              "  'text': 'Но'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'не', 'wt': 1, 'gr': 'PART='}], 'text': 'не'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'каждый',\n",
              "    'wt': 0.9985975623,\n",
              "    'gr': 'APRO=(вин,ед,муж,неод|им,ед,муж)'}],\n",
              "  'text': 'каждый'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'хотеть',\n",
              "    'wt': 1,\n",
              "    'gr': 'V,несов,пе=непрош,ед,изъяв,3-л'}],\n",
              "  'text': 'хочет'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'что-то', 'wt': 1, 'gr': 'SPRO,ед,сред,неод=(вин|им)'}],\n",
              "  'text': 'что-то'},\n",
              " {'text': ' '},\n",
              " {'analysis': [{'lex': 'исправлять', 'wt': 1, 'gr': 'V,пе=инф,несов'}],\n",
              "  'text': 'исправлять'},\n",
              " {'text': ' :(\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ],
      "source": [
        "mystem_analyzer.analyze(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADcGtz4JLVRl"
      },
      "source": [
        "Давайте терепь используем лемматизатор майстема в качестве токенизатора."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "x48Q56tiLVRn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def my_preproc(text):\n",
        "    text = re.sub('[{}]'.format(punctuation), '', text)\n",
        "    text = mystem_analyzer.lemmatize(text)\n",
        "    return [word for word in text if word not in stopwords.words('russian') + [' ', '\\n']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEwOQTJPLVRq",
        "outputId": "04587540-caaa-4398-fa31-6b1d739f84ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.78      0.74      0.76     29461\n",
            "    positive       0.73      0.77      0.75     27248\n",
            "\n",
            "    accuracy                           0.76     56709\n",
            "   macro avg       0.76      0.76      0.76     56709\n",
            "weighted avg       0.76      0.76      0.76     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=my_preproc)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJlvqWuALVRs"
      },
      "source": [
        "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
        "Это модуль на питоне, довольно быстрый и с кучей функций."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHDkurN1zf7g",
        "outputId": "6032051d-d67e-4360-a467-d4a8a18c458e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "7SlwsLU7LVRt"
      },
      "outputs": [],
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qaz0x7frLVRw"
      },
      "source": [
        "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdf6XoEbLVRw",
        "outputId": "165bef01-a652-47ec-8de0-69dfa6c55399"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parse(word='платили', tag=OpencorporaTag('VERB,impf,tran plur,past,indc'), normal_form='платить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'платили', 2472, 10),))]"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ],
      "source": [
        "ana = pymorphy2_analyzer.parse(sent[3])\n",
        "ana"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "0KuHQGPgLVRz",
        "outputId": "785035a1-f4f6-4f3d-f8e3-826c47ce7f65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'платить'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 155
        }
      ],
      "source": [
        "ana[0].normal_form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFTkF8xUARlS"
      },
      "source": [
        "### [Natasha](https://github.com/natasha/)\n",
        "\n",
        "В библиотеке natasha реализовано множество полезных библиотек для русского языка: разбиение на токены и предложения, русскоязычные word embeddings, морфологический, синтаксический анализ, лемматизация, извлечение именованных сущностей и т.д. Модуль библиотеки Razdel, основанный на правилах, предназначен для разбиения текста на токены и предложения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CVeDxeIA6rg",
        "outputId": "0aa9ec36-3884-49ac-96bf-bf4731abd2c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: razdel in /usr/local/lib/python3.7/dist-packages (0.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install razdel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOTkw9MpAnNN",
        "outputId": "4c144bf7-1c0d-4303-c06d-978364790911"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Substring(0, 13, 'Кружка-термос'),\n",
              " Substring(14, 16, 'на'),\n",
              " Substring(17, 20, '0.5'),\n",
              " Substring(20, 21, 'л'),\n",
              " Substring(22, 23, '('),\n",
              " Substring(23, 28, '50/64'),\n",
              " Substring(29, 32, 'см³'),\n",
              " Substring(32, 33, ','),\n",
              " Substring(34, 37, '516'),\n",
              " Substring(37, 38, ';'),\n",
              " Substring(38, 41, '...'),\n",
              " Substring(41, 42, ')')]"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ],
      "source": [
        "from razdel import tokenize\n",
        "\n",
        "tokens = list(tokenize('Кружка-термос на 0.5л (50/64 см³, 516;...)'))\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftx-WzUbBCpO",
        "outputId": "3e14e05d-1632-4e9a-feae-3a115e9c3db5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Кружка-термос',\n",
              " 'на',\n",
              " '0.5',\n",
              " 'л',\n",
              " '(',\n",
              " '50/64',\n",
              " 'см³',\n",
              " ',',\n",
              " '516',\n",
              " ';',\n",
              " '...',\n",
              " ')']"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ],
      "source": [
        "[_.text for _ in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyhsQp4MGbW8",
        "outputId": "d26320ca-7eba-4441-be50-ff4bd367eaf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: natasha in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: slovnet>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: razdel>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.9.1)\n",
            "Requirement already satisfied: yargy>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.15.0)\n",
            "Requirement already satisfied: ipymarkup>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.9.0)\n",
            "Requirement already satisfied: navec>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from natasha) (0.10.0)\n",
            "Requirement already satisfied: intervaltree>=3 in /usr/local/lib/python3.7/dist-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from navec>=0.9.0->natasha) (1.21.6)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2->natasha) (0.6.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install natasha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMO3jsqLKSIV"
      },
      "source": [
        "С помощью библиотеки natasha можно также лемматизировать тексты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "vJZgfRnvIS2q"
      },
      "outputs": [],
      "source": [
        "from natasha import Doc, MorphVocab, Segmenter, NewsEmbedding, NewsMorphTagger\n",
        "\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "\n",
        "def natasha_lemmatize(text):\n",
        "  doc = Doc(text)\n",
        "  doc.segment(segmenter)\n",
        "  doc.tag_morph(morph_tagger)\n",
        "  for token in doc.tokens:\n",
        "    token.lemmatize(morph_vocab)\n",
        "  return {_.text: _.lemma for _ in doc.tokens}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBtlnYlFBOKv",
        "outputId": "5e72d35b-013d-4d03-dcd2-9264dca3446e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Посол': 'посол',\n",
              " 'Израиля': 'израиль',\n",
              " 'на': 'на',\n",
              " 'Украине': 'украина',\n",
              " 'Йоэль': 'йоэль',\n",
              " 'Лион': 'лион',\n",
              " 'признался': 'признаться',\n",
              " ',': ',',\n",
              " 'что': 'что',\n",
              " 'пришел': 'прийти',\n",
              " 'в': 'в',\n",
              " 'шок': 'шок',\n",
              " 'узнав': 'узнать',\n",
              " 'о': 'о',\n",
              " 'решении': 'решение',\n",
              " 'властей': 'власть',\n",
              " 'Львовской': 'львовский',\n",
              " 'области': 'область',\n",
              " 'объявить': 'объявить',\n",
              " '2019': '2019',\n",
              " 'год': 'год',\n",
              " 'годом': 'год',\n",
              " 'лидера': 'лидер',\n",
              " 'запрещенной': 'запретить',\n",
              " 'России': 'россия',\n",
              " 'Организации': 'организация',\n",
              " 'украинских': 'украинский',\n",
              " 'националистов': 'националист',\n",
              " '(': '(',\n",
              " 'ОУН': 'оун',\n",
              " ')': ')',\n",
              " 'Степана': 'степан',\n",
              " 'Бандеры': 'бандера',\n",
              " '.': '.',\n",
              " 'Свое': 'свой',\n",
              " 'заявление': 'заявление',\n",
              " 'он': 'он',\n",
              " 'разместил': 'разместить',\n",
              " 'Twitter': 'twitter',\n",
              " '«': '«',\n",
              " 'Я': 'я',\n",
              " 'не': 'не',\n",
              " 'могу': 'мочь',\n",
              " 'понять': 'понять',\n",
              " 'как': 'как',\n",
              " 'прославление': 'прославление',\n",
              " 'тех': 'тот',\n",
              " 'кто': 'кто',\n",
              " 'непосредственно': 'непосредственно',\n",
              " 'принимал': 'принимать',\n",
              " 'участие': 'участие',\n",
              " 'ужасных': 'ужасный',\n",
              " 'антисемитских': 'антисемитский',\n",
              " 'преступлениях': 'преступление',\n",
              " 'помогает': 'помогать',\n",
              " 'бороться': 'бороться',\n",
              " 'с': 'с',\n",
              " 'антисемитизмом': 'антисемитизм',\n",
              " 'и': 'и',\n",
              " 'ксенофобией': 'ксенофобия',\n",
              " 'Украина': 'украина',\n",
              " 'должна': 'должный',\n",
              " 'забывать': 'забывать',\n",
              " 'совершенных': 'совершить',\n",
              " 'против': 'против',\n",
              " 'евреев': 'еврей',\n",
              " 'никоим': 'никой',\n",
              " 'образом': 'образ',\n",
              " 'отмечать': 'отмечать',\n",
              " 'их': 'они',\n",
              " 'через': 'через',\n",
              " 'почитание': 'почитание',\n",
              " 'исполнителей': 'исполнитель',\n",
              " '»': '»',\n",
              " '—': '—',\n",
              " 'написал': 'написать',\n",
              " 'дипломат': 'дипломат',\n",
              " '11': '11',\n",
              " 'декабря': 'декабрь',\n",
              " 'Львовский': 'львовский',\n",
              " 'областной': 'областной',\n",
              " 'совет': 'совет',\n",
              " 'принял': 'принять',\n",
              " 'решение': 'решение',\n",
              " 'провозгласить': 'провозгласить',\n",
              " 'регионе': 'регион',\n",
              " 'связи': 'связь',\n",
              " 'празднованием': 'празднование',\n",
              " '110-летия': '110-летие',\n",
              " 'со': 'с',\n",
              " 'дня': 'день',\n",
              " 'рождения': 'рождение',\n",
              " 'Бандера': 'бандера',\n",
              " 'родился': 'родиться',\n",
              " '1': '1',\n",
              " 'января': 'январь',\n",
              " '1909': '1909',\n",
              " 'года': 'год',\n",
              " 'В': 'в',\n",
              " 'июле': 'июль',\n",
              " 'аналогичное': 'аналогичный',\n",
              " 'Житомирский': 'житомирский',\n",
              " 'начале': 'начало',\n",
              " 'месяца': 'месяц',\n",
              " 'предложением': 'предложение',\n",
              " 'к': 'к',\n",
              " 'президенту': 'президент',\n",
              " 'страны': 'страна',\n",
              " 'Петру': 'петр',\n",
              " 'Порошенко': 'порошенко',\n",
              " 'вернуть': 'вернуть',\n",
              " 'Бандере': 'бандера',\n",
              " 'звание': 'звание',\n",
              " 'Героя': 'герой',\n",
              " 'Украины': 'украина',\n",
              " 'обратились': 'обратиться',\n",
              " 'депутаты': 'депутат',\n",
              " 'Верховной': 'верховный',\n",
              " 'Рады': 'рада',\n",
              " 'Парламентарии': 'парламентарий',\n",
              " 'уверены': 'уверить',\n",
              " 'признание': 'признание',\n",
              " 'национальным': 'национальный',\n",
              " 'героем': 'герой',\n",
              " 'поможет': 'помочь',\n",
              " 'борьбе': 'борьба',\n",
              " 'подрывной': 'подрывной',\n",
              " 'деятельностью': 'деятельность',\n",
              " 'информационном': 'информационный',\n",
              " 'поле': 'поле',\n",
              " 'а': 'а',\n",
              " 'также': 'также',\n",
              " 'остановит': 'остановить',\n",
              " 'распространение': 'распространение',\n",
              " 'мифов': 'миф',\n",
              " 'созданных': 'создать',\n",
              " 'российской': 'российский',\n",
              " 'пропагандой': 'пропаганда',\n",
              " 'Степан': 'степан',\n",
              " '1909-1959': '1909-1959',\n",
              " 'был': 'быть',\n",
              " 'одним': 'один',\n",
              " 'из': 'из',\n",
              " 'лидеров': 'лидер',\n",
              " 'выступающей': 'выступать',\n",
              " 'за': 'за',\n",
              " 'создание': 'создание',\n",
              " 'независимого': 'независимый',\n",
              " 'государства': 'государство',\n",
              " 'территориях': 'территория',\n",
              " 'украиноязычным': 'украиноязычный',\n",
              " 'населением': 'население',\n",
              " '2010': '2010',\n",
              " 'году': 'год',\n",
              " 'период': 'период',\n",
              " 'президентства': 'президентство',\n",
              " 'Виктора': 'виктор',\n",
              " 'Ющенко': 'ющенко',\n",
              " 'посмертно': 'посмертно',\n",
              " 'признан': 'признать',\n",
              " 'Героем': 'герой',\n",
              " 'однако': 'однако',\n",
              " 'впоследствии': 'впоследствии',\n",
              " 'это': 'это',\n",
              " 'было': 'быть',\n",
              " 'отменено': 'отменить',\n",
              " 'судом': 'суд'}"
            ]
          },
          "metadata": {},
          "execution_count": 161
        }
      ],
      "source": [
        "text = 'Посол Израиля на Украине Йоэль Лион признался, что пришел в шок, узнав о решении властей Львовской области объявить 2019 год годом лидера запрещенной в России Организации украинских националистов (ОУН) Степана Бандеры. Свое заявление он разместил в Twitter. «Я не могу понять, как прославление тех, кто непосредственно принимал участие в ужасных антисемитских преступлениях, помогает бороться с антисемитизмом и ксенофобией. Украина не должна забывать о преступлениях, совершенных против украинских евреев, и никоим образом не отмечать их через почитание их исполнителей», — написал дипломат. 11 декабря Львовский областной совет принял решение провозгласить 2019 год в регионе годом Степана Бандеры в связи с празднованием 110-летия со дня рождения лидера ОУН (Бандера родился 1 января 1909 года). В июле аналогичное решение принял Житомирский областной совет. В начале месяца с предложением к президенту страны Петру Порошенко вернуть Бандере звание Героя Украины обратились депутаты Верховной Рады. Парламентарии уверены, что признание Бандеры национальным героем поможет в борьбе с подрывной деятельностью против Украины в информационном поле, а также остановит «распространение мифов, созданных российской пропагандой». Степан Бандера (1909-1959) был одним из лидеров Организации украинских националистов, выступающей за создание независимого государства на территориях с украиноязычным населением. В 2010 году в период президентства Виктора Ющенко Бандера был посмертно признан Героем Украины, однако впоследствии это решение было отменено судом. '\n",
        "\n",
        "natasha_lemmatize(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rck5OVqhLVSA"
      },
      "source": [
        "### mystem vs. pymorphy vs. natasha\n",
        "\n",
        "1) *Мы надеемся, что вы пользуетесь линуксом*, но mystem работает невероятно медленно под windows на больших текстах.\n",
        "\n",
        "2) *Снятие омонимии*. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту, natasha тоже с этим тоже не справляется успешно:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "kH2GQ4ddLVSB"
      },
      "outputs": [],
      "source": [
        "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
        "homonym2 = 'Сорока своровала блестящее украшение со стола.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwF-XsjeI3eX",
        "outputId": "3d000991-457e-4a95-902d-254cc974b858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'analysis': [{'lex': 'сорок', 'wt': 0.8710292578, 'gr': 'NUM=(пр|дат|род|твор)'}], 'text': 'сорока'}\n",
            "{'analysis': [{'lex': 'сорока', 'wt': 0.1210970059, 'gr': 'S,жен,од=им,ед'}], 'text': 'Сорока'}\n"
          ]
        }
      ],
      "source": [
        "mystem_analyzer = Mystem() # инициализирую объект с дефолтными параметрами\n",
        "\n",
        "print(mystem_analyzer.analyze(homonym1)[-5])\n",
        "print(mystem_analyzer.analyze(homonym2)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9jezRVlFmDo",
        "outputId": "73c40fbc-abfe-461b-8647-5424c6b5c3fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'За': 'за', 'время': 'время', 'обучения': 'обучение', 'я': 'я', 'прослушал': 'прослушать', 'больше': 'большой', 'сорока': 'сорок', 'курсов': 'курс', '.': '.'}\n"
          ]
        }
      ],
      "source": [
        "print(natasha_lemmatize(homonym1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXjGBQPoI9gl",
        "outputId": "b1991e37-1902-497a-d9b3-46ebaf4a922c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Сорока': 'сорок', 'своровала': 'своровать', 'блестящее': 'блестящий', 'украшение': 'украшение', 'со': 'с', 'стола': 'стол', '.': '.'}\n"
          ]
        }
      ],
      "source": [
        "print(natasha_lemmatize(homonym2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aP5qFnilLVSI"
      },
      "source": [
        "## Словарь, закон Ципфа и закон Хипса"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1umtd3OLVSI"
      },
      "source": [
        "Закон Ципфа -- эмпирическая закономерность: если все слова корпуса текста упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n. Иными словами, частотность слов убывает очень быстро."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "lY0cWJ7eLVSJ"
      },
      "outputs": [],
      "source": [
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIjqSVjpLVSL",
        "outputId": "99a279df-d9e0-42b4-df62-1f9cbbfcc137"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2870536\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['first_timee', 'хоть', 'я', 'и', 'школота', 'но', 'поверь', 'у', 'нас', 'то']"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ],
      "source": [
        "corpus = [token for tweet in df.text for token in nltk.word_tokenize(tweet) if token not in punctuation]\n",
        "print(len(corpus))\n",
        "corpus[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "id": "_oWC7NpkLVSO",
        "outputId": "82a3e29a-98a0-443f-ee83-fd27512477e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('не', 69472),\n",
              " ('и', 55166),\n",
              " ('в', 52902),\n",
              " ('я', 52818),\n",
              " ('RT', 38070),\n",
              " ('на', 35759),\n",
              " ('http', 32998),\n",
              " ('что', 31541),\n",
              " ('с', 27217),\n",
              " ('а', 26860)]"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ],
      "source": [
        "freq_dict = Counter(corpus)\n",
        "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
        "list(freq_dict_sorted)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "FrPkce0SLVSQ",
        "outputId": "99cf7502-3d99-4ca7-95bf-49266ec467f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "first_100_freqs = [freq for word, freq in freq_dict_sorted[:100]]\n",
        "plt.plot(first_100_freqs)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_N5V_K-LVSU"
      },
      "source": [
        "Закон Хипса -- обратная сторона закона Ципфа. Он описывает, что чем больше корпус, тем меньше новых слов добавляется с добавлением новых текстов. В какой-то момент корпус насыщается."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw0GieJSMU-O"
      },
      "source": [
        "## Задание 1.\n",
        "\n",
        "**Задание**: обучите три классификатора: \n",
        "\n",
        "1) на токенах с высокой частотой \n",
        "\n",
        "2) на токенах со средней частотой \n",
        "\n",
        "3) на токенах с низкой частотой\n",
        "\n",
        "Сравните полученные результаты, оцените какие токены наиболее важные для классификации."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создадим корпус по тренировочным данным и отсортируем по частотам"
      ],
      "metadata": {
        "id": "P9eNQg0V88J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [token for tweet in x_train for token in nltk.word_tokenize(tweet) if token not in punctuation]\n",
        "print(len(corpus))\n",
        "corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umYRmote8a03",
        "outputId": "d4d2f805-134c-4bb2-e7d9-31c09543cccf"
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2152786\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['можно',\n",
              " 'бы',\n",
              " 'было',\n",
              " 'нарисовать',\n",
              " 'это',\n",
              " 'но',\n",
              " 'я',\n",
              " 'не',\n",
              " 'люблю',\n",
              " 'пейринг']"
            ]
          },
          "metadata": {},
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict = Counter(corpus)\n",
        "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
        "list(freq_dict_sorted)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BP0t_Shf85Km",
        "outputId": "10b89333-8979-4fad-89a8-2bfef79e0413"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('не', 52166),\n",
              " ('и', 41350),\n",
              " ('я', 39651),\n",
              " ('в', 39604),\n",
              " ('RT', 28499),\n",
              " ('на', 26806),\n",
              " ('http', 24663),\n",
              " ('что', 23765),\n",
              " ('с', 20397),\n",
              " ('а', 20173)]"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Получим частоты"
      ],
      "metadata": {
        "id": "lMVmUsgO0aGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freqs = [freq for word, freq in freq_dict_sorted]"
      ],
      "metadata": {
        "id": "mHvo87K6uSZO"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Найдем такие пороги, чтобы модели встречали одинаковое количество раз слова из своих словарей"
      ],
      "metadata": {
        "id": "9Xz4gS-x0g_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_freqs = 0\n",
        "flag_1 = True\n",
        "for i in range(len(freqs)):\n",
        "  sum_freqs+=freqs[i]\n",
        "  if flag_1:\n",
        "    if sum_freqs>=(sum(freqs)*1/3):\n",
        "      threshold_token_1 = i\n",
        "      flag_1 = False\n",
        "  if sum_freqs>=(sum(freqs)*2/3):\n",
        "    threshold_token_2 = i\n",
        "    break\n",
        "print(threshold_token_1)\n",
        "print(threshold_token_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h24FRYaiuMg2",
        "outputId": "029c632c-8a7c-44d0-aa19-702d7539af77"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "73\n",
            "3059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Посмторим на крайние токены"
      ],
      "metadata": {
        "id": "Ab6q77UC1iTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(freq_dict_sorted[threshold_token_1])\n",
        "print(freq_dict_sorted[threshold_token_2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWGuhTIr1ePV",
        "outputId": "bd9b0e02-2bf2-4f6d-f398-a6191fc0732b"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('всё', 2885)\n",
            "('Человек', 57)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Плучим слова"
      ],
      "metadata": {
        "id": "fxuvgle31mcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_sorted_words = [word for word, freq in freq_dict_sorted]"
      ],
      "metadata": {
        "id": "2IPa8ni0ykjh"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) на токенах с высокой частотой"
      ],
      "metadata": {
        "id": "CU0ybgaQqalO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "high_freq_tokens = freq_sorted_words[:threshold_token_1]"
      ],
      "metadata": {
        "id": "F4GT9UQftrwy"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "id": "QUQ6kAgPMqNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "161768d3-72c5-4279-a7a6-ddcf1f76dfca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1323: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  \"Upper case characters found in\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.57      0.62      0.59     25610\n",
            "    positive       0.66      0.61      0.64     31099\n",
            "\n",
            "    accuracy                           0.62     56709\n",
            "   macro avg       0.62      0.62      0.62     56709\n",
            "weighted avg       0.62      0.62      0.62     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=nltk.word_tokenize, stop_words=list(punctuation), vocabulary=high_freq_tokens)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) на токенах со средней частотой"
      ],
      "metadata": {
        "id": "loj7JMUZ175t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "medium_freq_tokens = freq_sorted_words[threshold_token_1:threshold_token_2]"
      ],
      "metadata": {
        "id": "GV6CYSP0yAVI"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=nltk.word_tokenize, stop_words=list(punctuation), vocabulary=medium_freq_tokens)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Spyp1puPx-BQ",
        "outputId": "f13e64cf-91d5-440a-f1fa-2c14f5125372"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1323: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  \"Upper case characters found in\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.64      0.71      0.67     25175\n",
            "    positive       0.74      0.68      0.71     31534\n",
            "\n",
            "    accuracy                           0.69     56709\n",
            "   macro avg       0.69      0.69      0.69     56709\n",
            "weighted avg       0.70      0.69      0.69     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) на токенах с низкой частотой"
      ],
      "metadata": {
        "id": "tnKQSscB2LBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "low_freq_tokens = freq_sorted_words[threshold_token_2:]"
      ],
      "metadata": {
        "id": "l-Ly4r-M2ObB"
      },
      "execution_count": 181,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=nltk.word_tokenize, stop_words=list(punctuation), vocabulary=low_freq_tokens)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SrJ9FWt03IfB",
        "outputId": "36d44642-0927-4890-8a7d-00d782470e62"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:1323: UserWarning: Upper case characters found in vocabulary while 'lowercase' is True. These entries will not be matched with any documents\n",
            "  \"Upper case characters found in\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.66      0.71     32299\n",
            "    positive       0.62      0.74      0.68     24410\n",
            "\n",
            "    accuracy                           0.70     56709\n",
            "   macro avg       0.70      0.70      0.69     56709\n",
            "weighted avg       0.71      0.70      0.70     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вывод:"
      ],
      "metadata": {
        "id": "OQ-QN8gS3L74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1). В пером случае у нас маленький словарь и большинство слов, это шум, который не несёт большого смысла, типо предлогов\n",
        "\n",
        "2). Метрики уже лучше\n",
        "\n",
        "3). Усреднённые метрики так же неплохие, но видно, что класс positive предсказывается хуже negative, скорее всего, это потому, что как я предпологал, радость (positive) выражается более тривиально, поэтому частота у слов выражающих positive больше, чем у слов выражающих negative, а так как вы взяли только слова с низкой частотой, то и получили соответствующие метрики"
      ],
      "metadata": {
        "id": "GOP4O-YB3OG9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV3fmzp-LVSU"
      },
      "source": [
        "## О важности эксплоративного анализа\n",
        "\n",
        "Но иногда пунктуация бывает и не шумом -- главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "qjkMxK9VLVSV",
        "outputId": "c7cc9ba2-7c6e-4915-b139-1705e80b5d6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     27846\n",
            "    positive       1.00      1.00      1.00     28863\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=nltk.word_tokenize)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2fRbUAvLVSX"
      },
      "source": [
        "Шок! Стоило оставить пунктуацию -- и все метрики равны 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?). Найдите фичи с самыми большими коэффициэнтами:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLmbTZMESJaS"
      },
      "source": [
        "## Задание 2.\n",
        "\n",
        "найти фичи с наибольшей значимостью, и вывести их"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### О том насколько влияют токены нам скажут их веса, чем они больше в абсолютном значении, тем большее влияние у токена"
      ],
      "metadata": {
        "id": "8s_VGa88MIlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dict_coef = list(zip(clf.coef_[0], range(len(clf.coef_[0]))))\n",
        "dict_coef[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a54eYCZjINcw",
        "outputId": "68d5812d-7af9-4e0e-8d53-439e2d1aee70"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.030784292058882005, 0),\n",
              " (0.41113267388728575, 1),\n",
              " (-0.11657384873465453, 2),\n",
              " (1.3144201690016943, 3),\n",
              " (-0.2475210086426806, 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dict_coef_sorted = sorted(dict_coef, key=(lambda x: -abs(x[0])))\n",
        "dict_coef_sorted[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q15GvNlfI0Mm",
        "outputId": "f740dd59-11c0-4c60-ac4c-63883b517551"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(-59.8126301986186, 7),\n",
              " (58.310687241732, 8),\n",
              " (26.944090144998846, 43300),\n",
              " (-10.980499685002645, 101241),\n",
              " (-10.91095083776024, 179872)]"
            ]
          },
          "metadata": {},
          "execution_count": 185
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(vec.vocabulary_.items())[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uz85eigrJXbI",
        "outputId": "a87d2146-e6d9-4e05-e1d0-82b9b41d91f7"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('можно', 168521),\n",
              " ('бы', 113822),\n",
              " ('было', 113929),\n",
              " ('нарисовать', 173395),\n",
              " ('это', 256999)]"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec_vocabulary_reverse = {y:x for x, y in vec.vocabulary_.items()}\n",
        "list(vec_vocabulary_reverse.items())[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg34kdcGKccW",
        "outputId": "93027e18-797f-425b-e85a-a8aa0eabc99c"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(168521, 'можно'),\n",
              " (113822, 'бы'),\n",
              " (113929, 'было'),\n",
              " (173395, 'нарисовать'),\n",
              " (256999, 'это')]"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_sorted = [vec_vocabulary_reverse[y] for x,y in dict_coef_sorted]\n",
        "vocabulary_sorted[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UOR-4LgJtwV",
        "outputId": "f39dc13e-3a4a-4801-8237-d0c0c0b51d91"
      },
      "execution_count": 188,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(', ')', 'd', '|', 'о_о', 'dd', '^_^', '-/', 'o_o', 'ddd']"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vtAyItvLVSb"
      },
      "source": [
        "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "uqH07o-7LVSc",
        "outputId": "fce64bd9-a4ed-424e-dcc1-377587347a5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.85      0.92     32764\n",
            "    positive       0.83      1.00      0.91     23945\n",
            "\n",
            "    accuracy                           0.91     56709\n",
            "   macro avg       0.92      0.93      0.91     56709\n",
            "weighted avg       0.93      0.91      0.91     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cool_token = ')'\n",
        "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вывод:"
      ],
      "metadata": {
        "id": "xB6WP91OMt6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В задаче Sentiment analysis огромную роль играют эмотиконы, а значит и символы(знаки пунктуации) из которых их часто состоявляют"
      ],
      "metadata": {
        "id": "1oB91nB1Mwf3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5THCOjMLVSg"
      },
      "source": [
        "## Символьные n-граммы\n",
        "\n",
        "Теперь в качестве фичей используем, например, униграммы символов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "AIUwDOabLVSh",
        "outputId": "35f41cde-8949-476f-f4e6-baecd0853151",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.99      1.00      1.00     27822\n",
            "    positive       1.00      0.99      1.00     28887\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_E0uPpgLVSj"
      },
      "source": [
        "В общем-то, теперь уже понятно, почему на этих данных здесь 1. Так или инчае, на символах классифицировать тоже можно: для некторых задач (например, для определения языка) фичи-символьные n-граммы решительно рулят.\n",
        "\n",
        "Ещё одна замечательная особенность фичей-символов: токенизация и лемматизация не нужна, можно использовать такой подход для языков, у которых нет готвых анализаторов."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3."
      ],
      "metadata": {
        "id": "ArV9y1QRNy_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  1). сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report)"
      ],
      "metadata": {
        "id": "yuOrW6lbN8Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer"
      ],
      "metadata": {
        "id": "ArbiAKtIORxF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFfuIwGNSJaW",
        "outputId": "8a70c1c3-6cfb-48ff-b9e8-2d8d1f545fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.80      0.76      0.78     29210\n",
            "    positive       0.76      0.80      0.78     27499\n",
            "\n",
            "    accuracy                           0.78     56709\n",
            "   macro avg       0.78      0.78      0.78     56709\n",
            "weighted avg       0.78      0.78      0.78     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "vec = CountVectorizer(ngram_range=(1, 1), tokenizer=nltk.word_tokenize, stop_words=noise)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TfidfVectorizer"
      ],
      "metadata": {
        "id": "2u-OSUQMOYWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=nltk.word_tokenize, stop_words=noise)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAYoDHDIOgZU",
        "outputId": "811a1973-a287-4660-a4d9-c043caf829d6"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.77      0.77      0.77     27779\n",
            "    positive       0.78      0.78      0.78     28930\n",
            "\n",
            "    accuracy                           0.77     56709\n",
            "   macro avg       0.77      0.77      0.77     56709\n",
            "weighted avg       0.77      0.77      0.77     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HashingVectorizer"
      ],
      "metadata": {
        "id": "Ap5yFQzSOkwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer"
      ],
      "metadata": {
        "id": "CVttacgfPRXs"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashing_n_features = 100"
      ],
      "metadata": {
        "id": "K0ATwidfQTaD"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vec = HashingVectorizer(ngram_range=(1, 1), tokenizer=nltk.word_tokenize, stop_words=noise, n_features=hashing_n_features)\n",
        "bow = vec.fit_transform(x_train)\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(bow, y_train)\n",
        "pred = clf.predict(vec.transform(x_test))\n",
        "print(classification_report(pred, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcQTzxQ6PQdK",
        "outputId": "5df59e0c-18aa-4f0a-bcf7-86cbff4003ab"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.61      0.59      0.60     29072\n",
            "    positive       0.59      0.61      0.60     27637\n",
            "\n",
            "    accuracy                           0.60     56709\n",
            "   macro avg       0.60      0.60      0.60     56709\n",
            "weighted avg       0.60      0.60      0.60     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Neural network"
      ],
      "metadata": {
        "id": "g0sWVGDMPJzW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предобработка данных:"
      ],
      "metadata": {
        "id": "fCVIusO6he69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [token for tweet in x_train for token in nltk.word_tokenize(tweet) if token not in noise]\n",
        "corpus[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMNqc2awYkE6",
        "outputId": "882a617d-e794-4ac2-ba5a-451379d7f681"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['нарисовать', 'это', 'люблю', 'пейринг', 'УиллГаннибал']"
            ]
          },
          "metadata": {},
          "execution_count": 196
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dict = Counter(corpus)\n",
        "freq_dict_sorted= sorted(freq_dict.items(), key=lambda x: -x[1])\n",
        "list(freq_dict_sorted)[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mbisigafNld",
        "outputId": "b5fdc74f-7b84-4b7c-a1b1-829de354b964"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('RT', 28499),\n",
              " ('http', 24663),\n",
              " ('...', 16825),\n",
              " ('это', 12402),\n",
              " ('D', 12393),\n",
              " (\"''\", 9389),\n",
              " ('Я', 9214),\n",
              " ('``', 8554),\n",
              " ('..', 8508),\n",
              " ('А', 6485)]"
            ]
          },
          "metadata": {},
          "execution_count": 197
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_size = len(freq_dict)\n",
        "vocabulary_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1mynV2RfumA",
        "outputId": "80525ae5-efb8-4d19-9318-7108ddd55a41"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "288162"
            ]
          },
          "metadata": {},
          "execution_count": 198
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VOCABULARY_SIZE = vocabulary_size\n",
        "VOCABULARY_SIZE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99ZYsNsqfYyr",
        "outputId": "123ca1d5-d0a2-4dbb-cbc6-b701cf383f2d"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "288162"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_n_words = [word for word, freq in freq_dict_sorted[:VOCABULARY_SIZE]]\n",
        "top_n_words[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPSF_zNIgAza",
        "outputId": "7c3cc8aa-123e-4df1-ba60-b7b79192b0ca"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RT', 'http', '...', 'это', 'D']"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {y:x for x,y in enumerate(top_n_words, start=1)}\n",
        "list(vocab.items())[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC5-aKoZYC_Y",
        "outputId": "68b2ac24-bbb0-4e0d-c172-fe2bef71a6ee"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('RT', 1), ('http', 2), ('...', 3), ('это', 4), ('D', 5)]"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_nn = x_train.apply(lambda text: [vocab.get(word, 0) for word in nltk.word_tokenize(text) if word not in noise])\n",
        "x_test_nn = x_test.apply(lambda text: [vocab.get(word, 0) for word in nltk.word_tokenize(text) if word not in noise])\n",
        "x_train_nn[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbD64APPZUct",
        "outputId": "e84b5e33-c6cb-4250-e31f-6c06dc6871dd"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50271    [3169, 4, 31, 15605, 91318, 1875, 2997, 2, 91319]\n",
              "83517                                 [219, 76, 91320, 75]\n",
              "69561     [55396, 2367, 55397, 46, 2368, 6327, 7067, 1305]\n",
              "52926    [91321, 38, 1038, 5162, 91, 7652, 5691, 55398,...\n",
              "35659                [91322, 206, 91323, 91324, 1326, 412]\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 202
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = x_train_nn.apply(lambda txt: len(txt)).max()\n",
        "max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiRhuEPoZNjh",
        "outputId": "0e912d3b-291f-45d1-9f86-e45ea871ce1f"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = max_len\n",
        "MAX_LEN"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xpekPdBFfkF4",
        "outputId": "26c5c769-edfa-46f3-8ec7-43d475effe14"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_nn = x_train_nn.apply(lambda x: x+[0]*(MAX_LEN-len(x)))\n",
        "x_test_nn = x_test_nn.apply(lambda x: x+[0]*(MAX_LEN-len(x)))\n",
        "x_train_nn[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0QmEMnheEs2",
        "outputId": "b61c7641-9cfc-469f-ccd1-5a2b32a5bd12"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50271    [3169, 4, 31, 15605, 91318, 1875, 2997, 2, 913...\n",
              "83517    [219, 76, 91320, 75, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
              "69561    [55396, 2367, 55397, 46, 2368, 6327, 7067, 130...\n",
              "52926    [91321, 38, 1038, 5162, 91, 7652, 5691, 55398,...\n",
              "35659    [91322, 206, 91323, 91324, 1326, 412, 0, 0, 0,...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_nn_array = np.array([j for j in [i for i in x_train_nn]])\n",
        "x_test_nn_array = np.array([j for j in [i for i in x_test_nn]])\n",
        "x_train_nn_array[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k70GPMTcpKxY",
        "outputId": "1bb0b777-da22-4e72-b57b-d35d7397db00"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3169,     4,    31, 15605, 91318,  1875,  2997,     2, 91319,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 206
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_nn = y_train.apply(lambda x: 1 if x=='positive' else 0)\n",
        "y_test_nn = y_test.apply(lambda x: 1 if x=='positive' else 0)\n",
        "y_train_nn[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csSae9NBj0yy",
        "outputId": "fe80b8ad-328f-4fd8-cde9-1864a868f84f"
      },
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50271    0\n",
              "83517    0\n",
              "69561    1\n",
              "52926    1\n",
              "35659    0\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель:"
      ],
      "metadata": {
        "id": "MJaQ5MKMhwca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalAveragePooling1D, BatchNormalization, Dropout"
      ],
      "metadata": {
        "id": "DafVxLP3RKd4"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 32\n",
        "hidden_dim = 16\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "S6iClTDziPJw"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(VOCABULARY_SIZE+1, embedding_dim), # VOCABULARY_SIZE+1 так как еще 0, который показывает где нет слова\n",
        "    BatchNormalization(),\n",
        "    Dropout(dropout),\n",
        "\n",
        "    GlobalAveragePooling1D(),\n",
        "\n",
        "    Dense(hidden_dim, activation='relu'),\n",
        "    Dense(1)\n",
        "])"
      ],
      "metadata": {
        "id": "AVE1ClpwPKY6"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "JwofM6RyQ-xR"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение"
      ],
      "metadata": {
        "id": "dBqQVhOvyXUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hh = model.fit(x_train_nn_array, y_train_nn, epochs=2, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR9tKtkARAgv",
        "outputId": "6485607d-e092-47b6-aab4-9b05a0706c37"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1330/1330 [==============================] - 149s 111ms/step - loss: 0.4999 - accuracy: 0.7252\n",
            "Epoch 2/2\n",
            "1330/1330 [==============================] - 142s 107ms/step - loss: 0.2752 - accuracy: 0.8792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_preds = model.predict(x_test_nn_array)\n",
        "nn_preds[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNowB0PSwoXy",
        "outputId": "8dc29ea9-6d58-45cb-ba95-54c570b76447"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3.348521 ],\n",
              "       [-2.7403553],\n",
              "       [-1.5601468],\n",
              "       [ 0.9593387],\n",
              "       [-1.1045706]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 290
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_preds_bin = (nn_preds > 0).astype(int)\n",
        "nn_preds_bin[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdl303qTzzem",
        "outputId": "311c4540-442e-44f3-d4fa-696c3b0d8d88"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(nn_preds_bin, y_test_nn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKiQIYx6yNiA",
        "outputId": "a6e6bbbf-3bb9-4c1a-a674-b7e8bed8b2c7"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.66      0.77     38944\n",
            "           1       0.54      0.87      0.67     17765\n",
            "\n",
            "    accuracy                           0.73     56709\n",
            "   macro avg       0.73      0.77      0.72     56709\n",
            "weighted avg       0.80      0.73      0.74     56709\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вывод:"
      ],
      "metadata": {
        "id": "0QgAczmzwWiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строить нейронную сеть и препроцессить для неё данные долго, еще дольше только подбирать параметры и обучать её, а результат считай такой же, но это я её не слишком настраивал\n",
        "\n",
        "HashingVectorizer быстрее остальных, но результаты хуже остальных, дальше буду подбирать количество признаков.\n",
        "\n",
        "Результаты CountVectorizer и TfidfVectorizer почти одинаковые, но у TfidfVectorizer они более уравновешанные относительно классов. "
      ],
      "metadata": {
        "id": "a1vW2gngwZ42"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  2). подобрать оптимальный размер для hashing векторайзера"
      ],
      "metadata": {
        "id": "RR-7xV6VynUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "3xxjihsd6IyH"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_features = [100, 1000, 10000, 100000, 200000]"
      ],
      "metadata": {
        "id": "TmkWvDJW2OX7"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for features in n_features:\n",
        "  start_time = time.time()\n",
        "  vec = HashingVectorizer(ngram_range=(1, 1), tokenizer=nltk.word_tokenize, stop_words=noise, n_features=features)\n",
        "  bow = vec.fit_transform(x_train)\n",
        "  clf = LogisticRegression(random_state=42)\n",
        "  clf.fit(bow, y_train)\n",
        "  pred = clf.predict(vec.transform(x_test))\n",
        "  acc = (pred == y_test).sum() / len(y_test)\n",
        "  print(f'!!!!!!!!!!!! n_features: {features}; acc: {acc}; time: {time.time()-start_time} !!!!!!!!!!!!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Hrn3bS62Xbr",
        "outputId": "633f46c3-5d9b-4277-d3d5-d38efee468d5"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!!!!!!!!!! n_features: 100; acc: 0.59949919765822; time: 48.75623631477356 !!!!!!!!!!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!!!!!!!!!! n_features: 1000; acc: 0.6619407854132501; time: 49.84330081939697 !!!!!!!!!!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!!!!!!!!!! n_features: 10000; acc: 0.7242765698566365; time: 51.74312472343445 !!!!!!!!!!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!!!!!!!!!! n_features: 100000; acc: 0.7616427727521204; time: 55.23396873474121 !!!!!!!!!!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!!!!!!!!!!! n_features: 200000; acc: 0.7655574952829357; time: 57.30839824676514 !!!!!!!!!!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Вывод:"
      ],
      "metadata": {
        "id": "yjAGecZ15oWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чем больше мы возьмём параметров, тем лучше результат, но тем дольше обучается модель, но например разница по метрикам между n_features = 100 и n_features = 1000 большая, а вот по времени маленькая, а вот для n_features = 100000 и n_features = 200000, наоборот, разница по метрики маленькая, а вот по времени уже больше, но всё равно приемлемая.\n",
        "\n",
        "Поэтому для меня оптимальный размер n_features = 200000, так как это лучший результат, а разница во времени с другими не существенная, если бы было больше данных, тогда да, можно было бы думать о n_features поменьше."
      ],
      "metadata": {
        "id": "jC2W-2cV5tij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  3). убедиться что для сетки нет переобучения"
      ],
      "metadata": {
        "id": "F0OEwdnw3qOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn_preds = model.predict(x_train_nn_array)\n",
        "nn_preds[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Nh6v5cQ8juK",
        "outputId": "d38fb51e-db08-4e61-bc56-01bca3482b4d"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-2.9176903 ],\n",
              "       [-1.8845387 ],\n",
              "       [-0.45846447],\n",
              "       [ 5.94189   ],\n",
              "       [-4.481045  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 293
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5"
      ],
      "metadata": {
        "id": "9waUbbFh8m85"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn_preds_bin = (nn_preds > 0).astype(int)\n",
        "nn_preds_bin[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqlpDd4H8o4y",
        "outputId": "8d2f4684-3a07-48c1-ff33-2c57a190d8d3"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [0],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0]])"
            ]
          },
          "metadata": {},
          "execution_count": 295
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(nn_preds_bin, y_train_nn))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blkB0gyA8qo8",
        "outputId": "e7e3ecee-11eb-45b5-907f-e3c99cf83987"
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.85      0.91     98718\n",
            "           1       0.82      0.99      0.90     71407\n",
            "\n",
            "    accuracy                           0.91    170125\n",
            "   macro avg       0.91      0.92      0.91    170125\n",
            "weighted avg       0.92      0.91      0.91    170125\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkZvXIAo9RvF",
        "outputId": "44a30eff-c82f-4c83-a44a-fd20a6a2cf9c"
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_10 (Embedding)    (None, None, 32)          9221216   \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, None, 32)         128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, None, 32)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d_9   (None, 32)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,221,889\n",
            "Trainable params: 9,221,825\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метрики на train не равны все 1, что уже хорошо, они конечно больше на 0.2 чем метрики на test, поэтому можно сказать, что небольшое переобучение возможно. Но я и так тренировал всего 2 эпохи, взял маленькое количество нейронов для классификации и использовал batchnorm и dropout(0.2). \n",
        "\n",
        "Стоит отметить, что сновная часть модели, это слой Embedding."
      ],
      "metadata": {
        "id": "XNUNoZd-3vsL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHNfeY7cSJaX"
      },
      "source": [
        "## Домашнее задание\n",
        "\n",
        "все материалы для выполения дз в `sem2.ipynb`\n",
        "\n",
        "\n",
        "### Задание 1.\n",
        "\n",
        "**Задание**: обучите три классификатора: \n",
        "\n",
        "1) на токенах с высокой частотой \n",
        "\n",
        "2) на токенах со средней частотой \n",
        "\n",
        "3) на токенах с низкой частотой\n",
        "\n",
        "\n",
        "Сравните полученные результаты, оцените какие токены наиболее важные для классификации.\n",
        "\n",
        "\n",
        "### Задание 2.\n",
        "\n",
        "найти фичи с наибольшей значимостью, и вывести их\n",
        "\n",
        "\n",
        "### Задание 3.\n",
        "\n",
        "1) сравнить count/tf-idf/hashing векторайзеры/полносвязанную сетку (построить classification_report)\n",
        "\n",
        "2) подобрать оптимальный размер для hashing векторайзера \n",
        "\n",
        "3) убедиться что для сетки нет переобучения"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Решение ДЗ выше "
      ],
      "metadata": {
        "id": "_2Lj_510G9yr"
      }
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lesson_2.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}